{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PRS On-Premises Deployment Guide","text":"<p>Welcome to the comprehensive deployment and maintenance guide for the PRS (Purchase Request System) on-premises production environment.</p>"},{"location":"#overview","title":"Overview","text":"<p>This documentation provides complete guidance for deploying, configuring, and maintaining the PRS system in an on-premises environment with enterprise-grade performance, security, and reliability.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Dual Storage Architecture: SSD for performance, HDD for capacity</li> <li>TimescaleDB Integration: Zero-deletion policy with automatic data tiering</li> <li>Enterprise Security: SSL/TLS, security hardening, and monitoring</li> <li>High Performance: Optimized for 100+ concurrent users</li> <li>Automated Operations: Backup, monitoring, and maintenance automation</li> <li>NAS Integration: Enterprise backup with network storage support</li> <li>One-Day Deployment: Complete setup in 6-8 hours with automation</li> </ul>"},{"location":"#quick-start-options","title":"Quick Start Options","text":""},{"location":"#quick-deployment-recommended","title":"\ud83d\ude80 Quick Deployment (Recommended)","text":"<p>Complete enterprise-grade deployment in 2-3 hours:</p> <p>Prerequisites First</p> <p>\ud83d\udccb Prerequisites Guide - Essential server setup required before deployment</p> <p>\ud83d\udcd6 Quick Start Guide - Simple 3-step process using proven scripts</p> <pre><code># Clone deployment repository first:\nsudo mkdir -p /opt/prs &amp;&amp; sudo chown $USER:$USER /opt/prs\ncd /opt/prs &amp;&amp; git clone https://github.com/stratpoint-engineering/prs-deployment.git\ncd prs-deployment/scripts\n\n# Prerequisites check:\n./check-prerequisites.sh\n\n# Simple 3-step process:\n1. Configure: ./quick-setup-helper.sh\n2. Deploy: sudo ./deploy-onprem.sh deploy\n3. Automate: ./setup-backup-automation.sh &amp;&amp; ./setup-monitoring-automation.sh\n</code></pre> <p>What you get: - Complete PRS application stack - SSL/TLS security (GoDaddy/Let's Encrypt/Self-signed) - Enterprise backup with NAS integration - Grafana monitoring and alerting - Automated maintenance and health checks - Office network security configuration (192.168.0.0/20 internal access only)</p>"},{"location":"#manual-deployment","title":"\ud83d\udccb Manual Deployment","text":"<p>Step-by-step deployment for custom requirements:</p> <ol> <li>Prerequisites - System requirements and preparation</li> <li>Hardware Setup - Configure dual storage and network</li> <li>Installation - Deploy the complete system</li> <li>Configuration - Customize for your environment</li> <li>Testing - Validate your deployment</li> </ol>"},{"location":"#specifications","title":"Specifications","text":""},{"location":"#requirements","title":"Requirements","text":"<ul> <li>RAM: 16 GB (4x improvement over cloud)</li> <li>Storage: 470 GB SSD RAID1 + 2.4 TB HDD RAID5</li> <li>Network: 1 Gbps dedicated interface</li> <li>Backup Power: UPS system available</li> </ul>"},{"location":"#targets","title":"Targets","text":"<ul> <li>Concurrent Users: 100+ users</li> <li>Response Time: &lt;200ms for recent data</li> <li>Uptime: 99.9% availability</li> <li>Data Growth: 20GB/day sustainable</li> </ul>"},{"location":"#overview_1","title":"Overview","text":"<pre><code>%%{init: {\"theme\": \"base\", \"themeVariables\": {\"background\": \"#ffffff\", \"primaryColor\": \"#fff2cc\", \"primaryTextColor\": \"#000000\", \"primaryBorderColor\": \"#d6b656\", \"lineColor\": \"#666666\", \"secondaryColor\": \"#eeeeee\", \"tertiaryColor\": \"#ffffff\"}}}%%\ngraph TB\n    subgraph \"Client Network\"\n        U[Users] --&gt; LB[Load Balancer]\n    end\n\n    subgraph \"Application Layer\"\n        LB --&gt; N[Nginx Proxy]\n        N --&gt; F[Frontend]\n        N --&gt; B[Backend API]\n        B --&gt; W[Redis Worker]\n    end\n\n    subgraph \"Data Layer\"\n        B --&gt; R[Redis Cache]\n        B --&gt; DB[(TimescaleDB)]\n        W --&gt; R\n        W --&gt; DB\n    end\n\n    subgraph \"Storage Tiers\"\n        DB --&gt; SSD[SSD Storage&lt;br/&gt;Hot Data 0-30 days]\n        DB --&gt; HDD[HDD Storage&lt;br/&gt;Cold Data 30+ days]\n    end\n\n    subgraph \"Monitoring\"\n        P[Prometheus] --&gt; G[Grafana]\n        P --&gt; A[Alerting]\n    end\n\n    style SSD fill:#e3f2fd,stroke:#3498db,stroke-width:2px\n    style HDD fill:#f3e5f5,stroke:#9b59b6,stroke-width:2px\n    style DB fill:#e8f5e8,stroke:#27ae60,stroke-width:2px\n    style U fill:#fff3e0,stroke:#f39c12,stroke-width:2px\n    style LB fill:#fce4ec,stroke:#e74c3c,stroke-width:2px</code></pre>"},{"location":"#sections","title":"Sections","text":""},{"location":"#installation","title":"&amp; Installation","text":"<ul> <li>Getting Started - Initial setup and prerequisites</li> <li>Hardware Configuration - Physical infrastructure setup</li> <li>Installation Guide - Step-by-step deployment</li> </ul>"},{"location":"#management","title":"&amp; Management","text":"<ul> <li>Application Configuration - Service configuration</li> <li>Database Management - TimescaleDB setup and tuning</li> <li>Security Configuration - Hardening and SSL setup</li> </ul>"},{"location":"#maintenance","title":"&amp; Maintenance","text":"<ul> <li>Daily Operations - Routine operational tasks</li> <li>Monitoring &amp; Alerting - System health monitoring</li> <li>Backup &amp; Recovery - Data protection procedures</li> </ul>"},{"location":"#automation","title":"&amp; Automation","text":"<ul> <li>Deployment Scripts - Automated deployment tools</li> <li>Maintenance Scripts - Automated maintenance tasks</li> <li>Monitoring Scripts - Health check automation</li> </ul>"},{"location":"#benefits","title":"Benefits","text":""},{"location":"#improvements","title":"Improvements","text":"Metric Previous (Cloud) On-Premises Improvement Concurrent Users 30 100+ 233% Response Time 200-500ms 50-200ms 60-75% Storage Capacity 20 GB 2.4+ TB 12,000% Database Performance 100 queries/sec 500+ queries/sec 400%"},{"location":"#features_1","title":"Features","text":"<ul> <li>Zero Data Loss: Complete backup and recovery procedures</li> <li>Automatic Scaling: TimescaleDB handles unlimited data growth</li> <li>Security Compliance: Enterprise-grade security hardening</li> <li>24/7 Monitoring: Comprehensive health monitoring and alerting</li> </ul>"},{"location":"#resources","title":"&amp; Resources","text":"<ul> <li>FAQ - Common questions and solutions</li> <li>Troubleshooting - Problem resolution guide</li> <li>Command Reference - Quick command lookup</li> <li>Support Contacts - Get help when needed</li> </ul>"},{"location":"#steps","title":"Steps","text":"<ol> <li>Review the Prerequisites to ensure your environment is ready</li> <li>Follow the Quick Start Guide for rapid deployment</li> <li>Configure Monitoring for production readiness</li> <li>Set up Automated Backups for data protection</li> </ol> <p>Production Ready</p> <p>This deployment guide has been tested and optimized for production environments with 100+ concurrent users and enterprise-grade requirements.</p> <p>Important</p> <p>Always test deployment procedures in a staging environment before applying to production systems.</p>"},{"location":"appendix/changelog/","title":"Changelog","text":""},{"location":"appendix/changelog/#overview","title":"Overview","text":"<p>This changelog documents all notable changes to the PRS on-premises deployment, including new features, improvements, bug fixes, and breaking changes.</p>"},{"location":"appendix/changelog/#version-210-2024-08-22","title":"Version 2.1.0 (2024-08-22)","text":""},{"location":"appendix/changelog/#new-features","title":"\ud83d\ude80 New Features","text":"<p>Dual Storage Architecture - Implemented automatic SSD/HDD tiering for optimal performance and capacity - Added TimescaleDB data movement policies for 30-day hot/cold storage - Configured automatic compression for data older than 7 days - Achieved 60-80% storage compression ratios</p> <p>Enhanced Performance - Increased concurrent user capacity to 100+ users (233% improvement) - Reduced API response times to 50-200ms (60-75% improvement) - Optimized database performance to 500+ queries per second (400% improvement) - Implemented intelligent caching with 98%+ cache hit ratios</p> <p>Zero-Deletion Data Policy - Implemented comprehensive data retention without deletion - Added automatic data archival to cold storage - Configured point-in-time recovery with WAL archiving - Ensured compliance with regulatory requirements</p>"},{"location":"appendix/changelog/#improvements","title":"\ud83d\udd27 Improvements","text":"<p>Infrastructure Enhancements - Upgraded to TimescaleDB with PostgreSQL 15 - Implemented RAID1 for SSD and RAID5 for HDD storage - Added comprehensive monitoring with Prometheus and Grafana - Enhanced SSL/TLS security configuration</p> <p>Operational Excellence - Added automated daily, weekly, and monthly maintenance procedures - Implemented comprehensive health check systems - Created automated backup and recovery procedures - Added performance monitoring and alerting</p> <p>Documentation - Created comprehensive deployment documentation (40+ guides) - Added troubleshooting procedures and FAQ - Implemented command reference and environment variable guide - Created maintenance and operations procedures</p>"},{"location":"appendix/changelog/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<p>Database Issues - Fixed TimescaleDB chunk management for large datasets - Resolved connection pool exhaustion under high load - Corrected index optimization for time-series queries - Fixed backup restoration procedures for encrypted backups</p> <p>Application Fixes - Resolved file upload issues for large files (50MB+) - Fixed session management with Redis clustering - Corrected API rate limiting configuration - Fixed frontend routing issues with custom domains</p> <p>Infrastructure Fixes - Resolved Docker container health check failures - Fixed SSL certificate auto-renewal procedures - Corrected storage permission issues - Fixed log rotation and cleanup procedures</p>"},{"location":"appendix/changelog/#breaking-changes","title":"\u26a0\ufe0f Breaking Changes","text":"<p>Configuration Changes - Updated environment variable structure (see migration guide) - Changed Docker Compose service names for consistency - Modified database connection parameters for TimescaleDB - Updated SSL certificate paths and configuration</p> <p>API Changes - Updated authentication endpoints for enhanced security - Modified file upload API for better error handling - Changed pagination parameters for consistency - Updated response formats for time-series data</p>"},{"location":"appendix/changelog/#migration-guide","title":"\ud83d\udccb Migration Guide","text":"<p>From Version 2.0.x</p> <ol> <li> <p>Backup Current System <pre><code>./scripts/backup-full.sh\n./scripts/backup-application-data.sh\n</code></pre></p> </li> <li> <p>Update Environment Configuration <pre><code># Update .env file with new variables\ncp .env .env.backup\n# Add new TimescaleDB configuration\n# Update storage paths for dual-tier architecture\n</code></pre></p> </li> <li> <p>Migrate Database <pre><code># Run migration scripts\n./scripts/migrate-to-timescaledb.sh\n./scripts/setup-dual-storage.sh\n</code></pre></p> </li> <li> <p>Update Docker Configuration <pre><code># Pull new images\ndocker-compose pull\n# Restart with new configuration\ndocker-compose up -d\n</code></pre></p> </li> <li> <p>Verify Migration <pre><code>./scripts/system-health-check.sh\n./scripts/verify-migration.sh\n</code></pre></p> </li> </ol>"},{"location":"appendix/changelog/#version-205-2024-07-15","title":"Version 2.0.5 (2024-07-15)","text":""},{"location":"appendix/changelog/#improvements_1","title":"\ud83d\udd27 Improvements","text":"<p>Security Enhancements - Updated SSL/TLS configuration for better security - Enhanced password policy enforcement - Improved session management security - Added security headers for web protection</p> <p>Performance Optimizations - Optimized database queries for better response times - Improved caching mechanisms - Enhanced file upload performance - Reduced memory usage in backend services</p>"},{"location":"appendix/changelog/#bug-fixes_1","title":"\ud83d\udc1b Bug Fixes","text":"<p>Critical Fixes - Fixed memory leak in background workers - Resolved database connection timeout issues - Corrected file permission problems - Fixed email notification delivery issues</p> <p>Minor Fixes - Improved error messages for better user experience - Fixed UI responsiveness on mobile devices - Corrected timezone handling in reports - Fixed export functionality for large datasets</p>"},{"location":"appendix/changelog/#version-200-2024-06-01","title":"Version 2.0.0 (2024-06-01)","text":""},{"location":"appendix/changelog/#major-release","title":"\ud83d\ude80 Major Release","text":"<p>Complete Architecture Overhaul - Migrated from monolithic to microservices architecture - Implemented Docker containerization for all services - Added Redis for caching and session management - Introduced comprehensive monitoring and logging</p> <p>New Features - Real-time notifications system - Advanced reporting and analytics - File upload and document management - API-first architecture with REST endpoints</p> <p>Enhanced Security - Implemented JWT-based authentication - Added role-based access control (RBAC) - Enhanced data encryption at rest and in transit - Comprehensive audit logging</p>"},{"location":"appendix/changelog/#breaking-changes_1","title":"\u26a0\ufe0f Breaking Changes","text":"<p>Database Schema - Complete database schema redesign - Migration required from version 1.x - New table structures for better performance - Updated relationships and constraints</p> <p>API Changes - New REST API endpoints - Updated authentication mechanisms - Changed response formats - New error handling structure</p>"},{"location":"appendix/changelog/#version-152-2024-04-10","title":"Version 1.5.2 (2024-04-10)","text":""},{"location":"appendix/changelog/#bug-fixes_2","title":"\ud83d\udc1b Bug Fixes","text":"<p>Critical Issues - Fixed data corruption issue in requisition processing - Resolved backup restoration failures - Corrected user permission inheritance problems - Fixed email template rendering issues</p> <p>Performance Issues - Improved query performance for large datasets - Reduced page load times - Optimized database indexes - Fixed memory usage spikes</p>"},{"location":"appendix/changelog/#version-150-2024-03-01","title":"Version 1.5.0 (2024-03-01)","text":""},{"location":"appendix/changelog/#new-features_1","title":"\ud83d\ude80 New Features","text":"<p>Workflow Enhancements - Added multi-level approval workflows - Implemented conditional approval routing - Added workflow templates - Enhanced notification system</p> <p>Reporting Improvements - New dashboard with real-time metrics - Advanced filtering and search capabilities - Export functionality for reports - Scheduled report generation</p>"},{"location":"appendix/changelog/#improvements_2","title":"\ud83d\udd27 Improvements","text":"<p>User Experience - Redesigned user interface - Improved mobile responsiveness - Enhanced search functionality - Better error handling and messages</p> <p>System Performance - Database query optimization - Improved caching mechanisms - Reduced server response times - Enhanced concurrent user support</p>"},{"location":"appendix/changelog/#version-100-2024-01-15","title":"Version 1.0.0 (2024-01-15)","text":""},{"location":"appendix/changelog/#initial-release","title":"\ud83c\udf89 Initial Release","text":"<p>Core Features - Requisition management system - Purchase order generation - Vendor management - User authentication and authorization - Basic reporting functionality</p> <p>System Requirements - PHP 7.4+ with MySQL 5.7+ - Apache/Nginx web server - Basic backup and recovery procedures - Manual deployment process</p>"},{"location":"appendix/changelog/#upgrade-instructions","title":"Upgrade Instructions","text":""},{"location":"appendix/changelog/#general-upgrade-process","title":"General Upgrade Process","text":"<ol> <li>Pre-Upgrade Checklist</li> <li>[ ] Create full system backup</li> <li>[ ] Review changelog for breaking changes</li> <li>[ ] Test upgrade in staging environment</li> <li>[ ] Schedule maintenance window</li> <li> <p>[ ] Notify users of planned downtime</p> </li> <li> <p>Backup Procedures <pre><code># Create comprehensive backup\n./scripts/backup-full.sh\n./scripts/backup-application-data.sh\n\n# Verify backup integrity\n./scripts/verify-backups.sh\n</code></pre></p> </li> <li> <p>Upgrade Execution <pre><code># Download new version\ngit fetch origin\ngit checkout v2.1.0\n\n# Run upgrade script\n./scripts/upgrade-system.sh\n\n# Verify upgrade\n./scripts/system-health-check.sh\n</code></pre></p> </li> <li> <p>Post-Upgrade Verification</p> </li> <li>[ ] Verify all services are running</li> <li>[ ] Test core functionality</li> <li>[ ] Check system performance</li> <li>[ ] Validate data integrity</li> <li>[ ] Confirm backup procedures</li> </ol>"},{"location":"appendix/changelog/#version-specific-upgrade-notes","title":"Version-Specific Upgrade Notes","text":""},{"location":"appendix/changelog/#upgrading-to-210","title":"Upgrading to 2.1.0","text":"<p>Prerequisites - Minimum 16GB RAM required - SSD storage for hot data tier - HDD storage for cold data tier - Docker 20.10+ and Docker Compose 2.0+</p> <p>Special Considerations - TimescaleDB migration requires extended downtime (2-4 hours) - Dual storage setup requires storage reconfiguration - New monitoring stack requires additional resources - SSL certificate configuration may need updates</p>"},{"location":"appendix/changelog/#upgrading-from-1x-to-20","title":"Upgrading from 1.x to 2.0","text":"<p>Major Migration Required - Complete database schema migration - Application architecture change - New deployment procedures - Updated configuration format</p> <p>Migration Time - Small deployments: 4-6 hours - Medium deployments: 8-12 hours - Large deployments: 12-24 hours</p>"},{"location":"appendix/changelog/#support-and-compatibility","title":"Support and Compatibility","text":""},{"location":"appendix/changelog/#supported-versions","title":"Supported Versions","text":"Version Support Status End of Life 2.1.x \u2705 Active TBD 2.0.x \u2705 Maintenance 2025-06-01 1.5.x \u26a0\ufe0f Security Only 2024-12-31 1.0.x \u274c End of Life 2024-06-01"},{"location":"appendix/changelog/#compatibility-matrix","title":"Compatibility Matrix","text":"Component Version 2.1.0 Version 2.0.x Version 1.5.x PostgreSQL 15+ 13+ 12+ TimescaleDB 2.11+ N/A N/A Redis 7.0+ 6.0+ N/A Docker 20.10+ 20.10+ N/A Node.js 18+ 16+ N/A PHP N/A N/A 7.4+ <p>Stay Updated</p> <p>Subscribe to release notifications to stay informed about new versions, security updates, and important announcements.</p> <p>Backup Before Upgrade</p> <p>Always create a complete backup before upgrading and test the upgrade process in a staging environment first.</p> <p>Version Support</p> <p>For questions about specific versions or upgrade assistance, consult the Support Guide for available resources and contact information.</p>"},{"location":"appendix/faq/","title":"Frequently Asked Questions","text":""},{"location":"appendix/faq/#general-questions","title":"General Questions","text":""},{"location":"appendix/faq/#what-is-the-prs-on-premises-deployment","title":"What is the PRS on-premises deployment?","text":"<p>A: The PRS (Procurement and Requisition System) on-premises deployment is a complete enterprise-grade solution that runs on your own hardware infrastructure. It provides:</p> <ul> <li>100+ concurrent user support (vs 30 on cloud)</li> <li>Dual storage architecture (SSD for performance, HDD for capacity)</li> <li>Zero-deletion data policy with TimescaleDB</li> <li>Enterprise security and compliance</li> <li>Complete data ownership and control</li> </ul>"},{"location":"appendix/faq/#what-are-the-main-benefits-over-cloud-deployment","title":"What are the main benefits over cloud deployment?","text":"<p>A: Key improvements include:</p> Metric Cloud On-Premises Improvement Concurrent Users 30 100+ 233% Response Time 200-500ms 50-200ms 60-75% Storage Capacity 20 GB 2.4+ TB 12,000% Database Performance 100 queries/sec 500+ queries/sec 400%"},{"location":"appendix/faq/#how-does-the-zero-deletion-policy-work","title":"How does the zero-deletion policy work?","text":"<p>A: The system never deletes data. Instead, it uses:</p> <ol> <li>Automatic Compression: Reduces storage by 60-80%</li> <li>Data Tiering: Moves old data from SSD to HDD automatically</li> <li>Transparent Access: Applications query data normally regardless of storage tier</li> <li>Compliance Ready: Meets regulatory requirements for data retention</li> </ol>"},{"location":"appendix/faq/#architecture-questions","title":"Architecture Questions","text":""},{"location":"appendix/faq/#how-does-the-dual-storage-system-work","title":"How does the dual storage system work?","text":"<p>A: The system automatically manages data across two storage tiers:</p> <ul> <li>SSD Storage (470GB): Hot data (0-30 days), fast access (&lt;50ms)</li> <li>HDD Storage (2.4TB): Cold data (30+ days), slower access (&lt;2s)</li> </ul> <p>TimescaleDB automatically moves data between tiers based on age and access patterns. Your application never needs to know which tier contains the data.</p>"},{"location":"appendix/faq/#what-happens-when-ssd-storage-fills-up","title":"What happens when SSD storage fills up?","text":"<p>A: The system has multiple automatic safeguards:</p> <ol> <li>Automatic Compression: Compresses data after 7-30 days (saves 60-80% space)</li> <li>Automatic Movement: Moves data older than 30 days to HDD</li> <li>Emergency Procedures: Manual compression and movement if needed</li> <li>Monitoring Alerts: Warns at 80% usage, critical at 90%</li> </ol>"},{"location":"appendix/faq/#can-i-scale-the-system-horizontally","title":"Can I scale the system horizontally?","text":"<p>A: Yes, the system supports several scaling options:</p> <ul> <li>Application Scaling: Multiple backend instances with load balancing</li> <li>Database Scaling: Read replicas for reporting workloads</li> <li>Storage Scaling: Additional SSD/HDD tiers</li> <li>Network Scaling: Link aggregation and 10Gbps upgrades</li> </ul>"},{"location":"appendix/faq/#technical-questions","title":"Technical Questions","text":""},{"location":"appendix/faq/#what-operating-systems-are-supported","title":"What operating systems are supported?","text":"<p>A: Supported operating systems include:</p> <ul> <li>Ubuntu 20.04 LTS or later (Recommended: Ubuntu 22.04 LTS)</li> <li>CentOS 8 or later</li> <li>RHEL 8 or later</li> <li>Debian 11 or later</li> </ul>"},{"location":"appendix/faq/#what-are-the-minimum-hardware-requirements","title":"What are the minimum hardware requirements?","text":"<p>A: Minimum requirements:</p> <ul> <li>CPU: 4 cores (8+ recommended)</li> <li>RAM: 16 GB (32 GB recommended)</li> <li>SSD: 470 GB RAID1 (1 TB recommended)</li> <li>HDD: 2.4 TB RAID5 (5+ TB recommended)</li> <li>Network: 1 Gbps (10 Gbps recommended)</li> </ul>"},{"location":"appendix/faq/#how-long-does-deployment-take","title":"How long does deployment take?","text":"<p>A: Deployment timeline:</p> <ul> <li>Environment Setup: 30 minutes</li> <li>Complete Deployment: 2-3 hours</li> <li>Testing and Validation: 1-2 hours</li> <li>Total: 4-6 hours for complete setup</li> </ul>"},{"location":"appendix/faq/#can-i-migrate-from-cloud-to-on-premises","title":"Can I migrate from cloud to on-premises?","text":"<p>A: Yes, migration is supported with:</p> <ol> <li>Database Export: Export cloud database to SQL</li> <li>File Transfer: Copy uploaded files and attachments</li> <li>Configuration Migration: Transfer settings and configurations</li> <li>Data Validation: Verify data integrity after migration</li> <li>Cutover: Switch DNS and go live</li> </ol>"},{"location":"appendix/faq/#security-questions","title":"Security Questions","text":""},{"location":"appendix/faq/#how-secure-is-the-on-premises-deployment","title":"How secure is the on-premises deployment?","text":"<p>A: Security features include:</p> <ul> <li>SSL/TLS Encryption: All communications encrypted</li> <li>Firewall Protection: Network-level security</li> <li>Access Controls: Role-based permissions</li> <li>Security Hardening: System-level security measures</li> <li>Audit Logging: Complete activity tracking</li> <li>Regular Updates: Security patches and updates</li> </ul>"},{"location":"appendix/faq/#how-are-passwords-and-secrets-managed","title":"How are passwords and secrets managed?","text":"<p>A: Security best practices:</p> <ul> <li>Auto-generated Secrets: Cryptographically secure random generation</li> <li>Environment Variables: Secrets stored in protected environment files</li> <li>No Hardcoded Passwords: All credentials configurable</li> <li>Regular Rotation: Automated password rotation capabilities</li> </ul>"},{"location":"appendix/faq/#what-compliance-standards-does-it-meet","title":"What compliance standards does it meet?","text":"<p>A: The system supports:</p> <ul> <li>Data Retention: Zero-deletion policy for compliance</li> <li>Audit Trails: Complete activity logging</li> <li>Access Controls: Role-based permissions</li> <li>Data Encryption: At-rest and in-transit encryption</li> <li>Backup Requirements: Automated backup procedures</li> </ul>"},{"location":"appendix/faq/#data-management-questions","title":"Data Management Questions","text":""},{"location":"appendix/faq/#how-does-backup-and-recovery-work","title":"How does backup and recovery work?","text":"<p>A: Comprehensive backup strategy:</p> <ul> <li>Daily Full Backups: Complete database backups</li> <li>Incremental Backups: Every 6 hours</li> <li>Real-time WAL: Continuous transaction log archiving</li> <li>File Backups: Daily upload and configuration backups</li> <li>Point-in-Time Recovery: Restore to any point in time</li> </ul>"},{"location":"appendix/faq/#what-happens-if-hardware-fails","title":"What happens if hardware fails?","text":"<p>A: Redundancy and recovery:</p> <ul> <li>RAID Protection: SSD RAID1 and HDD RAID5</li> <li>UPS Power: Uninterruptible power supply</li> <li>Automated Backups: Multiple backup copies</li> <li>Quick Recovery: Restore procedures documented</li> <li>Hardware Replacement: Hot-swappable components</li> </ul>"},{"location":"appendix/faq/#how-much-data-can-the-system-handle","title":"How much data can the system handle?","text":"<p>A: Scalability limits:</p> <ul> <li>Current Capacity: 2.4TB+ with room for expansion</li> <li>Daily Growth: 20GB+ sustainable</li> <li>User Capacity: 100+ concurrent users</li> <li>Transaction Volume: 500+ database queries/second</li> <li>Unlimited Expansion: Add more HDD storage as needed</li> </ul>"},{"location":"appendix/faq/#operations-questions","title":"Operations Questions","text":""},{"location":"appendix/faq/#what-maintenance-is-required","title":"What maintenance is required?","text":"<p>A: Maintenance schedule:</p> <ul> <li>Daily: Automated health checks and backups</li> <li>Weekly: Performance monitoring and log rotation</li> <li>Monthly: Security updates and capacity planning</li> <li>Quarterly: Full system maintenance and optimization</li> </ul>"},{"location":"appendix/faq/#how-do-i-monitor-system-health","title":"How do I monitor system health?","text":"<p>A: Monitoring tools:</p> <ul> <li>Grafana Dashboards: Real-time metrics and alerts</li> <li>Prometheus Metrics: System and application monitoring</li> <li>Health Check Scripts: Automated system validation</li> <li>Log Analysis: Centralized logging and analysis</li> <li>Email Alerts: Automated notification system</li> </ul>"},{"location":"appendix/faq/#what-if-i-need-support","title":"What if I need support?","text":"<p>A: Support options:</p> <ul> <li>Documentation: Comprehensive guides and references</li> <li>Health Checks: Automated problem detection</li> <li>Troubleshooting Guides: Step-by-step problem resolution</li> <li>Command Reference: Quick access to common commands</li> <li>Emergency Procedures: Critical issue resolution</li> </ul>"},{"location":"appendix/faq/#performance-questions","title":"Performance Questions","text":""},{"location":"appendix/faq/#how-fast-is-the-system-compared-to-cloud","title":"How fast is the system compared to cloud?","text":"<p>A: Performance improvements:</p> <ul> <li>Response Time: 60-75% faster (50-200ms vs 200-500ms)</li> <li>Database Queries: 400% faster (500+ vs 100 queries/sec)</li> <li>File Uploads: 400% faster (50 MB/s vs 10 MB/s)</li> <li>Concurrent Users: 233% more (100+ vs 30 users)</li> </ul>"},{"location":"appendix/faq/#what-causes-performance-issues","title":"What causes performance issues?","text":"<p>A: Common performance factors:</p> <ul> <li>High CPU Usage: &gt;85% sustained usage</li> <li>Memory Pressure: &gt;90% memory utilization</li> <li>Storage Full: SSD &gt;90% or HDD &gt;85%</li> <li>Network Congestion: &gt;90% bandwidth utilization</li> <li>Database Locks: Long-running queries or transactions</li> </ul>"},{"location":"appendix/faq/#how-can-i-optimize-performance","title":"How can I optimize performance?","text":"<p>A: Performance optimization:</p> <ul> <li>Resource Monitoring: Track CPU, memory, storage, network</li> <li>Query Optimization: Analyze and optimize slow database queries</li> <li>Index Management: Ensure proper database indexing</li> <li>Cache Tuning: Optimize Redis cache configuration</li> <li>Storage Management: Balance data across SSD/HDD tiers</li> </ul>"},{"location":"appendix/faq/#troubleshooting-questions","title":"Troubleshooting Questions","text":""},{"location":"appendix/faq/#services-wont-start-what-should-i-check","title":"Services won't start - what should I check?","text":"<p>A: Troubleshooting steps:</p> <ol> <li>Check Docker: <code>docker --version</code> and <code>docker-compose --version</code></li> <li>Check Logs: <code>docker-compose logs service-name</code></li> <li>Check Resources: <code>df -h</code> and <code>free -h</code></li> <li>Check Permissions: Verify storage directory permissions</li> <li>Check Configuration: Validate environment variables</li> </ol>"},{"location":"appendix/faq/#database-connection-fails-how-to-fix","title":"Database connection fails - how to fix?","text":"<p>A: Database troubleshooting:</p> <ol> <li>Check Status: <code>docker exec prs-onprem-postgres-timescale pg_isready</code></li> <li>Check Logs: <code>docker logs prs-onprem-postgres-timescale</code></li> <li>Check Connections: Verify connection pool settings</li> <li>Check Credentials: Validate database username/password</li> <li>Restart Service: <code>docker-compose restart postgres backend</code></li> </ol>"},{"location":"appendix/faq/#ssl-certificate-issues-how-to-resolve","title":"SSL certificate issues - how to resolve?","text":"<p>A: SSL troubleshooting:</p> <ol> <li>Check Certificate: <code>openssl x509 -in certificate.crt -text -noout</code></li> <li>Check Expiration: <code>openssl x509 -in certificate.crt -noout -dates</code></li> <li>Regenerate Certificate: <code>./scripts/ssl-automation-citylandcondo.sh --force</code></li> <li>Check Nginx Config: Verify SSL configuration in nginx</li> <li>Restart Nginx: <code>docker-compose restart nginx</code></li> </ol>"},{"location":"appendix/faq/#system-running-slowly-how-to-diagnose","title":"System running slowly - how to diagnose?","text":"<p>A: Performance diagnosis:</p> <ol> <li>Check Resources: <code>htop</code>, <code>free -h</code>, <code>df -h</code></li> <li>Check Containers: <code>docker stats</code></li> <li>Check Database: Look for slow queries and locks</li> <li>Check Network: Monitor bandwidth and connections</li> <li>Check Logs: Look for errors and warnings</li> </ol>"},{"location":"appendix/faq/#scaling-questions","title":"Scaling Questions","text":""},{"location":"appendix/faq/#when-should-i-consider-scaling","title":"When should I consider scaling?","text":"<p>A: Scaling indicators:</p> <ul> <li>CPU Usage: Consistently &gt;70%</li> <li>Memory Usage: Consistently &gt;80%</li> <li>Storage Usage: SSD &gt;80% or HDD &gt;70%</li> <li>Response Time: &gt;500ms average</li> <li>User Complaints: Performance issues reported</li> </ul>"},{"location":"appendix/faq/#how-do-i-add-more-storage","title":"How do I add more storage?","text":"<p>A: Storage expansion:</p> <ol> <li>SSD Expansion: Add drives to RAID1 array</li> <li>HDD Expansion: Add drives to RAID5 array</li> <li>New Tiers: Create additional storage tiers</li> <li>Update Configuration: Modify Docker volumes</li> <li>Test Performance: Validate improved performance</li> </ol>"},{"location":"appendix/faq/#can-i-add-more-servers","title":"Can I add more servers?","text":"<p>A: Horizontal scaling:</p> <ol> <li>Load Balancer: Configure nginx load balancing</li> <li>Application Servers: Deploy additional backend instances</li> <li>Database Replicas: Set up read replicas for reporting</li> <li>Shared Storage: Configure shared file storage</li> <li>Session Management: Use Redis for session sharing</li> </ol> <p>Getting Help</p> <p>If you can't find the answer to your question here, check the Troubleshooting Guide or Support section.</p> <p>Documentation Updates</p> <p>This FAQ is regularly updated based on common questions and issues. Suggest improvements through the support channels.</p>"},{"location":"appendix/glossary/","title":"Glossary","text":""},{"location":"appendix/glossary/#overview","title":"Overview","text":"<p>This glossary provides definitions for technical terms, acronyms, and concepts used throughout the PRS on-premises deployment documentation.</p>"},{"location":"appendix/glossary/#a","title":"A","text":"<p>API (Application Programming Interface) : A set of protocols and tools for building software applications. In PRS, the REST API allows external systems to interact with the procurement system.</p> <p>Audit Log : A chronological record of system activities and user actions for security and compliance purposes. PRS maintains comprehensive audit logs for all transactions.</p> <p>Authentication : The process of verifying the identity of a user or system. PRS supports multiple authentication methods including local accounts and external identity providers.</p> <p>Authorization : The process of determining what actions an authenticated user is allowed to perform. PRS implements role-based access control (RBAC).</p> <p>Automated Backup : Scheduled backup processes that run without manual intervention. PRS includes daily automated backups with verification.</p>"},{"location":"appendix/glossary/#b","title":"B","text":"<p>Backend : The server-side component of the application that handles business logic, database operations, and API endpoints. Built with Node.js.</p> <p>Backup Retention : The policy defining how long backup files are kept before deletion. PRS maintains 30 days of local backups and 90 days of offsite backups.</p> <p>Business Logic : The core functionality and rules that define how the procurement system operates, including approval workflows and validation rules.</p>"},{"location":"appendix/glossary/#c","title":"C","text":"<p>Cache Hit Ratio : The percentage of data requests served from cache rather than the database. A higher ratio indicates better performance.</p> <p>Chunk : In TimescaleDB, a partition of a hypertable that contains data for a specific time range. Chunks enable efficient data management and querying.</p> <p>Cold Storage : Storage tier for infrequently accessed data, typically on slower but higher-capacity drives (HDD in PRS deployment).</p> <p>Compression : The process of reducing data size to save storage space. TimescaleDB provides automatic compression for older data chunks.</p> <p>Container : A lightweight, portable unit that packages an application and its dependencies. PRS uses Docker containers for all services.</p> <p>CORS (Cross-Origin Resource Sharing) : A security mechanism that allows web applications to make requests to different domains. Configured in PRS for secure frontend-backend communication.</p>"},{"location":"appendix/glossary/#d","title":"D","text":"<p>Data Movement : The automatic process of moving data between storage tiers based on age and access patterns. PRS moves data from SSD to HDD after 30 days.</p> <p>Data Retention : Policies defining how long different types of data are kept in the system. PRS implements a zero-deletion policy with automatic archival.</p> <p>Database Migration : The process of updating database schema or moving data between database versions. Includes scripts to modify table structures and data.</p> <p>Docker Compose : A tool for defining and running multi-container Docker applications using YAML configuration files.</p> <p>Dual Storage Architecture : PRS's storage strategy using both SSD (fast) and HDD (capacity) storage tiers for optimal performance and cost efficiency.</p>"},{"location":"appendix/glossary/#e","title":"E","text":"<p>Environment Variables : Configuration values stored outside the application code, used to customize behavior for different deployment environments.</p> <p>ETL (Extract, Transform, Load) : The process of extracting data from source systems, transforming it to fit operational needs, and loading it into the target system.</p>"},{"location":"appendix/glossary/#f","title":"F","text":"<p>Failover : The automatic switching to a backup system when the primary system fails. PRS includes failover capabilities for high availability.</p> <p>Frontend : The client-side component of the application that users interact with directly. Built with React and Vite.</p> <p>Full Backup : A complete backup of all data in the system. PRS performs daily full backups for comprehensive data protection.</p>"},{"location":"appendix/glossary/#g","title":"G","text":"<p>Grafana : An open-source analytics and monitoring platform used in PRS for creating dashboards and visualizing system metrics.</p> <p>GPU (Graphics Processing Unit) : Specialized hardware for parallel processing. Not required for PRS deployment but can accelerate certain data processing tasks.</p>"},{"location":"appendix/glossary/#h","title":"H","text":"<p>Health Check : Automated tests that verify system components are functioning correctly. PRS includes comprehensive health checks for all services.</p> <p>Hot Storage : Storage tier for frequently accessed data, typically on faster drives (SSD in PRS deployment).</p> <p>Hypertable : TimescaleDB's abstraction for time-series data that automatically partitions data across multiple chunks for efficient querying.</p>"},{"location":"appendix/glossary/#i","title":"I","text":"<p>Incremental Backup : A backup that only includes data changed since the last backup. More efficient than full backups for frequent backup schedules.</p> <p>Index : A database structure that improves query performance by creating shortcuts to data. PRS uses optimized indexes for common query patterns.</p> <p>Infrastructure as Code (IaC) : The practice of managing infrastructure through code rather than manual processes. PRS deployment scripts follow IaC principles.</p>"},{"location":"appendix/glossary/#j","title":"J","text":"<p>JWT (JSON Web Token) : A secure method for transmitting information between parties. Used in PRS for user authentication and session management.</p>"},{"location":"appendix/glossary/#k","title":"K","text":"<p>Kubernetes : Container orchestration platform. While PRS uses Docker Compose, it can be adapted for Kubernetes deployment.</p>"},{"location":"appendix/glossary/#l","title":"L","text":"<p>Load Balancer : A system that distributes incoming requests across multiple servers. Nginx serves as the load balancer in PRS deployment.</p> <p>Log Rotation : The process of archiving and managing log files to prevent disk space issues. PRS implements automatic log rotation.</p>"},{"location":"appendix/glossary/#m","title":"M","text":"<p>Microservices : An architectural approach where applications are built as a collection of small, independent services. PRS uses a microservices architecture.</p> <p>Migration : The process of moving from one system version to another, including data and schema changes.</p> <p>Monitoring : The continuous observation of system performance and health. PRS includes comprehensive monitoring with Prometheus and Grafana.</p>"},{"location":"appendix/glossary/#n","title":"N","text":"<p>Nginx : A web server and reverse proxy used in PRS for handling HTTP requests, SSL termination, and load balancing.</p> <p>Node.js : A JavaScript runtime used for building the PRS backend services.</p>"},{"location":"appendix/glossary/#o","title":"O","text":"<p>On-Premises : Software deployed and run on the organization's own hardware and infrastructure, as opposed to cloud-based deployment.</p> <p>Orchestration : The automated coordination of multiple services or processes. Docker Compose orchestrates PRS services.</p>"},{"location":"appendix/glossary/#p","title":"P","text":"<p>Point-in-Time Recovery (PITR) : The ability to restore a database to a specific moment in time. Enabled through WAL archiving in PRS.</p> <p>PostgreSQL : An open-source relational database system used as the foundation for TimescaleDB in PRS.</p> <p>Prometheus : An open-source monitoring and alerting system used in PRS for collecting and storing metrics.</p> <p>Procurement : The process of acquiring goods and services. PRS is a system designed to manage and automate procurement workflows.</p>"},{"location":"appendix/glossary/#q","title":"Q","text":"<p>Query Optimization : The process of improving database query performance through better indexing, query structure, and execution plans.</p> <p>Queue : A data structure for managing tasks in order. PRS uses Redis queues for background job processing.</p>"},{"location":"appendix/glossary/#r","title":"R","text":"<p>RAID (Redundant Array of Independent Disks) : A storage technology that combines multiple drives for improved performance and/or redundancy. PRS uses RAID1 for SSD and RAID5 for HDD.</p> <p>Redis : An in-memory data store used in PRS for caching and session management.</p> <p>Requisition : A formal request for goods or services within an organization. The core entity managed by PRS.</p> <p>REST API : A web service architecture style used by PRS for communication between frontend and backend components.</p> <p>Rollback : The process of reverting to a previous version of software or data after a failed update or deployment.</p>"},{"location":"appendix/glossary/#s","title":"S","text":"<p>Scaling : The process of adjusting system capacity to handle changing load. Can be vertical (more powerful hardware) or horizontal (more servers).</p> <p>SSL/TLS : Security protocols for encrypting data transmission. PRS uses SSL/TLS for all web communications.</p> <p>SSD (Solid State Drive) : Fast storage technology used in PRS for hot data storage and high-performance operations.</p>"},{"location":"appendix/glossary/#t","title":"T","text":"<p>TimescaleDB : A time-series database built on PostgreSQL, used in PRS for efficient storage and querying of time-based data.</p> <p>Throughput : The amount of work performed in a given time period. Measured in requests per second for web applications.</p> <p>Two-Factor Authentication (2FA) : A security method requiring two different authentication factors. Optional feature in PRS for enhanced security.</p>"},{"location":"appendix/glossary/#u","title":"U","text":"<p>Uptime : The amount of time a system is operational and available. PRS is designed for high uptime with redundancy and monitoring.</p> <p>User Interface (UI) : The visual elements and controls that users interact with. PRS provides a modern web-based UI.</p>"},{"location":"appendix/glossary/#v","title":"V","text":"<p>VACUUM : A PostgreSQL operation that reclaims storage space and updates statistics. Part of regular PRS maintenance procedures.</p> <p>Virtual Machine (VM) : A software emulation of a computer system. PRS can be deployed on VMs or physical hardware.</p> <p>Volume : In Docker, a mechanism for persisting data generated and used by containers. PRS uses volumes for database and file storage.</p>"},{"location":"appendix/glossary/#w","title":"W","text":"<p>WAL (Write-Ahead Logging) : A database technique where changes are logged before being applied. Used in PRS for point-in-time recovery.</p> <p>Webhook : A method for applications to provide real-time information to other applications. PRS supports webhooks for external integrations.</p> <p>Workflow : A sequence of processes through which work passes. PRS manages procurement workflows including approvals and processing.</p>"},{"location":"appendix/glossary/#x","title":"X","text":"<p>XML : A markup language for storing and transporting data. Some PRS integrations may use XML format for data exchange.</p>"},{"location":"appendix/glossary/#y","title":"Y","text":"<p>YAML : A human-readable data serialization standard. Used in PRS for Docker Compose configuration files.</p>"},{"location":"appendix/glossary/#z","title":"Z","text":"<p>Zero-Deletion Policy : PRS's data retention strategy where no data is permanently deleted, only archived to different storage tiers.</p> <p>Zone : In networking and security contexts, a logical grouping of resources with similar security requirements.</p>"},{"location":"appendix/glossary/#acronyms-and-abbreviations","title":"Acronyms and Abbreviations","text":"Acronym Full Form Description API Application Programming Interface Interface for software communication CPU Central Processing Unit Main processor of a computer CRUD Create, Read, Update, Delete Basic database operations CSV Comma-Separated Values File format for data exchange DNS Domain Name System System for translating domain names to IP addresses GPU Graphics Processing Unit Specialized processor for parallel computing HDD Hard Disk Drive Traditional magnetic storage device HTTP Hypertext Transfer Protocol Protocol for web communication HTTPS HTTP Secure Encrypted version of HTTP I/O Input/Output Data transfer operations JSON JavaScript Object Notation Data interchange format JWT JSON Web Token Token-based authentication standard LDAP Lightweight Directory Access Protocol Directory service protocol NGINX Engine X Web server and reverse proxy OS Operating System System software managing hardware PDF Portable Document Format File format for documents PITR Point-in-Time Recovery Database recovery to specific time PRS Procurement and Requisition System The application system QPS Queries Per Second Database performance metric RAID Redundant Array of Independent Disks Storage redundancy technology RAM Random Access Memory Computer memory RBAC Role-Based Access Control Access control method REST Representational State Transfer Web service architecture SLA Service Level Agreement Performance guarantee SQL Structured Query Language Database query language SSD Solid State Drive Flash-based storage device SSL Secure Sockets Layer Encryption protocol TLS Transport Layer Security Successor to SSL UI User Interface User interaction layer UPS Uninterruptible Power Supply Backup power system URL Uniform Resource Locator Web address UUID Universally Unique Identifier Unique identifier standard VM Virtual Machine Virtualized computer system VPN Virtual Private Network Secure network connection WAL Write-Ahead Logging Database logging technique XML eXtensible Markup Language Markup language for data YAML YAML Ain't Markup Language Data serialization standard <p>Quick Reference</p> <p>Use Ctrl+F (or Cmd+F on Mac) to quickly search for specific terms in this glossary.</p> <p>Comprehensive Coverage</p> <p>This glossary covers all major technical terms used in the PRS on-premises deployment documentation and operations.</p>"},{"location":"appendix/support/","title":"Support","text":""},{"location":"appendix/support/#overview","title":"Overview","text":"<p>This guide provides support resources, contact information, and procedures for getting help with the PRS on-premises deployment.</p>"},{"location":"appendix/support/#self-service-resources","title":"Self-Service Resources","text":""},{"location":"appendix/support/#documentation","title":"Documentation","text":"<p>Primary Documentation - Getting Started - Initial setup and requirements - Installation Guide - Complete deployment procedures - Operations Guide - Daily maintenance and monitoring - Troubleshooting - Problem resolution procedures</p> <p>Quick References - Command Reference - Common commands and operations - FAQ - Frequently asked questions and solutions - Configuration Reference - Complete configuration guide</p>"},{"location":"appendix/support/#diagnostic-tools","title":"Diagnostic Tools","text":""},{"location":"appendix/support/#system-health-check","title":"System Health Check","text":"<pre><code># Run comprehensive health check\ncd /opt/prs-deployment/scripts\n./system-health-check.sh\n\n# Check specific components\n./system-health-check.sh --component database\n./system-health-check.sh --component storage\n./system-health-check.sh --component network\n</code></pre>"},{"location":"appendix/support/#log-analysis","title":"Log Analysis","text":"<pre><code># Application logs\ndocker logs prs-onprem-backend --tail 100\ndocker logs prs-onprem-frontend --tail 100\ndocker logs prs-onprem-nginx --tail 100\n\n# System logs\nsudo journalctl -u docker --since \"1 hour ago\"\nsudo tail -f /var/log/syslog\n\n# Database logs\ndocker logs prs-onprem-postgres-timescale --tail 50\n</code></pre>"},{"location":"appendix/support/#performance-analysis","title":"Performance Analysis","text":"<pre><code># System performance\nhtop\niostat -x 1\niotop\n\n# Container performance\ndocker stats\n\n# Database performance\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT query, calls, total_time, mean_time \nFROM pg_stat_statements \nORDER BY total_time DESC \nLIMIT 10;\n\"\n</code></pre>"},{"location":"appendix/support/#issue-reporting","title":"Issue Reporting","text":""},{"location":"appendix/support/#before-reporting-an-issue","title":"Before Reporting an Issue","text":"<ol> <li>Check Documentation</li> <li>Review relevant documentation sections</li> <li>Check the FAQ for similar issues</li> <li> <p>Follow troubleshooting procedures</p> </li> <li> <p>Gather Information</p> </li> <li>Run system health check</li> <li>Collect relevant logs</li> <li>Note error messages and timestamps</li> <li> <p>Document steps to reproduce the issue</p> </li> <li> <p>Attempt Basic Resolution</p> </li> <li>Restart affected services</li> <li>Check system resources</li> <li>Verify configuration settings</li> </ol>"},{"location":"appendix/support/#issue-report-template","title":"Issue Report Template","text":"<p>When reporting an issue, please include the following information:</p> <p><pre><code>## Issue Summary\nBrief description of the problem\n\n## Environment Information\n- OS Version: Ubuntu 22.04 LTS\n- Docker Version: 20.10.x\n- PRS Version: [version]\n- Deployment Date: [date]\n\n## Steps to Reproduce\n1. Step one\n2. Step two\n3. Step three\n\n## Expected Behavior\nWhat should happen\n\n## Actual Behavior\nWhat actually happens\n\n## Error Messages\n</code></pre> [Include exact error messages] <pre><code>## System Information\n</code></pre> [Output from system-health-check.sh] <pre><code>## Logs\n</code></pre> [Relevant log entries with timestamps] <pre><code>## Additional Context\nAny other relevant information\n</code></pre></p>"},{"location":"appendix/support/#log-collection-script","title":"Log Collection Script","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/collect-support-info.sh\n\nSUPPORT_DIR=\"/tmp/prs-support-$(date +%Y%m%d_%H%M%S)\"\nmkdir -p \"$SUPPORT_DIR\"\n\necho \"Collecting PRS support information...\"\n\n# System information\necho \"=== System Information ===\" &gt; \"$SUPPORT_DIR/system-info.txt\"\nuname -a &gt;&gt; \"$SUPPORT_DIR/system-info.txt\"\nlsb_release -a &gt;&gt; \"$SUPPORT_DIR/system-info.txt\" 2&gt;/dev/null\nfree -h &gt;&gt; \"$SUPPORT_DIR/system-info.txt\"\ndf -h &gt;&gt; \"$SUPPORT_DIR/system-info.txt\"\n\n# Docker information\necho \"=== Docker Information ===\" &gt; \"$SUPPORT_DIR/docker-info.txt\"\ndocker --version &gt;&gt; \"$SUPPORT_DIR/docker-info.txt\"\ndocker-compose --version &gt;&gt; \"$SUPPORT_DIR/docker-info.txt\"\ndocker ps &gt;&gt; \"$SUPPORT_DIR/docker-info.txt\"\ndocker stats --no-stream &gt;&gt; \"$SUPPORT_DIR/docker-info.txt\"\n\n# Service logs\necho \"Collecting service logs...\"\ndocker logs prs-onprem-backend --tail 500 &gt; \"$SUPPORT_DIR/backend.log\" 2&gt;&amp;1\ndocker logs prs-onprem-frontend --tail 500 &gt; \"$SUPPORT_DIR/frontend.log\" 2&gt;&amp;1\ndocker logs prs-onprem-nginx --tail 500 &gt; \"$SUPPORT_DIR/nginx.log\" 2&gt;&amp;1\ndocker logs prs-onprem-postgres-timescale --tail 500 &gt; \"$SUPPORT_DIR/postgres.log\" 2&gt;&amp;1\ndocker logs prs-onprem-redis --tail 500 &gt; \"$SUPPORT_DIR/redis.log\" 2&gt;&amp;1\n\n# System logs\necho \"Collecting system logs...\"\nsudo journalctl -u docker --since \"24 hours ago\" &gt; \"$SUPPORT_DIR/docker-system.log\"\nsudo tail -1000 /var/log/syslog &gt; \"$SUPPORT_DIR/syslog.log\"\n\n# Configuration (sanitized)\necho \"Collecting configuration...\"\ncp 02-docker-configuration/.env \"$SUPPORT_DIR/env-config.txt\"\nsed -i 's/PASSWORD=.*/PASSWORD=***REDACTED***/g' \"$SUPPORT_DIR/env-config.txt\"\nsed -i 's/SECRET=.*/SECRET=***REDACTED***/g' \"$SUPPORT_DIR/env-config.txt\"\n\n# Health check\necho \"Running health check...\"\n./system-health-check.sh &gt; \"$SUPPORT_DIR/health-check.txt\" 2&gt;&amp;1\n\n# Create archive\ntar -czf \"${SUPPORT_DIR}.tar.gz\" -C /tmp \"$(basename \"$SUPPORT_DIR\")\"\nrm -rf \"$SUPPORT_DIR\"\n\necho \"Support information collected: ${SUPPORT_DIR}.tar.gz\"\necho \"Please attach this file to your support request.\"\n</code></pre>"},{"location":"appendix/support/#support-channels","title":"Support Channels","text":""},{"location":"appendix/support/#internal-support","title":"Internal Support","text":""},{"location":"appendix/support/#system-administrator","title":"System Administrator","text":"<ul> <li>Primary Contact: System Administrator</li> <li>Scope: System-level issues, hardware problems, network connectivity</li> <li>Response Time: 4 hours during business hours</li> <li>Escalation: IT Manager</li> </ul>"},{"location":"appendix/support/#database-administrator","title":"Database Administrator","text":"<ul> <li>Primary Contact: Database Administrator</li> <li>Scope: Database performance, backup/recovery, data integrity</li> <li>Response Time: 2 hours for critical issues</li> <li>Escalation: Senior DBA</li> </ul>"},{"location":"appendix/support/#application-support","title":"Application Support","text":"<ul> <li>Primary Contact: Development Team Lead</li> <li>Scope: Application functionality, user interface issues, business logic</li> <li>Response Time: 8 hours during business hours</li> <li>Escalation: Technical Manager</li> </ul>"},{"location":"appendix/support/#external-support","title":"External Support","text":""},{"location":"appendix/support/#vendor-support","title":"Vendor Support","text":"<p>TimescaleDB Support - Contact: TimescaleDB Community Forum - Website: https://github.com/timescale/timescaledb/issues - Scope: TimescaleDB-specific issues, performance optimization - Documentation: https://docs.timescale.com/</p> <p>Docker Support - Contact: Docker Community Forum - Website: https://forums.docker.com/ - Scope: Docker engine issues, container problems - Documentation: https://docs.docker.com/</p> <p>Ubuntu Support - Contact: Ubuntu Community Support - Website: https://askubuntu.com/ - Scope: Operating system issues, package management - Documentation: https://help.ubuntu.com/</p>"},{"location":"appendix/support/#professional-services","title":"Professional Services","text":"<p>Infrastructure Consulting - Scope: Performance optimization, scaling, architecture review - Engagement: Project-based consulting - Deliverables: Performance reports, optimization recommendations</p> <p>Security Auditing - Scope: Security assessment, compliance review, penetration testing - Engagement: Annual security audit - Deliverables: Security report, remediation plan</p>"},{"location":"appendix/support/#emergency-procedures","title":"Emergency Procedures","text":""},{"location":"appendix/support/#critical-issue-response","title":"Critical Issue Response","text":""},{"location":"appendix/support/#severity-levels","title":"Severity Levels","text":"<p>Critical (Severity 1) - System completely down - Data loss or corruption - Security breach - Response Time: Immediate (within 1 hour) - Escalation: Immediate notification to all stakeholders</p> <p>High (Severity 2) - Major functionality impaired - Performance severely degraded - Backup failures - Response Time: 4 hours during business hours - Escalation: Notification to management within 2 hours</p> <p>Medium (Severity 3) - Minor functionality issues - Performance degradation - Non-critical errors - Response Time: 24 hours during business hours - Escalation: Weekly status report</p> <p>Low (Severity 4) - Enhancement requests - Documentation updates - Minor configuration changes - Response Time: 5 business days - Escalation: Monthly review</p>"},{"location":"appendix/support/#emergency-contacts","title":"Emergency Contacts","text":"<pre><code>Primary On-Call: [Phone Number]\nSecondary On-Call: [Phone Number]\nManagement Escalation: [Phone Number]\nSecurity Team: [Phone Number]\n</code></pre>"},{"location":"appendix/support/#emergency-recovery-procedures","title":"Emergency Recovery Procedures","text":""},{"location":"appendix/support/#system-recovery","title":"System Recovery","text":"<pre><code># Emergency system restart\nsudo systemctl restart docker\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml down\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml up -d\n\n# Emergency database recovery\n./scripts/restore-database.sh /mnt/hdd/postgres-backups/daily/latest-backup.sql\n\n# Emergency storage cleanup\nsudo find /mnt/ssd -name \"*.tmp\" -delete\nsudo find /mnt/ssd/logs -name \"*.log\" -mtime +1 -exec gzip {} \\;\n</code></pre>"},{"location":"appendix/support/#data-recovery","title":"Data Recovery","text":"<pre><code># Point-in-time recovery\n./scripts/restore-database.sh --point-in-time \"2024-08-22 14:30:00\"\n\n# File recovery from backup\nrsync -av /mnt/hdd/file-backups/latest/ /mnt/ssd/uploads/\n\n# Configuration recovery\ncp /mnt/hdd/config-backups/latest/docker-configuration/ 02-docker-configuration/\n</code></pre>"},{"location":"appendix/support/#knowledge-base","title":"Knowledge Base","text":""},{"location":"appendix/support/#common-solutions","title":"Common Solutions","text":""},{"location":"appendix/support/#performance-issues","title":"Performance Issues","text":"<ol> <li>High CPU Usage</li> <li>Check for runaway processes</li> <li>Restart affected containers</li> <li> <p>Review database queries</p> </li> <li> <p>Memory Issues</p> </li> <li>Clear system cache</li> <li>Restart memory-intensive services</li> <li> <p>Check for memory leaks</p> </li> <li> <p>Storage Issues</p> </li> <li>Clean temporary files</li> <li>Compress old logs</li> <li>Move data to appropriate tier</li> </ol>"},{"location":"appendix/support/#connectivity-issues","title":"Connectivity Issues","text":"<ol> <li>Network Problems</li> <li>Check firewall rules</li> <li>Verify DNS resolution</li> <li> <p>Test network connectivity</p> </li> <li> <p>SSL Issues</p> </li> <li>Verify certificate validity</li> <li>Check certificate permissions</li> <li> <p>Renew expired certificates</p> </li> <li> <p>Database Connectivity</p> </li> <li>Check database status</li> <li>Verify connection pool</li> <li>Test network connectivity</li> </ol>"},{"location":"appendix/support/#best-practices","title":"Best Practices","text":""},{"location":"appendix/support/#preventive-maintenance","title":"Preventive Maintenance","text":"<ul> <li>Run daily health checks</li> <li>Monitor system resources</li> <li>Keep backups current</li> <li>Apply security updates regularly</li> </ul>"},{"location":"appendix/support/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Monitor database performance</li> <li>Optimize queries regularly</li> <li>Balance storage tiers</li> <li>Review system metrics</li> </ul>"},{"location":"appendix/support/#security-maintenance","title":"Security Maintenance","text":"<ul> <li>Update security configurations</li> <li>Review access logs</li> <li>Monitor for vulnerabilities</li> <li>Conduct security audits</li> </ul>"},{"location":"appendix/support/#training-and-documentation","title":"Training and Documentation","text":""},{"location":"appendix/support/#training-resources","title":"Training Resources","text":""},{"location":"appendix/support/#administrator-training","title":"Administrator Training","text":"<ul> <li>System Administration: Linux, Docker, networking</li> <li>Database Management: PostgreSQL, TimescaleDB, backup/recovery</li> <li>Security: Hardening, monitoring, incident response</li> <li>Monitoring: Grafana, Prometheus, alerting</li> </ul>"},{"location":"appendix/support/#user-training","title":"User Training","text":"<ul> <li>Application Usage: PRS functionality, workflows</li> <li>Troubleshooting: Basic problem resolution</li> <li>Reporting: Issue reporting procedures</li> </ul>"},{"location":"appendix/support/#documentation-maintenance","title":"Documentation Maintenance","text":""},{"location":"appendix/support/#regular-updates","title":"Regular Updates","text":"<ul> <li>Monthly documentation review</li> <li>Quarterly procedure updates</li> <li>Annual comprehensive review</li> <li>Version control for all changes</li> </ul>"},{"location":"appendix/support/#feedback-process","title":"Feedback Process","text":"<ul> <li>User feedback collection</li> <li>Documentation improvement suggestions</li> <li>Regular usability reviews</li> <li>Continuous improvement process</li> </ul> <p>Support Available</p> <p>Comprehensive support resources are available to ensure smooth operation of your PRS deployment.</p> <p>Proactive Support</p> <p>Use monitoring tools and regular health checks to identify and resolve issues before they impact users.</p> <p>Emergency Preparedness</p> <p>Ensure all team members are familiar with emergency procedures and have access to necessary contact information.</p>"},{"location":"configuration/application/","title":"Application Configuration","text":""},{"location":"configuration/application/#overview","title":"Overview","text":"<p>This guide covers the complete application configuration for the PRS on-premises deployment, including environment variables, service settings, and performance tuning.</p>"},{"location":"configuration/application/#configuration-structure","title":"Configuration Structure","text":"<pre><code>02-docker-configuration/\n\u251c\u2500\u2500 .env                          # Main environment file\n\u251c\u2500\u2500 .env.example                  # Environment template\n\u251c\u2500\u2500 docker-compose.onprem.yml     # Docker services configuration\n\u251c\u2500\u2500 nginx/                        # Nginx configuration\n\u2502   \u251c\u2500\u2500 nginx.conf               # Main nginx config\n\u2502   \u2514\u2500\u2500 sites-enabled/           # Virtual host configs\n\u251c\u2500\u2500 config/                       # Service configurations\n\u2502   \u251c\u2500\u2500 grafana/                 # Grafana dashboards and settings\n\u2502   \u251c\u2500\u2500 prometheus/              # Prometheus rules and config\n\u2502   \u2514\u2500\u2500 postgres/                # PostgreSQL initialization scripts\n\u2514\u2500\u2500 ssl/                          # SSL certificates\n    \u251c\u2500\u2500 certificate.crt\n    \u251c\u2500\u2500 private.key\n    \u2514\u2500\u2500 ca-bundle.crt\n</code></pre>"},{"location":"configuration/application/#environment-configuration","title":"Environment Configuration","text":""},{"location":"configuration/application/#environment-variables","title":"Environment Variables","text":""},{"location":"configuration/application/#and-network-settings","title":"and Network Settings","text":"<pre><code># Domain Configuration\nDOMAIN=your-domain.com\nSERVER_IP=192.168.0.100\nNETWORK_SUBNET=192.168.0.0/20\nNETWORK_GATEWAY=192.168.0.1\n\n# SSL Configuration\nSSL_EMAIL=admin@your-domain.com\nENABLE_SSL=true\nSSL_CERT_PATH=./ssl/certificate.crt\nSSL_KEY_PATH=./ssl/private.key\nSSL_CA_PATH=./ssl/ca-bundle.crt\n</code></pre>"},{"location":"configuration/application/#configuration","title":"Configuration","text":"<pre><code># PostgreSQL Settings\nPOSTGRES_DB=prs_production\nPOSTGRES_USER=prs_admin\nPOSTGRES_PASSWORD=your_secure_password_here\nPOSTGRES_PORT=5432\nPOSTGRES_HOST=postgres\n\n# Database Performance Settings\nPOSTGRES_MAX_CONNECTIONS=150\nPOSTGRES_SHARED_BUFFERS=2GB\nPOSTGRES_EFFECTIVE_CACHE_SIZE=4GB\nPOSTGRES_WORK_MEM=32MB\nPOSTGRES_MAINTENANCE_WORK_MEM=512MB\n\n# TimescaleDB Settings\nTIMESCALEDB_TELEMETRY=off\nTIMESCALEDB_MAX_BACKGROUND_WORKERS=16\n</code></pre>"},{"location":"configuration/application/#secrets","title":"Secrets","text":"<pre><code># Generate secure secrets\nJWT_SECRET=$(openssl rand -base64 32)\nENCRYPTION_KEY=$(openssl rand -base64 32)\nOTP_KEY=$(openssl rand -base64 16)\nPASS_SECRET=$(openssl rand -base64 32)\nSESSION_SECRET=$(openssl rand -base64 32)\n\n# API Keys\nAPI_KEY=$(openssl rand -hex 16)\nWEBHOOK_SECRET=$(openssl rand -base64 24)\n</code></pre>"},{"location":"configuration/application/#configuration_1","title":"Configuration","text":"<pre><code># Redis Settings\nREDIS_HOST=redis\nREDIS_PORT=6379\nREDIS_PASSWORD=your_redis_password_here\nREDIS_MEMORY_LIMIT=2g\nREDIS_MAXMEMORY_POLICY=allkeys-lru\n\n# Redis Performance\nREDIS_SAVE_INTERVAL=900 1\nREDIS_APPENDONLY=yes\nREDIS_APPENDFSYNC=everysec\n</code></pre>"},{"location":"configuration/application/#api-configuration","title":"API Configuration","text":""},{"location":"configuration/application/#api-integration","title":"API Integration","text":"<pre><code># Cityland API Settings\nCITYLAND_API_URL=https://your-api-endpoint.com\nCITYLAND_ACCOUNTING_URL=https://your-accounting-endpoint.com\nCITYLAND_API_USERNAME=your_api_username\nCITYLAND_API_PASSWORD=your_api_password\nCITYLAND_API_TIMEOUT=30000\nCITYLAND_API_RETRY_ATTEMPTS=3\n</code></pre>"},{"location":"configuration/application/#configuration_2","title":"Configuration","text":"<pre><code># SMTP Settings\nSMTP_HOST=smtp.your-domain.com\nSMTP_PORT=587\nSMTP_SECURE=true\nSMTP_USER=noreply@your-domain.com\nSMTP_PASSWORD=your_smtp_password\nSMTP_FROM_NAME=PRS System\nSMTP_FROM_EMAIL=noreply@your-domain.com\n</code></pre>"},{"location":"configuration/application/#performance-settings","title":"Performance Settings","text":""},{"location":"configuration/application/#api-configuration_1","title":"API Configuration","text":"<pre><code># Node.js Settings\nNODE_ENV=production\nPORT=4000\nNODEJS_MAX_OLD_SPACE_SIZE=2048\nNODEJS_MAX_SEMI_SPACE_SIZE=128\n\n# API Performance\nAPI_RATE_LIMIT=1000\nAPI_RATE_WINDOW=900000\nAPI_TIMEOUT=30000\nAPI_MAX_PAYLOAD_SIZE=50mb\n\n# Connection Pool Settings\nDB_POOL_MIN=5\nDB_POOL_MAX=20\nDB_POOL_ACQUIRE=30000\nDB_POOL_IDLE=10000\nDB_POOL_EVICT=20000\n</code></pre>"},{"location":"configuration/application/#configuration_3","title":"Configuration","text":"<pre><code># Vite Build Settings\nVITE_APP_API_URL=https://${DOMAIN}/api\nVITE_APP_WS_URL=wss://${DOMAIN}/ws\nVITE_APP_UPLOAD_MAX_SIZE=10485760\nVITE_APP_CHUNK_SIZE=1048576\n\n# Frontend Performance\nVITE_BUILD_TARGET=es2015\nVITE_BUILD_MINIFY=terser\nVITE_BUILD_SOURCEMAP=false\n</code></pre>"},{"location":"configuration/application/#nginx-configuration","title":"Nginx Configuration","text":""},{"location":"configuration/application/#nginx-configuration_1","title":"Nginx Configuration","text":"<pre><code># /nginx/nginx.conf\nuser nginx;\nworker_processes auto;\nerror_log /var/log/nginx/error.log warn;\npid /var/run/nginx.pid;\n\nevents {\n    worker_connections 4096;\n    use epoll;\n    multi_accept on;\n}\n\nhttp {\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n\n    # Logging\n    log_format main '$remote_addr - $remote_user [$time_local] \"$request\" '\n                    '$status $body_bytes_sent \"$http_referer\" '\n                    '\"$http_user_agent\" \"$http_x_forwarded_for\" '\n                    'rt=$request_time uct=\"$upstream_connect_time\" '\n                    'uht=\"$upstream_header_time\" urt=\"$upstream_response_time\"';\n\n    access_log /var/log/nginx/access.log main;\n\n    # Performance Settings\n    sendfile on;\n    tcp_nopush on;\n    tcp_nodelay on;\n    keepalive_timeout 65;\n    types_hash_max_size 2048;\n    client_max_body_size 50M;\n\n    # Gzip Compression\n    gzip on;\n    gzip_vary on;\n    gzip_min_length 1024;\n    gzip_proxied any;\n    gzip_comp_level 6;\n    gzip_types\n        text/plain\n        text/css\n        text/xml\n        text/javascript\n        application/json\n        application/javascript\n        application/xml+rss\n        application/atom+xml\n        image/svg+xml;\n\n    # Security Headers\n    add_header X-Frame-Options DENY;\n    add_header X-Content-Type-Options nosniff;\n    add_header X-XSS-Protection \"1; mode=block\";\n    add_header Referrer-Policy \"strict-origin-when-cross-origin\";\n\n    # Rate Limiting\n    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;\n    limit_req_zone $binary_remote_addr zone=login:10m rate=1r/s;\n\n    # Include virtual hosts\n    include /etc/nginx/sites-enabled/*;\n}\n</code></pre>"},{"location":"configuration/application/#host-configuration","title":"Host Configuration","text":"<pre><code># /nginx/sites-enabled/prs.conf\nupstream backend {\n    least_conn;\n    server frontend:3000 max_fails=3 fail_timeout=30s;\n    keepalive 32;\n}\n\nupstream api {\n    least_conn;\n    server backend:4000 max_fails=3 fail_timeout=30s;\n    keepalive 32;\n}\n\n# HTTP to HTTPS redirect\nserver {\n    listen 80;\n    server_name ${DOMAIN};\n    return 301 https://$server_name$request_uri;\n}\n\n# Main HTTPS server\nserver {\n    listen 443 ssl http2;\n    server_name ${DOMAIN};\n\n    # SSL Configuration\n    ssl_certificate /etc/nginx/ssl/certificate.crt;\n    ssl_certificate_key /etc/nginx/ssl/private.key;\n    ssl_trusted_certificate /etc/nginx/ssl/ca-bundle.crt;\n\n    ssl_protocols TLSv1.2 TLSv1.3;\n    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384;\n    ssl_prefer_server_ciphers off;\n    ssl_session_cache shared:SSL:10m;\n    ssl_session_timeout 10m;\n\n    # HSTS\n    add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\" always;\n\n    # Frontend Application\n    location / {\n        proxy_pass http://backend;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n\n        proxy_connect_timeout 30s;\n        proxy_send_timeout 30s;\n        proxy_read_timeout 30s;\n\n        proxy_buffering on;\n        proxy_buffer_size 4k;\n        proxy_buffers 8 4k;\n    }\n\n    # API Endpoints\n    location /api/ {\n        limit_req zone=api burst=20 nodelay;\n\n        proxy_pass http://api;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n\n        proxy_connect_timeout 30s;\n        proxy_send_timeout 60s;\n        proxy_read_timeout 60s;\n    }\n\n    # Login endpoint with stricter rate limiting\n    location /api/auth/login {\n        limit_req zone=login burst=5 nodelay;\n\n        proxy_pass http://api;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n\n    # File Uploads\n    location /api/upload {\n        client_max_body_size 50M;\n        proxy_request_buffering off;\n\n        proxy_pass http://api;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n\n        proxy_connect_timeout 60s;\n        proxy_send_timeout 300s;\n        proxy_read_timeout 300s;\n    }\n\n    # Static Files\n    location /uploads/ {\n        alias /var/www/uploads/;\n        expires 1y;\n        add_header Cache-Control \"public, immutable\";\n        add_header X-Content-Type-Options nosniff;\n    }\n\n    # Health Check\n    location /health {\n        access_log off;\n        return 200 \"healthy\\n\";\n        add_header Content-Type text/plain;\n    }\n\n    # Security\n    location ~ /\\. {\n        deny all;\n    }\n}\n</code></pre>"},{"location":"configuration/application/#monitoring-configuration","title":"Monitoring Configuration","text":""},{"location":"configuration/application/#configuration_4","title":"Configuration","text":"<pre><code># config/grafana/grafana.ini\n[server]\nprotocol = http\nhttp_port = 3000\ndomain = ${DOMAIN}\nroot_url = https://${DOMAIN}:3001/\n\n[database]\ntype = postgres\nhost = postgres:5432\nname = grafana\nuser = ${POSTGRES_USER}\npassword = ${POSTGRES_PASSWORD}\n\n[security]\nadmin_user = admin\nadmin_password = ${GRAFANA_ADMIN_PASSWORD}\nsecret_key = ${GRAFANA_SECRET_KEY}\n\n[users]\nallow_sign_up = false\nallow_org_create = false\nauto_assign_org = true\nauto_assign_org_role = Viewer\n\n[auth.anonymous]\nenabled = false\n\n[snapshots]\nexternal_enabled = false\n\n[alerting]\nenabled = true\nexecute_alerts = true\n</code></pre>"},{"location":"configuration/application/#configuration_5","title":"Configuration","text":"<pre><code># config/prometheus/prometheus.yml\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nrule_files:\n  - \"alerts.yml\"\n\nscrape_configs:\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n\n  - job_name: 'node-exporter'\n    static_configs:\n      - targets: ['node-exporter:9100']\n\n  - job_name: 'postgres-exporter'\n    static_configs:\n      - targets: ['postgres-exporter:9187']\n\n  - job_name: 'redis-exporter'\n    static_configs:\n      - targets: ['redis-exporter:9121']\n\n  - job_name: 'nginx-exporter'\n    static_configs:\n      - targets: ['nginx-exporter:9113']\n\n  - job_name: 'prs-backend'\n    static_configs:\n      - targets: ['backend:4000']\n    metrics_path: '/metrics'\n    scrape_interval: 30s\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n          - alertmanager:9093\n</code></pre>"},{"location":"configuration/application/#security-configuration","title":"Security Configuration","text":""},{"location":"configuration/application/#security-settings","title":"Security Settings","text":"<pre><code># Security Headers\nSECURITY_HSTS_MAX_AGE=31536000\nSECURITY_CONTENT_TYPE_OPTIONS=nosniff\nSECURITY_FRAME_OPTIONS=DENY\nSECURITY_XSS_PROTECTION=1; mode=block\n\n# CORS Settings\nCORS_ORIGIN=https://${DOMAIN}\nCORS_METHODS=GET,POST,PUT,DELETE,OPTIONS\nCORS_ALLOWED_HEADERS=Content-Type,Authorization,X-Requested-With\n\n# Rate Limiting\nRATE_LIMIT_WINDOW_MS=900000\nRATE_LIMIT_MAX_REQUESTS=1000\nRATE_LIMIT_SKIP_SUCCESSFUL_REQUESTS=false\n\n# Session Security\nSESSION_SECURE=true\nSESSION_HTTP_ONLY=true\nSESSION_SAME_SITE=strict\nSESSION_MAX_AGE=86400000\n</code></pre>"},{"location":"configuration/application/#security","title":"Security","text":"<pre><code>-- Create application-specific roles\nCREATE ROLE prs_app_read;\nCREATE ROLE prs_app_write;\nCREATE ROLE prs_app_admin;\n\n-- Grant appropriate permissions\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO prs_app_read;\nGRANT SELECT, INSERT, UPDATE ON ALL TABLES IN SCHEMA public TO prs_app_write;\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO prs_app_admin;\n\n-- Create application user\nCREATE USER prs_application WITH PASSWORD 'secure_app_password';\nGRANT prs_app_write TO prs_application;\n</code></pre>"},{"location":"configuration/application/#configuration-management","title":"Configuration Management","text":""},{"location":"configuration/application/#file-generation","title":"File Generation","text":"<pre><code>#!/bin/bash\n# Generate secure environment file\n\nENV_FILE=\"/opt/prs-deployment/02-docker-configuration/.env\"\n\n# Generate secure passwords\nPOSTGRES_PASSWORD=$(openssl rand -base64 32)\nREDIS_PASSWORD=$(openssl rand -base64 32)\nJWT_SECRET=$(openssl rand -base64 32)\nENCRYPTION_KEY=$(openssl rand -base64 32)\nGRAFANA_ADMIN_PASSWORD=$(openssl rand -base64 16)\n\n# Create environment file\ncat &gt; \"$ENV_FILE\" &lt;&lt; EOF\n# Generated on $(date)\n\n# Domain Configuration\nDOMAIN=your-domain.com\nSERVER_IP=192.168.0.100\n\n# Database Configuration\nPOSTGRES_DB=prs_production\nPOSTGRES_USER=prs_admin\nPOSTGRES_PASSWORD=$POSTGRES_PASSWORD\n\n# Redis Configuration\nREDIS_PASSWORD=$REDIS_PASSWORD\n\n# Application Secrets\nJWT_SECRET=$JWT_SECRET\nENCRYPTION_KEY=$ENCRYPTION_KEY\n\n# Monitoring\nGRAFANA_ADMIN_PASSWORD=$GRAFANA_ADMIN_PASSWORD\n\n# External APIs\nCITYLAND_API_URL=https://your-api-endpoint.com\nCITYLAND_API_USERNAME=your_username\nCITYLAND_API_PASSWORD=your_password\nEOF\n\necho \"Environment file generated: $ENV_FILE\"\necho \"Please update the external API credentials and domain settings.\"\n</code></pre>"},{"location":"configuration/application/#validation","title":"Validation","text":"<pre><code>#!/bin/bash\n# Validate configuration\n\nENV_FILE=\"/opt/prs-deployment/02-docker-configuration/.env\"\n\n# Check required variables\nREQUIRED_VARS=(\n    \"DOMAIN\"\n    \"POSTGRES_PASSWORD\"\n    \"REDIS_PASSWORD\"\n    \"JWT_SECRET\"\n    \"ENCRYPTION_KEY\"\n)\n\necho \"Validating configuration...\"\n\nfor var in \"${REQUIRED_VARS[@]}\"; do\n    if ! grep -q \"^${var}=\" \"$ENV_FILE\"; then\n        echo \"\u274c Missing required variable: $var\"\n        exit 1\n    else\n        echo \"Found: $var\"\n    fi\ndone\n\n# Check password strength\nif grep -q \"POSTGRES_PASSWORD=password\" \"$ENV_FILE\"; then\n    echo \"\u274c Weak PostgreSQL password detected\"\n    exit 1\nfi\n\necho \"Configuration validation passed\"\n</code></pre> <p>Configuration Complete</p> <p>With proper application configuration, the PRS system provides optimal performance, security, and reliability for production use.</p> <p>Security Best Practices</p> <p>Always use strong, randomly generated passwords and keep configuration files secure with appropriate file permissions.</p>"},{"location":"configuration/database/","title":"Database Configuration","text":""},{"location":"configuration/database/#overview","title":"Overview","text":"<p>This guide covers advanced database configuration for optimal performance, security, and reliability in the PRS on-premises deployment.</p>"},{"location":"configuration/database/#postgresql-configuration","title":"PostgreSQL Configuration","text":""},{"location":"configuration/database/#memory-configuration","title":"Memory Configuration","text":"<pre><code>-- Memory settings optimized for 16GB RAM system\n-- Allocating 6GB total to PostgreSQL\n\n-- Shared memory (33% of allocated RAM)\nALTER SYSTEM SET shared_buffers = '2GB';\n\n-- Cache size hint (67% of allocated RAM)\nALTER SYSTEM SET effective_cache_size = '4GB';\n\n-- Working memory per operation\nALTER SYSTEM SET work_mem = '32MB';\n\n-- Maintenance operations memory\nALTER SYSTEM SET maintenance_work_mem = '512MB';\n\n-- WAL buffer size\nALTER SYSTEM SET wal_buffers = '32MB';\n\n-- Reload configuration\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"configuration/database/#connection-management","title":"Connection Management","text":"<pre><code>-- Connection settings for high concurrency\nALTER SYSTEM SET max_connections = 150;\n\n-- Connection pooling optimization\nALTER SYSTEM SET shared_preload_libraries = 'pg_stat_statements,timescaledb';\n\n-- Background worker processes\nALTER SYSTEM SET max_worker_processes = 32;\nALTER SYSTEM SET max_parallel_workers = 16;\nALTER SYSTEM SET max_parallel_workers_per_gather = 4;\n\n-- TimescaleDB background workers\nALTER SYSTEM SET timescaledb.max_background_workers = 16;\n\n-- Reload configuration\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"configuration/database/#storage-optimization","title":"Storage Optimization","text":"<pre><code>-- SSD optimization settings\nALTER SYSTEM SET random_page_cost = 1.1;\nALTER SYSTEM SET effective_io_concurrency = 200;\nALTER SYSTEM SET seq_page_cost = 1.0;\n\n-- Checkpoint configuration\nALTER SYSTEM SET checkpoint_completion_target = 0.9;\nALTER SYSTEM SET checkpoint_timeout = '15min';\nALTER SYSTEM SET max_wal_size = '2GB';\nALTER SYSTEM SET min_wal_size = '512MB';\n\n-- Vacuum and autovacuum settings\nALTER SYSTEM SET autovacuum_max_workers = 6;\nALTER SYSTEM SET autovacuum_naptime = '30s';\nALTER SYSTEM SET autovacuum_vacuum_threshold = 50;\nALTER SYSTEM SET autovacuum_analyze_threshold = 50;\n\n-- Reload configuration\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"configuration/database/#timescaledb-configuration","title":"TimescaleDB Configuration","text":""},{"location":"configuration/database/#hypertable-setup","title":"Hypertable Setup","text":"<pre><code>-- Create hypertables for time-series data\nSELECT create_hypertable('notifications', 'created_at', chunk_time_interval =&gt; INTERVAL '1 day');\nSELECT create_hypertable('audit_logs', 'created_at', chunk_time_interval =&gt; INTERVAL '1 day');\nSELECT create_hypertable('histories', 'created_at', chunk_time_interval =&gt; INTERVAL '1 day');\n\n-- Create hypertables for business data with longer intervals\nSELECT create_hypertable('requisitions', 'created_at', chunk_time_interval =&gt; INTERVAL '7 days');\nSELECT create_hypertable('purchase_orders', 'created_at', chunk_time_interval =&gt; INTERVAL '7 days');\nSELECT create_hypertable('delivery_receipts', 'created_at', chunk_time_interval =&gt; INTERVAL '7 days');\n\n-- Verify hypertables\nSELECT * FROM timescaledb_information.hypertables;\n</code></pre>"},{"location":"configuration/database/#compression-configuration","title":"Compression Configuration","text":"<pre><code>-- Configure compression for optimal storage\nALTER TABLE notifications SET (\n    timescaledb.compress,\n    timescaledb.compress_segmentby = 'user_id',\n    timescaledb.compress_orderby = 'created_at DESC'\n);\n\nALTER TABLE audit_logs SET (\n    timescaledb.compress,\n    timescaledb.compress_segmentby = 'user_id, action',\n    timescaledb.compress_orderby = 'created_at DESC'\n);\n\nALTER TABLE requisitions SET (\n    timescaledb.compress,\n    timescaledb.compress_segmentby = 'department_id, status',\n    timescaledb.compress_orderby = 'created_at DESC'\n);\n\n-- Add compression policies\nSELECT add_compression_policy('notifications', INTERVAL '7 days');\nSELECT add_compression_policy('audit_logs', INTERVAL '7 days');\nSELECT add_compression_policy('requisitions', INTERVAL '30 days');\n</code></pre>"},{"location":"configuration/database/#data-movement-policies","title":"Data Movement Policies","text":"<pre><code>-- Configure automatic data movement to HDD storage\nSELECT add_move_chunk_policy('notifications', INTERVAL '30 days', 'hdd_cold');\nSELECT add_move_chunk_policy('audit_logs', INTERVAL '30 days', 'hdd_cold');\nSELECT add_move_chunk_policy('requisitions', INTERVAL '30 days', 'hdd_cold');\nSELECT add_move_chunk_policy('purchase_orders', INTERVAL '30 days', 'hdd_cold');\n\n-- Configure faster movement for history tables\nSELECT add_move_chunk_policy('requisition_canvass_histories', INTERVAL '14 days', 'hdd_cold');\nSELECT add_move_chunk_policy('requisition_item_histories', INTERVAL '14 days', 'hdd_cold');\n</code></pre>"},{"location":"configuration/database/#index-optimization","title":"Index Optimization","text":""},{"location":"configuration/database/#primary-indexes","title":"Primary Indexes","text":"<pre><code>-- Time-based indexes for efficient queries\nCREATE INDEX CONCURRENTLY idx_notifications_time \nON notifications (created_at DESC);\n\nCREATE INDEX CONCURRENTLY idx_audit_logs_time \nON audit_logs (created_at DESC);\n\nCREATE INDEX CONCURRENTLY idx_requisitions_time \nON requisitions (created_at DESC);\n</code></pre>"},{"location":"configuration/database/#composite-indexes","title":"Composite Indexes","text":"<pre><code>-- User-specific time-based queries\nCREATE INDEX CONCURRENTLY idx_notifications_user_time \nON notifications (user_id, created_at DESC);\n\nCREATE INDEX CONCURRENTLY idx_audit_logs_user_action_time \nON audit_logs (user_id, action, created_at DESC);\n\n-- Department-specific queries\nCREATE INDEX CONCURRENTLY idx_requisitions_dept_status_time \nON requisitions (department_id, status, created_at DESC);\n</code></pre>"},{"location":"configuration/database/#partial-indexes","title":"Partial Indexes","text":"<pre><code>-- Indexes for hot data only (performance optimization)\nCREATE INDEX CONCURRENTLY idx_notifications_recent \nON notifications (user_id, created_at DESC) \nWHERE created_at &gt;= NOW() - INTERVAL '30 days';\n\nCREATE INDEX CONCURRENTLY idx_requisitions_active \nON requisitions (department_id, status, created_at DESC) \nWHERE status IN ('pending', 'approved', 'processing');\n</code></pre>"},{"location":"configuration/database/#query-optimization","title":"Query Optimization","text":""},{"location":"configuration/database/#statistics-configuration","title":"Statistics Configuration","text":"<pre><code>-- Increase statistics target for better query planning\nALTER SYSTEM SET default_statistics_target = 100;\n\n-- Enable constraint exclusion for partitioning\nALTER SYSTEM SET constraint_exclusion = 'partition';\n\n-- Query planning settings\nALTER SYSTEM SET enable_partitionwise_join = on;\nALTER SYSTEM SET enable_partitionwise_aggregate = on;\n\n-- Reload configuration\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"configuration/database/#query-monitoring","title":"Query Monitoring","text":"<pre><code>-- Enable query statistics collection\nALTER SYSTEM SET shared_preload_libraries = 'pg_stat_statements,timescaledb';\nALTER SYSTEM SET pg_stat_statements.track = 'all';\nALTER SYSTEM SET pg_stat_statements.max = 10000;\n\n-- Create extension\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\n-- View slow queries\nSELECT \n    query,\n    calls,\n    total_time,\n    mean_time,\n    rows\nFROM pg_stat_statements \nORDER BY total_time DESC \nLIMIT 10;\n</code></pre>"},{"location":"configuration/database/#connection-pooling","title":"Connection Pooling","text":""},{"location":"configuration/database/#application-level-pooling","title":"Application-Level Pooling","text":"<pre><code>// Backend connection pool configuration\nconst poolConfig = {\n  host: process.env.POSTGRES_HOST,\n  database: process.env.POSTGRES_DB,\n  user: process.env.POSTGRES_USER,\n  password: process.env.POSTGRES_PASSWORD,\n  port: process.env.POSTGRES_PORT,\n\n  // Pool settings optimized for on-premises\n  min: 5,                    // Minimum connections\n  max: 20,                   // Maximum connections (vs 3 on cloud)\n  acquire: 30000,            // Connection timeout (30s)\n  idle: 10000,               // Idle timeout (10s)\n  evict: 20000,              // Eviction timeout (20s)\n\n  // Retry configuration\n  retry: {\n    max: 3,\n    timeout: 5000,\n    match: [/ECONNRESET/, /ETIMEDOUT/]\n  }\n};\n</code></pre>"},{"location":"configuration/database/#pgbouncer-configuration-optional","title":"PgBouncer Configuration (Optional)","text":"<pre><code># /etc/pgbouncer/pgbouncer.ini\n[databases]\nprs_production = host=localhost port=5432 dbname=prs_production\n\n[pgbouncer]\nlisten_port = 6432\nlisten_addr = 127.0.0.1\nauth_type = md5\nauth_file = /etc/pgbouncer/userlist.txt\n\n# Pool settings\npool_mode = transaction\nmax_client_conn = 200\ndefault_pool_size = 25\nreserve_pool_size = 5\n\n# Timeouts\nserver_connect_timeout = 15\nserver_login_retry = 15\nquery_timeout = 0\nquery_wait_timeout = 120\nclient_idle_timeout = 0\nserver_idle_timeout = 600\n</code></pre>"},{"location":"configuration/database/#backup-configuration","title":"Backup Configuration","text":""},{"location":"configuration/database/#continuous-archiving","title":"Continuous Archiving","text":"<pre><code>-- Configure WAL archiving\nALTER SYSTEM SET wal_level = 'replica';\nALTER SYSTEM SET archive_mode = 'on';\nALTER SYSTEM SET archive_command = 'cp %p /var/lib/postgresql/wal-archive/%f';\nALTER SYSTEM SET archive_timeout = '300s';\n\n-- Configure for streaming replication (future use)\nALTER SYSTEM SET max_wal_senders = 3;\nALTER SYSTEM SET wal_keep_segments = 64;\n\n-- Reload configuration\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"configuration/database/#backup-scheduling","title":"Backup Scheduling","text":"<pre><code># Full backup script configuration\ncat &gt; /opt/prs-deployment/scripts/backup-config.sh &lt;&lt; 'EOF'\n#!/bin/bash\n\n# Backup configuration\nBACKUP_DIR=\"/mnt/hdd/postgres-backups\"\nRETENTION_DAYS=30\nCOMPRESSION_LEVEL=9\n\n# Database connection\nPGHOST=\"localhost\"\nPGPORT=\"5432\"\nPGUSER=\"prs_admin\"\nPGDATABASE=\"prs_production\"\n\n# Backup types\nFULL_BACKUP_SCHEDULE=\"0 2 * * *\"      # Daily at 2 AM\nINCREMENTAL_SCHEDULE=\"0 */6 * * *\"     # Every 6 hours\nWAL_ARCHIVE_SCHEDULE=\"*/5 * * * *\"     # Every 5 minutes\n\nexport PGHOST PGPORT PGUSER PGDATABASE\nEOF\n</code></pre>"},{"location":"configuration/database/#security-configuration","title":"Security Configuration","text":""},{"location":"configuration/database/#user-management","title":"User Management","text":"<pre><code>-- Create role hierarchy\nCREATE ROLE prs_readonly;\nCREATE ROLE prs_readwrite;\nCREATE ROLE prs_admin_role;\n\n-- Grant permissions\nGRANT CONNECT ON DATABASE prs_production TO prs_readonly;\nGRANT USAGE ON SCHEMA public TO prs_readonly;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO prs_readonly;\n\nGRANT prs_readonly TO prs_readwrite;\nGRANT INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO prs_readwrite;\n\nGRANT prs_readwrite TO prs_admin_role;\nGRANT CREATE ON SCHEMA public TO prs_admin_role;\n\n-- Create application users\nCREATE USER prs_app_user WITH PASSWORD 'secure_app_password';\nGRANT prs_readwrite TO prs_app_user;\n\nCREATE USER prs_readonly_user WITH PASSWORD 'secure_readonly_password';\nGRANT prs_readonly TO prs_readonly_user;\n</code></pre>"},{"location":"configuration/database/#access-control","title":"Access Control","text":"<pre><code>-- Configure connection limits\nALTER ROLE prs_admin CONNECTION LIMIT 10;\nALTER ROLE prs_app_user CONNECTION LIMIT 50;\nALTER ROLE prs_readonly_user CONNECTION LIMIT 20;\n\n-- Set session timeouts\nALTER ROLE prs_app_user SET statement_timeout = '30s';\nALTER ROLE prs_readonly_user SET statement_timeout = '60s';\n\n-- Restrict dangerous functions\nREVOKE EXECUTE ON FUNCTION pg_read_file(text) FROM PUBLIC;\nREVOKE EXECUTE ON FUNCTION pg_ls_dir(text) FROM PUBLIC;\n</code></pre>"},{"location":"configuration/database/#monitoring-configuration","title":"Monitoring Configuration","text":""},{"location":"configuration/database/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>-- Create monitoring views\nCREATE OR REPLACE VIEW v_database_stats AS\nSELECT \n    datname,\n    numbackends as connections,\n    xact_commit as commits,\n    xact_rollback as rollbacks,\n    blks_read,\n    blks_hit,\n    round(blks_hit::numeric / (blks_hit + blks_read) * 100, 2) as cache_hit_ratio\nFROM pg_stat_database \nWHERE datname = 'prs_production';\n\nCREATE OR REPLACE VIEW v_table_stats AS\nSELECT \n    schemaname,\n    tablename,\n    n_tup_ins as inserts,\n    n_tup_upd as updates,\n    n_tup_del as deletes,\n    n_live_tup as live_tuples,\n    n_dead_tup as dead_tuples,\n    last_vacuum,\n    last_autovacuum,\n    last_analyze,\n    last_autoanalyze\nFROM pg_stat_user_tables\nORDER BY n_live_tup DESC;\n</code></pre>"},{"location":"configuration/database/#timescaledb-monitoring","title":"TimescaleDB Monitoring","text":"<pre><code>-- Create TimescaleDB monitoring views\nCREATE OR REPLACE VIEW v_chunk_stats AS\nSELECT \n    hypertable_name,\n    chunk_name,\n    tablespace_name,\n    is_compressed,\n    pg_size_pretty(chunk_size) as size,\n    range_start,\n    range_end\nFROM timescaledb_information.chunks\nORDER BY hypertable_name, range_start DESC;\n\nCREATE OR REPLACE VIEW v_compression_stats AS\nSELECT \n    hypertable_name,\n    pg_size_pretty(before_compression_total_bytes) as before_compression,\n    pg_size_pretty(after_compression_total_bytes) as after_compression,\n    round(\n        (before_compression_total_bytes::numeric - after_compression_total_bytes::numeric) \n        / before_compression_total_bytes::numeric * 100, 2\n    ) as compression_ratio_percent\nFROM timescaledb_information.compressed_hypertable_stats;\n</code></pre>"},{"location":"configuration/database/#maintenance-procedures","title":"Maintenance Procedures","text":""},{"location":"configuration/database/#daily-maintenance","title":"Daily Maintenance","text":"<pre><code>-- Update table statistics\nANALYZE notifications;\nANALYZE audit_logs;\nANALYZE requisitions;\n\n-- Check for bloated tables\nSELECT \n    schemaname,\n    tablename,\n    n_dead_tup,\n    n_live_tup,\n    round(n_dead_tup::numeric / (n_live_tup + n_dead_tup) * 100, 2) as dead_ratio\nFROM pg_stat_user_tables\nWHERE n_dead_tup &gt; 1000\nORDER BY dead_ratio DESC;\n</code></pre>"},{"location":"configuration/database/#weekly-maintenance","title":"Weekly Maintenance","text":"<pre><code>-- Vacuum and analyze all tables\nVACUUM ANALYZE;\n\n-- Reindex if needed (check for index bloat first)\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    pg_size_pretty(pg_relation_size(indexrelid)) as index_size\nFROM pg_stat_user_indexes\nORDER BY pg_relation_size(indexrelid) DESC;\n</code></pre>"},{"location":"configuration/database/#monthly-maintenance","title":"Monthly Maintenance","text":"<pre><code>-- Full vacuum for heavily updated tables\nVACUUM FULL notifications;\nVACUUM FULL audit_logs;\n\n-- Update statistics targets if needed\nALTER TABLE notifications ALTER COLUMN created_at SET STATISTICS 1000;\nALTER TABLE requisitions ALTER COLUMN status SET STATISTICS 1000;\n\n-- Analyze after statistics changes\nANALYZE notifications;\nANALYZE requisitions;\n</code></pre> <p>Database Optimized</p> <p>Your PostgreSQL/TimescaleDB configuration is now optimized for high-performance on-premises deployment with automatic data lifecycle management.</p> <p>Performance Monitoring</p> <p>Regularly monitor the created views and adjust configuration based on actual usage patterns and performance metrics.</p> <p>Configuration Changes</p> <p>Always test configuration changes in a staging environment before applying to production, and monitor performance after changes.</p>"},{"location":"configuration/monitoring/","title":"Monitoring Configuration","text":""},{"location":"configuration/monitoring/#overview","title":"Overview","text":"<p>This guide covers the complete configuration of monitoring, alerting, and observability for the PRS on-premises deployment using Prometheus, Grafana, and custom monitoring solutions.</p>"},{"location":"configuration/monitoring/#monitoring-architecture","title":"Monitoring Architecture","text":"<pre><code>graph TB\n    subgraph \"Data Collection\"\n        APP[PRS Application&lt;br/&gt;Custom Metrics] --&gt; PROM[Prometheus&lt;br/&gt;Metrics Collection]\n        NODE[Node Exporter&lt;br/&gt;System Metrics] --&gt; PROM\n        POSTGRES[PostgreSQL Exporter&lt;br/&gt;Database Metrics] --&gt; PROM\n        REDIS[Redis Exporter&lt;br/&gt;Cache Metrics] --&gt; PROM\n        NGINX[Nginx Exporter&lt;br/&gt;Web Server Metrics] --&gt; PROM\n        CADVISOR[cAdvisor&lt;br/&gt;Container Metrics] --&gt; PROM\n    end\n\n    subgraph \"Storage &amp; Processing\"\n        PROM --&gt; TSDB[Prometheus TSDB&lt;br/&gt;Time Series Storage]\n    end\n\n    subgraph \"Visualization\"\n        TSDB --&gt; GRAFANA[Grafana&lt;br/&gt;Dashboards &amp; Visualization]\n    end\n\n    subgraph \"Alerting\"\n        TSDB --&gt; ALERTMGR[Alert Manager&lt;br/&gt;Alert Processing]\n        ALERTMGR --&gt; EMAIL[Email Notifications]\n        ALERTMGR --&gt; SLACK[Slack Integration]\n        ALERTMGR --&gt; WEBHOOK[Custom Webhooks]\n    end\n\n    style PROM fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style GRAFANA fill:#e8f5e8,stroke:#4caf50,stroke-width:2px\n    style ALERTMGR fill:#fff3e0,stroke:#ff9800,stroke-width:2px</code></pre>"},{"location":"configuration/monitoring/#prometheus-configuration","title":"Prometheus Configuration","text":""},{"location":"configuration/monitoring/#main-configuration-file","title":"Main Configuration File","text":"<pre><code># config/prometheus/prometheus.yml\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n  external_labels:\n    cluster: 'prs-onprem'\n    environment: 'production'\n    datacenter: 'primary'\n\nrule_files:\n  - \"alerts/*.yml\"\n  - \"recording_rules/*.yml\"\n\nscrape_configs:\n  # Prometheus self-monitoring\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n    scrape_interval: 30s\n    metrics_path: /metrics\n\n  # System metrics\n  - job_name: 'node-exporter'\n    static_configs:\n      - targets: ['node-exporter:9100']\n    scrape_interval: 30s\n    relabel_configs:\n      - source_labels: [__address__]\n        target_label: instance\n        replacement: 'prs-server'\n\n  # PostgreSQL metrics\n  - job_name: 'postgres-exporter'\n    static_configs:\n      - targets: ['postgres-exporter:9187']\n    scrape_interval: 30s\n    relabel_configs:\n      - source_labels: [__address__]\n        target_label: database\n        replacement: 'prs-production'\n\n  # Redis metrics\n  - job_name: 'redis-exporter'\n    static_configs:\n      - targets: ['redis-exporter:9121']\n    scrape_interval: 30s\n\n  # Nginx metrics\n  - job_name: 'nginx-exporter'\n    static_configs:\n      - targets: ['nginx-exporter:9113']\n    scrape_interval: 30s\n\n  # PRS Backend application\n  - job_name: 'prs-backend'\n    static_configs:\n      - targets: ['backend:4000']\n    metrics_path: '/metrics'\n    scrape_interval: 15s\n    scrape_timeout: 10s\n\n  # Docker container metrics\n  - job_name: 'cadvisor'\n    static_configs:\n      - targets: ['cadvisor:8080']\n    scrape_interval: 30s\n    metrics_path: /metrics\n\n  # Custom application metrics\n  - job_name: 'prs-custom-metrics'\n    file_sd_configs:\n      - files:\n          - '/etc/prometheus/targets/*.json'\n    scrape_interval: 30s\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n          - alertmanager:9093\n      timeout: 10s\n      api_version: v2\n\n# Storage configuration\nstorage:\n  tsdb:\n    path: /prometheus/data\n    retention.time: 90d\n    retention.size: 50GB\n    wal-compression: true\n</code></pre>"},{"location":"configuration/monitoring/#recording-rules","title":"Recording Rules","text":"<pre><code># config/prometheus/recording_rules/prs_rules.yml\ngroups:\n  - name: prs_application_rules\n    interval: 30s\n    rules:\n      # HTTP request metrics\n      - record: prs:http_request_duration_seconds:rate5m\n        expr: rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m])\n        labels:\n          job: prs-backend\n\n      - record: prs:http_requests_per_second:rate5m\n        expr: rate(http_requests_total[5m])\n        labels:\n          job: prs-backend\n\n      - record: prs:http_error_rate:rate5m\n        expr: rate(http_requests_total{status=~\"5..\"}[5m]) / rate(http_requests_total[5m])\n        labels:\n          job: prs-backend\n\n      # Database metrics\n      - record: prs:database_connections:active\n        expr: pg_stat_database_numbackends{datname=\"prs_production\"}\n\n      - record: prs:database_cache_hit_ratio\n        expr: |\n          (\n            pg_stat_database_blks_hit{datname=\"prs_production\"} / \n            (pg_stat_database_blks_hit{datname=\"prs_production\"} + pg_stat_database_blks_read{datname=\"prs_production\"})\n          )\n\n      - record: prs:database_transaction_rate:rate5m\n        expr: rate(pg_stat_database_xact_commit{datname=\"prs_production\"}[5m])\n\n  - name: prs_system_rules\n    interval: 30s\n    rules:\n      # System resource metrics\n      - record: prs:cpu_usage_percent\n        expr: 100 - (avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)\n\n      - record: prs:memory_usage_percent\n        expr: |\n          (\n            (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / \n            node_memory_MemTotal_bytes\n          ) * 100\n\n      - record: prs:disk_usage_percent\n        expr: |\n          (\n            (node_filesystem_size_bytes - node_filesystem_avail_bytes) / \n            node_filesystem_size_bytes\n          ) * 100\n\n      # Storage tier metrics\n      - record: prs:ssd_usage_percent\n        expr: |\n          (\n            (node_filesystem_size_bytes{mountpoint=\"/mnt/ssd\"} - node_filesystem_avail_bytes{mountpoint=\"/mnt/ssd\"}) / \n            node_filesystem_size_bytes{mountpoint=\"/mnt/ssd\"}\n          ) * 100\n\n      - record: prs:hdd_usage_percent\n        expr: |\n          (\n            (node_filesystem_size_bytes{mountpoint=\"/mnt/hdd\"} - node_filesystem_avail_bytes{mountpoint=\"/mnt/hdd\"}) / \n            node_filesystem_size_bytes{mountpoint=\"/mnt/hdd\"}\n          ) * 100\n\n  - name: prs_business_rules\n    interval: 60s\n    rules:\n      # Business metrics\n      - record: prs:active_users:count\n        expr: count(increase(http_requests_total{endpoint=\"/api/auth/login\"}[1h]))\n\n      - record: prs:requisitions_created:rate1h\n        expr: rate(prs_requisitions_created_total[1h])\n\n      - record: prs:purchase_orders_generated:rate1h\n        expr: rate(prs_purchase_orders_generated_total[1h])\n\n      - record: prs:file_uploads:rate1h\n        expr: rate(prs_file_uploads_total[1h])\n</code></pre>"},{"location":"configuration/monitoring/#alert-rules","title":"Alert Rules","text":"<pre><code># config/prometheus/alerts/prs_alerts.yml\ngroups:\n  - name: prs_critical_alerts\n    rules:\n      # Service availability\n      - alert: ServiceDown\n        expr: up == 0\n        for: 1m\n        labels:\n          severity: critical\n          team: infrastructure\n        annotations:\n          summary: \"Service {{ $labels.job }} is down\"\n          description: \"Service {{ $labels.job }} on {{ $labels.instance }} has been down for more than 1 minute\"\n          runbook_url: \"https://docs.prs.local/troubleshooting/service-down\"\n\n      # High error rate\n      - alert: HighErrorRate\n        expr: prs:http_error_rate:rate5m &gt; 0.05\n        for: 5m\n        labels:\n          severity: critical\n          team: application\n        annotations:\n          summary: \"High error rate detected\"\n          description: \"Error rate is {{ $value | humanizePercentage }} for the last 5 minutes\"\n          runbook_url: \"https://docs.prs.local/troubleshooting/high-error-rate\"\n\n      # Database issues\n      - alert: DatabaseConnectionsHigh\n        expr: prs:database_connections:active &gt; 120\n        for: 2m\n        labels:\n          severity: warning\n          team: database\n        annotations:\n          summary: \"High database connections\"\n          description: \"Database has {{ $value }} active connections (threshold: 120)\"\n\n      - alert: DatabaseCacheHitRatioLow\n        expr: prs:database_cache_hit_ratio &lt; 0.95\n        for: 5m\n        labels:\n          severity: warning\n          team: database\n        annotations:\n          summary: \"Low database cache hit ratio\"\n          description: \"Cache hit ratio is {{ $value | humanizePercentage }} (threshold: 95%)\"\n\n  - name: prs_storage_alerts\n    rules:\n      # SSD storage alerts\n      - alert: SSDStorageHigh\n        expr: prs:ssd_usage_percent &gt; 85\n        for: 1m\n        labels:\n          severity: warning\n          team: infrastructure\n        annotations:\n          summary: \"SSD storage usage high\"\n          description: \"SSD usage is {{ $value | humanizePercentage }} (threshold: 85%)\"\n\n      - alert: SSDStorageCritical\n        expr: prs:ssd_usage_percent &gt; 90\n        for: 1m\n        labels:\n          severity: critical\n          team: infrastructure\n        annotations:\n          summary: \"SSD storage usage critical\"\n          description: \"SSD usage is {{ $value | humanizePercentage }} (threshold: 90%)\"\n\n      # HDD storage alerts\n      - alert: HDDStorageHigh\n        expr: prs:hdd_usage_percent &gt; 80\n        for: 5m\n        labels:\n          severity: warning\n          team: infrastructure\n        annotations:\n          summary: \"HDD storage usage high\"\n          description: \"HDD usage is {{ $value | humanizePercentage }} (threshold: 80%)\"\n\n  - name: prs_performance_alerts\n    rules:\n      # System performance\n      - alert: HighCPUUsage\n        expr: prs:cpu_usage_percent &gt; 80\n        for: 5m\n        labels:\n          severity: warning\n          team: infrastructure\n        annotations:\n          summary: \"High CPU usage\"\n          description: \"CPU usage is {{ $value | humanizePercentage }} for the last 5 minutes\"\n\n      - alert: HighMemoryUsage\n        expr: prs:memory_usage_percent &gt; 85\n        for: 5m\n        labels:\n          severity: warning\n          team: infrastructure\n        annotations:\n          summary: \"High memory usage\"\n          description: \"Memory usage is {{ $value | humanizePercentage }}\"\n\n      # Application performance\n      - alert: SlowResponseTime\n        expr: prs:http_request_duration_seconds:rate5m &gt; 1.0\n        for: 5m\n        labels:\n          severity: warning\n          team: application\n        annotations:\n          summary: \"Slow response times\"\n          description: \"Average response time is {{ $value }}s for the last 5 minutes\"\n\n      - alert: LowThroughput\n        expr: prs:http_requests_per_second:rate5m &lt; 1\n        for: 10m\n        labels:\n          severity: info\n          team: application\n        annotations:\n          summary: \"Low application throughput\"\n          description: \"Request rate is {{ $value }} requests/second\"\n</code></pre>"},{"location":"configuration/monitoring/#grafana-configuration","title":"Grafana Configuration","text":""},{"location":"configuration/monitoring/#dashboard-configuration","title":"Dashboard Configuration","text":"<pre><code>{\n  \"dashboard\": {\n    \"id\": null,\n    \"title\": \"PRS System Overview\",\n    \"tags\": [\"prs\", \"overview\"],\n    \"timezone\": \"browser\",\n    \"refresh\": \"30s\",\n    \"time\": {\n      \"from\": \"now-1h\",\n      \"to\": \"now\"\n    },\n    \"panels\": [\n      {\n        \"id\": 1,\n        \"title\": \"System Health\",\n        \"type\": \"stat\",\n        \"targets\": [\n          {\n            \"expr\": \"up\",\n            \"legendFormat\": \"{{ job }}\"\n          }\n        ],\n        \"fieldConfig\": {\n          \"defaults\": {\n            \"color\": {\n              \"mode\": \"thresholds\"\n            },\n            \"thresholds\": {\n              \"steps\": [\n                {\"color\": \"red\", \"value\": 0},\n                {\"color\": \"green\", \"value\": 1}\n              ]\n            }\n          }\n        }\n      },\n      {\n        \"id\": 2,\n        \"title\": \"Request Rate\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"prs:http_requests_per_second:rate5m\",\n            \"legendFormat\": \"Requests/sec\"\n          }\n        ]\n      },\n      {\n        \"id\": 3,\n        \"title\": \"Response Time\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"prs:http_request_duration_seconds:rate5m\",\n            \"legendFormat\": \"Avg Response Time\"\n          }\n        ]\n      },\n      {\n        \"id\": 4,\n        \"title\": \"Error Rate\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"prs:http_error_rate:rate5m * 100\",\n            \"legendFormat\": \"Error Rate %\"\n          }\n        ]\n      },\n      {\n        \"id\": 5,\n        \"title\": \"CPU Usage\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"prs:cpu_usage_percent\",\n            \"legendFormat\": \"CPU Usage %\"\n          }\n        ]\n      },\n      {\n        \"id\": 6,\n        \"title\": \"Memory Usage\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"prs:memory_usage_percent\",\n            \"legendFormat\": \"Memory Usage %\"\n          }\n        ]\n      },\n      {\n        \"id\": 7,\n        \"title\": \"Storage Usage\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"prs:ssd_usage_percent\",\n            \"legendFormat\": \"SSD Usage %\"\n          },\n          {\n            \"expr\": \"prs:hdd_usage_percent\",\n            \"legendFormat\": \"HDD Usage %\"\n          }\n        ]\n      },\n      {\n        \"id\": 8,\n        \"title\": \"Database Connections\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"prs:database_connections:active\",\n            \"legendFormat\": \"Active Connections\"\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"configuration/monitoring/#data-source-configuration","title":"Data Source Configuration","text":"<pre><code># config/grafana/provisioning/datasources/prometheus.yml\napiVersion: 1\n\ndatasources:\n  - name: Prometheus\n    type: prometheus\n    access: proxy\n    url: http://prometheus:9090\n    isDefault: true\n    editable: true\n    jsonData:\n      timeInterval: \"15s\"\n      queryTimeout: \"60s\"\n      httpMethod: \"POST\"\n    secureJsonData:\n      httpHeaderValue1: \"Bearer ${PROMETHEUS_API_TOKEN}\"\n</code></pre>"},{"location":"configuration/monitoring/#dashboard-provisioning","title":"Dashboard Provisioning","text":"<pre><code># config/grafana/provisioning/dashboards/prs.yml\napiVersion: 1\n\nproviders:\n  - name: 'PRS Dashboards'\n    orgId: 1\n    folder: 'PRS'\n    type: file\n    disableDeletion: false\n    updateIntervalSeconds: 10\n    allowUiUpdates: true\n    options:\n      path: /etc/grafana/provisioning/dashboards/prs\n</code></pre>"},{"location":"configuration/monitoring/#alert-manager-configuration","title":"Alert Manager Configuration","text":""},{"location":"configuration/monitoring/#main-configuration","title":"Main Configuration","text":"<pre><code># config/alertmanager/alertmanager.yml\nglobal:\n  smtp_smarthost: 'smtp.your-domain.com:587'\n  smtp_from: 'alerts@your-domain.com'\n  smtp_auth_username: 'alerts@your-domain.com'\n  smtp_auth_password: '${SMTP_PASSWORD}'\n  smtp_require_tls: true\n\nroute:\n  group_by: ['alertname', 'cluster', 'service']\n  group_wait: 10s\n  group_interval: 10s\n  repeat_interval: 12h\n  receiver: 'default'\n  routes:\n    - match:\n        severity: critical\n      receiver: 'critical-alerts'\n      group_wait: 5s\n      repeat_interval: 5m\n    - match:\n        severity: warning\n      receiver: 'warning-alerts'\n      repeat_interval: 1h\n    - match:\n        team: database\n      receiver: 'database-team'\n    - match:\n        team: application\n      receiver: 'application-team'\n\nreceivers:\n  - name: 'default'\n    webhook_configs:\n      - url: 'http://webhook-service:5001/alerts'\n        send_resolved: true\n\n  - name: 'critical-alerts'\n    email_configs:\n      - to: 'admin@your-domain.com'\n        subject: 'CRITICAL: PRS Alert - {{ .GroupLabels.alertname }}'\n        body: |\n          {{ range .Alerts }}\n          Alert: {{ .Annotations.summary }}\n          Description: {{ .Annotations.description }}\n          Severity: {{ .Labels.severity }}\n          Instance: {{ .Labels.instance }}\n          Time: {{ .StartsAt }}\n          {{ end }}\n        headers:\n          Priority: 'high'\n    slack_configs:\n      - api_url: '${SLACK_WEBHOOK_URL}'\n        channel: '#prs-critical-alerts'\n        title: 'CRITICAL: PRS Alert'\n        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'\n        color: 'danger'\n\n  - name: 'warning-alerts'\n    email_configs:\n      - to: 'team@your-domain.com'\n        subject: 'WARNING: PRS Alert - {{ .GroupLabels.alertname }}'\n        body: |\n          {{ range .Alerts }}\n          Alert: {{ .Annotations.summary }}\n          Description: {{ .Annotations.description }}\n          Severity: {{ .Labels.severity }}\n          Instance: {{ .Labels.instance }}\n          Time: {{ .StartsAt }}\n          {{ end }}\n\n  - name: 'database-team'\n    email_configs:\n      - to: 'dba@your-domain.com'\n        subject: 'Database Alert: {{ .GroupLabels.alertname }}'\n        body: |\n          Database Alert Details:\n          {{ range .Alerts }}\n          Alert: {{ .Annotations.summary }}\n          Description: {{ .Annotations.description }}\n          Database: {{ .Labels.database }}\n          Instance: {{ .Labels.instance }}\n          Runbook: {{ .Annotations.runbook_url }}\n          {{ end }}\n\n  - name: 'application-team'\n    email_configs:\n      - to: 'developers@your-domain.com'\n        subject: 'Application Alert: {{ .GroupLabels.alertname }}'\n        body: |\n          Application Alert Details:\n          {{ range .Alerts }}\n          Alert: {{ .Annotations.summary }}\n          Description: {{ .Annotations.description }}\n          Service: {{ .Labels.job }}\n          Instance: {{ .Labels.instance }}\n          Runbook: {{ .Annotations.runbook_url }}\n          {{ end }}\n\ninhibit_rules:\n  - source_match:\n      severity: 'critical'\n    target_match:\n      severity: 'warning'\n    equal: ['alertname', 'instance']\n\ntemplates:\n  - '/etc/alertmanager/templates/*.tmpl'\n</code></pre>"},{"location":"configuration/monitoring/#custom-metrics","title":"Custom Metrics","text":""},{"location":"configuration/monitoring/#application-metrics","title":"Application Metrics","text":"<pre><code>// Backend application metrics (Node.js)\nconst prometheus = require('prom-client');\n\n// Create custom metrics\nconst httpRequestDuration = new prometheus.Histogram({\n  name: 'http_request_duration_seconds',\n  help: 'Duration of HTTP requests in seconds',\n  labelNames: ['method', 'route', 'status_code'],\n  buckets: [0.1, 0.3, 0.5, 0.7, 1, 3, 5, 7, 10]\n});\n\nconst httpRequestsTotal = new prometheus.Counter({\n  name: 'http_requests_total',\n  help: 'Total number of HTTP requests',\n  labelNames: ['method', 'route', 'status_code']\n});\n\nconst activeConnections = new prometheus.Gauge({\n  name: 'active_connections',\n  help: 'Number of active connections'\n});\n\nconst databaseQueryDuration = new prometheus.Histogram({\n  name: 'database_query_duration_seconds',\n  help: 'Duration of database queries in seconds',\n  labelNames: ['query_type', 'table'],\n  buckets: [0.01, 0.05, 0.1, 0.3, 0.5, 1, 3, 5]\n});\n\n// Business metrics\nconst requisitionsCreated = new prometheus.Counter({\n  name: 'prs_requisitions_created_total',\n  help: 'Total number of requisitions created'\n});\n\nconst purchaseOrdersGenerated = new prometheus.Counter({\n  name: 'prs_purchase_orders_generated_total',\n  help: 'Total number of purchase orders generated'\n});\n\nconst fileUploads = new prometheus.Counter({\n  name: 'prs_file_uploads_total',\n  help: 'Total number of file uploads',\n  labelNames: ['file_type', 'size_category']\n});\n\n// Middleware to collect metrics\nconst metricsMiddleware = (req, res, next) =&gt; {\n  const start = Date.now();\n\n  res.on('finish', () =&gt; {\n    const duration = (Date.now() - start) / 1000;\n    const route = req.route ? req.route.path : req.path;\n\n    httpRequestDuration\n      .labels(req.method, route, res.statusCode)\n      .observe(duration);\n\n    httpRequestsTotal\n      .labels(req.method, route, res.statusCode)\n      .inc();\n  });\n\n  next();\n};\n\n// Metrics endpoint\napp.get('/metrics', (req, res) =&gt; {\n  res.set('Content-Type', prometheus.register.contentType);\n  res.end(prometheus.register.metrics());\n});\n</code></pre>"},{"location":"configuration/monitoring/#system-metrics-collection","title":"System Metrics Collection","text":"<pre><code>#!/bin/bash\n# Custom metrics collection script\n\nMETRICS_FILE=\"/var/lib/node_exporter/textfile_collector/prs_custom.prom\"\n\n# Collect custom application metrics\ncollect_app_metrics() {\n    # Active user sessions\n    ACTIVE_SESSIONS=$(docker exec prs-onprem-redis redis-cli -a \"$REDIS_PASSWORD\" eval \"return #redis.call('keys', 'session:*')\" 0)\n    echo \"prs_active_sessions_total $ACTIVE_SESSIONS\" &gt;&gt; \"$METRICS_FILE\"\n\n    # Queue lengths\n    QUEUE_LENGTH=$(docker exec prs-onprem-redis redis-cli -a \"$REDIS_PASSWORD\" llen \"queue:default\")\n    echo \"prs_queue_length{queue=\\\"default\\\"} $QUEUE_LENGTH\" &gt;&gt; \"$METRICS_FILE\"\n\n    # File upload storage\n    UPLOAD_SIZE=$(du -sb /mnt/ssd/uploads | cut -f1)\n    echo \"prs_upload_storage_bytes $UPLOAD_SIZE\" &gt;&gt; \"$METRICS_FILE\"\n}\n\n# Collect database metrics\ncollect_db_metrics() {\n    # Table sizes\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT \n        'prs_table_size_bytes{table=\\\"' || tablename || '\\\"} ' || pg_total_relation_size(schemaname||'.'||tablename)\n    FROM pg_tables \n    WHERE schemaname = 'public';\n    \" &gt;&gt; \"$METRICS_FILE\"\n\n    # TimescaleDB chunk metrics\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT \n        'prs_timescaledb_chunks{hypertable=\\\"' || hypertable_name || '\\\",tablespace=\\\"' || tablespace_name || '\\\"} ' || COUNT(*)\n    FROM timescaledb_information.chunks\n    GROUP BY hypertable_name, tablespace_name;\n    \" &gt;&gt; \"$METRICS_FILE\"\n}\n\n# Main execution\nmain() {\n    &gt; \"$METRICS_FILE\"\n    collect_app_metrics\n    collect_db_metrics\n}\n\nmain \"$@\"\n</code></pre> <p>Monitoring Configured</p> <p>Your PRS deployment now has comprehensive monitoring with real-time dashboards, automated alerts, and custom metrics collection.</p> <p>Dashboard Customization</p> <p>Customize Grafana dashboards based on your specific monitoring needs and business requirements.</p> <p>Alert Tuning</p> <p>Regularly review and tune alert thresholds to reduce false positives while maintaining effective monitoring coverage.</p>"},{"location":"configuration/security/","title":"Security Configuration","text":""},{"location":"configuration/security/#overview","title":"Overview","text":"<p>This guide covers comprehensive security hardening for the PRS on-premises deployment, including system-level security, application security, and compliance measures.</p>"},{"location":"configuration/security/#system-security","title":"System Security","text":""},{"location":"configuration/security/#firewall-configuration","title":"Firewall Configuration","text":""},{"location":"configuration/security/#ufw-firewall-setup","title":"UFW Firewall Setup","text":"<pre><code># Reset and configure UFW firewall\nsudo ufw --force reset\nsudo ufw default deny incoming\nsudo ufw default allow outgoing\n\n# SSH access (restrict to admin network)\nsudo ufw allow from 192.168.0.201/24 to any port 22 comment 'SSH admin access'\n\n# HTTP/HTTPS access (internal network only)\nsudo ufw allow from 192.168.0.0/20 to any port 80 comment 'HTTP internal'\nsudo ufw allow from 192.168.0.0/20 to any port 443 comment 'HTTPS internal'\n\n# Management interfaces (admin network only)\nsudo ufw allow from 192.168.0.201/24 to any port 8080 comment 'Adminer'\nsudo ufw allow from 192.168.0.201/24 to any port 3001 comment 'Grafana'\nsudo ufw allow from 192.168.0.201/24 to any port 9000 comment 'Portainer'\nsudo ufw allow from 192.168.0.201/24 to any port 9090 comment 'Prometheus'\n\n# Rate limiting for HTTP services\nsudo ufw limit 80/tcp\nsudo ufw limit 443/tcp\n\n# Enable firewall\nsudo ufw --force enable\nsudo ufw status verbose\n</code></pre>"},{"location":"configuration/security/#advanced-firewall-rules","title":"Advanced Firewall Rules","text":"<pre><code># Block common attack patterns\nsudo ufw deny from 192.168.0.0/20 to any port 22 comment 'Block SSH from clients'\nsudo ufw deny from 172.20.0.0/24 to any port 22 comment 'Block SSH from containers'\n\n# Allow Docker networks\nsudo ufw allow in on docker0\nsudo ufw allow in on br-*\n\n# Log dropped packets\nsudo ufw logging on\n\n# Custom iptables rules for additional security\nsudo iptables -A INPUT -p tcp --dport 22 -m conntrack --ctstate NEW -m recent --set\nsudo iptables -A INPUT -p tcp --dport 22 -m conntrack --ctstate NEW -m recent --update --seconds 60 --hitcount 4 -j DROP\n</code></pre>"},{"location":"configuration/security/#intrusion-detection","title":"Intrusion Detection","text":""},{"location":"configuration/security/#fail2ban-configuration","title":"Fail2Ban Configuration","text":"<pre><code># Install and configure Fail2Ban\nsudo apt install fail2ban\n\n# Create custom configuration\nsudo tee /etc/fail2ban/jail.local &lt;&lt; 'EOF'\n[DEFAULT]\nbantime = 3600\nfindtime = 600\nmaxretry = 3\nbackend = systemd\n\n[sshd]\nenabled = true\nport = ssh\nfilter = sshd\nlogpath = /var/log/auth.log\nmaxretry = 3\nbantime = 3600\n\n[nginx-http-auth]\nenabled = true\nfilter = nginx-http-auth\nlogpath = /var/log/nginx/error.log\nmaxretry = 3\nbantime = 1800\n\n[nginx-limit-req]\nenabled = true\nfilter = nginx-limit-req\nlogpath = /var/log/nginx/error.log\nmaxretry = 10\nbantime = 600\n\n[docker-auth]\nenabled = true\nfilter = docker-auth\nlogpath = /var/log/docker.log\nmaxretry = 3\nbantime = 3600\nEOF\n\n# Create custom filters\nsudo tee /etc/fail2ban/filter.d/nginx-http-auth.conf &lt;&lt; 'EOF'\n[Definition]\nfailregex = ^ \\[error\\] \\d+#\\d+: \\*\\d+ user \"\\S+\":? (password mismatch|was not found in), client: &lt;HOST&gt;, server: \\S+, request: \"\\S+ \\S+ HTTP/\\d+\\.\\d+\", host: \"\\S+\"$\n            ^ \\[error\\] \\d+#\\d+: \\*\\d+ no user/password was provided for basic authentication, client: &lt;HOST&gt;, server: \\S+, request: \"\\S+ \\S+ HTTP/\\d+\\.\\d+\", host: \"\\S+\"$\n\nignoreregex =\nEOF\n\nsudo tee /etc/fail2ban/filter.d/nginx-limit-req.conf &lt;&lt; 'EOF'\n[Definition]\nfailregex = limiting requests, excess: \\S+ by zone \"\\S+\", client: &lt;HOST&gt;\n\nignoreregex =\nEOF\n\n# Start and enable Fail2Ban\nsudo systemctl enable fail2ban\nsudo systemctl start fail2ban\nsudo fail2ban-client status\n</code></pre>"},{"location":"configuration/security/#system-hardening","title":"System Hardening","text":""},{"location":"configuration/security/#kernel-security-parameters","title":"Kernel Security Parameters","text":"<pre><code># Apply security-focused kernel parameters\nsudo tee -a /etc/sysctl.conf &lt;&lt; 'EOF'\n# Network security\nnet.ipv4.ip_forward = 0\nnet.ipv4.conf.all.send_redirects = 0\nnet.ipv4.conf.default.send_redirects = 0\nnet.ipv4.conf.all.accept_redirects = 0\nnet.ipv4.conf.default.accept_redirects = 0\nnet.ipv4.conf.all.accept_source_route = 0\nnet.ipv4.conf.default.accept_source_route = 0\nnet.ipv4.conf.all.log_martians = 1\nnet.ipv4.conf.default.log_martians = 1\nnet.ipv4.icmp_echo_ignore_broadcasts = 1\nnet.ipv4.icmp_ignore_bogus_error_responses = 1\nnet.ipv4.tcp_syncookies = 1\n\n# IPv6 security (disable if not used)\nnet.ipv6.conf.all.disable_ipv6 = 1\nnet.ipv6.conf.default.disable_ipv6 = 1\n\n# Kernel security\nkernel.dmesg_restrict = 1\nkernel.kptr_restrict = 2\nkernel.yama.ptrace_scope = 1\nfs.protected_hardlinks = 1\nfs.protected_symlinks = 1\nEOF\n\n# Apply changes\nsudo sysctl -p\n</code></pre>"},{"location":"configuration/security/#file-system-security","title":"File System Security","text":"<pre><code># Set secure file permissions\nsudo chmod 600 /etc/shadow\nsudo chmod 600 /etc/gshadow\nsudo chmod 644 /etc/passwd\nsudo chmod 644 /etc/group\n\n# Secure important directories\nsudo chmod 700 /root\nsudo chmod 755 /etc\nsudo chmod 755 /var\n\n# Set immutable flag on critical files\nsudo chattr +i /etc/passwd\nsudo chattr +i /etc/shadow\nsudo chattr +i /etc/group\nsudo chattr +i /etc/gshadow\n\n# Create secure tmp directory\nsudo mount -o remount,noexec,nosuid,nodev /tmp\n</code></pre>"},{"location":"configuration/security/#application-security","title":"Application Security","text":""},{"location":"configuration/security/#container-security","title":"Container Security","text":""},{"location":"configuration/security/#docker-security-configuration","title":"Docker Security Configuration","text":"<pre><code># Security-hardened Docker Compose configuration\nservices:\n  backend:\n    security_opt:\n      - no-new-privileges:true\n      - apparmor:docker-default\n    read_only: true\n    tmpfs:\n      - /tmp\n      - /var/tmp\n    user: \"1000:1000\"\n    cap_drop:\n      - ALL\n    cap_add:\n      - NET_BIND_SERVICE\n\n  postgres:\n    security_opt:\n      - no-new-privileges:true\n    user: \"999:999\"\n    cap_drop:\n      - ALL\n    cap_add:\n      - CHOWN\n      - DAC_OVERRIDE\n      - FOWNER\n      - SETGID\n      - SETUID\n</code></pre>"},{"location":"configuration/security/#container-image-security","title":"Container Image Security","text":"<pre><code># Scan Docker images for vulnerabilities\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\\n  aquasec/trivy image prs-backend:latest\n\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\\n  aquasec/trivy image prs-frontend:latest\n\n# Update base images regularly\ndocker pull timescale/timescaledb:latest-pg15\ndocker pull redis:7-alpine\ndocker pull nginx:1.24-alpine\n</code></pre>"},{"location":"configuration/security/#database-security","title":"Database Security","text":""},{"location":"configuration/security/#postgresql-security-hardening","title":"PostgreSQL Security Hardening","text":"<pre><code>-- Remove default databases and users\nDROP DATABASE IF EXISTS template0;\nDROP DATABASE IF EXISTS template1;\nDROP ROLE IF EXISTS postgres;\n\n-- Create security-focused configuration\nALTER SYSTEM SET log_connections = 'on';\nALTER SYSTEM SET log_disconnections = 'on';\nALTER SYSTEM SET log_statement = 'all';\nALTER SYSTEM SET log_min_duration_statement = 1000;\nALTER SYSTEM SET log_checkpoints = 'on';\nALTER SYSTEM SET log_lock_waits = 'on';\n\n-- Password security\nALTER SYSTEM SET password_encryption = 'scram-sha-256';\nALTER SYSTEM SET ssl = 'on';\nALTER SYSTEM SET ssl_cert_file = '/var/lib/postgresql/ssl/server.crt';\nALTER SYSTEM SET ssl_key_file = '/var/lib/postgresql/ssl/server.key';\n\n-- Connection security\nALTER SYSTEM SET listen_addresses = 'localhost,172.20.0.30';\nALTER SYSTEM SET port = 5432;\n\n-- Reload configuration\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"configuration/security/#database-access-control","title":"Database Access Control","text":"<pre><code>-- Create role hierarchy with minimal privileges\nCREATE ROLE prs_readonly NOLOGIN;\nCREATE ROLE prs_readwrite NOLOGIN;\nCREATE ROLE prs_admin NOLOGIN;\n\n-- Grant minimal required permissions\nGRANT CONNECT ON DATABASE prs_production TO prs_readonly;\nGRANT USAGE ON SCHEMA public TO prs_readonly;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO prs_readonly;\n\nGRANT prs_readonly TO prs_readwrite;\nGRANT INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO prs_readwrite;\nGRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO prs_readwrite;\n\nGRANT prs_readwrite TO prs_admin;\nGRANT CREATE ON SCHEMA public TO prs_admin;\n\n-- Create application users with strong passwords\nCREATE USER prs_app_user WITH \n  PASSWORD 'secure_random_password_32_chars'\n  CONNECTION LIMIT 50\n  VALID UNTIL 'infinity';\nGRANT prs_readwrite TO prs_app_user;\n\n-- Revoke dangerous functions from public\nREVOKE EXECUTE ON FUNCTION pg_read_file(text) FROM PUBLIC;\nREVOKE EXECUTE ON FUNCTION pg_ls_dir(text) FROM PUBLIC;\nREVOKE EXECUTE ON FUNCTION pg_read_binary_file(text) FROM PUBLIC;\n</code></pre>"},{"location":"configuration/security/#application-security-headers","title":"Application Security Headers","text":""},{"location":"configuration/security/#nginx-security-configuration","title":"Nginx Security Configuration","text":"<pre><code># Security headers configuration\nserver {\n    # Security headers\n    add_header X-Frame-Options \"DENY\" always;\n    add_header X-Content-Type-Options \"nosniff\" always;\n    add_header X-XSS-Protection \"1; mode=block\" always;\n    add_header Referrer-Policy \"strict-origin-when-cross-origin\" always;\n    add_header Content-Security-Policy \"default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self'; connect-src 'self'; frame-ancestors 'none';\" always;\n    add_header Permissions-Policy \"geolocation=(), microphone=(), camera=()\" always;\n\n    # HSTS with preload\n    add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains; preload\" always;\n\n    # Hide server information\n    server_tokens off;\n\n    # Rate limiting\n    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;\n    limit_req_zone $binary_remote_addr zone=login:10m rate=1r/s;\n\n    location /api/ {\n        limit_req zone=api burst=20 nodelay;\n        limit_req_status 429;\n    }\n\n    location /api/auth/login {\n        limit_req zone=login burst=3 nodelay;\n        limit_req_status 429;\n    }\n\n    # Block common attack patterns\n    location ~* \\.(php|asp|aspx|jsp)$ {\n        deny all;\n    }\n\n    location ~* /\\.(git|svn|hg) {\n        deny all;\n    }\n\n    location ~* \\.(env|config|ini|log|bak)$ {\n        deny all;\n    }\n}\n</code></pre>"},{"location":"configuration/security/#ssltls-security","title":"SSL/TLS Security","text":""},{"location":"configuration/security/#ssl-configuration-hardening","title":"SSL Configuration Hardening","text":"<pre><code># Modern SSL configuration\nssl_protocols TLSv1.2 TLSv1.3;\nssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384;\nssl_prefer_server_ciphers off;\n\n# SSL session optimization\nssl_session_cache shared:SSL:50m;\nssl_session_timeout 1d;\nssl_session_tickets off;\n\n# OCSP stapling\nssl_stapling on;\nssl_stapling_verify on;\nssl_trusted_certificate /etc/nginx/ssl/ca-bundle.crt;\nresolver 8.8.8.8 8.8.4.4 valid=300s;\nresolver_timeout 5s;\n\n# DH parameters for perfect forward secrecy\nssl_dhparam /etc/nginx/ssl/dhparam.pem;\n</code></pre>"},{"location":"configuration/security/#certificate-security","title":"Certificate Security","text":"<pre><code># Generate strong DH parameters\nsudo openssl dhparam -out /opt/prs-deployment/02-docker-configuration/ssl/dhparam.pem 2048\n\n# Set secure permissions\nsudo chmod 600 /opt/prs-deployment/02-docker-configuration/ssl/dhparam.pem\n\n# Certificate monitoring script\ncat &gt; /opt/prs-deployment/scripts/ssl-monitor.sh &lt;&lt; 'EOF'\n#!/bin/bash\nCERT_FILE=\"/opt/prs-deployment/02-docker-configuration/ssl/certificate.crt\"\nDAYS_WARNING=30\n\n# Check certificate expiration\nEXPIRY_DATE=$(openssl x509 -in \"$CERT_FILE\" -noout -enddate | cut -d= -f2)\nEXPIRY_EPOCH=$(date -d \"$EXPIRY_DATE\" +%s)\nCURRENT_EPOCH=$(date +%s)\nDAYS_UNTIL_EXPIRY=$(( (EXPIRY_EPOCH - CURRENT_EPOCH) / 86400 ))\n\nif [ $DAYS_UNTIL_EXPIRY -lt $DAYS_WARNING ]; then\n    echo \"WARNING: SSL certificate expires in $DAYS_UNTIL_EXPIRY days\"\n    # Send alert email or notification\nfi\nEOF\n\nchmod +x /opt/prs-deployment/scripts/ssl-monitor.sh\n\n# Add to crontab for daily monitoring\n(crontab -l 2&gt;/dev/null; echo \"0 6 * * * /opt/prs-deployment/scripts/ssl-monitor.sh\") | crontab -\n</code></pre>"},{"location":"configuration/security/#access-control","title":"Access Control","text":""},{"location":"configuration/security/#user-management","title":"User Management","text":""},{"location":"configuration/security/#system-user-security","title":"System User Security","text":"<pre><code># Disable unused system accounts\nsudo usermod -s /usr/sbin/nologin bin\nsudo usermod -s /usr/sbin/nologin daemon\nsudo usermod -s /usr/sbin/nologin adm\nsudo usermod -s /usr/sbin/nologin lp\nsudo usermod -s /usr/sbin/nologin sync\nsudo usermod -s /usr/sbin/nologin shutdown\nsudo usermod -s /usr/sbin/nologin halt\nsudo usermod -s /usr/sbin/nologin mail\nsudo usermod -s /usr/sbin/nologin news\nsudo usermod -s /usr/sbin/nologin uucp\nsudo usermod -s /usr/sbin/nologin operator\nsudo usermod -s /usr/sbin/nologin games\nsudo usermod -s /usr/sbin/nologin gopher\nsudo usermod -s /usr/sbin/nologin ftp\n\n# Set password policies\nsudo tee -a /etc/login.defs &lt;&lt; 'EOF'\nPASS_MAX_DAYS 90\nPASS_MIN_DAYS 1\nPASS_WARN_AGE 7\nPASS_MIN_LEN 12\nEOF\n\n# Configure PAM for strong passwords\nsudo tee -a /etc/pam.d/common-password &lt;&lt; 'EOF'\npassword requisite pam_pwquality.so retry=3 minlen=12 difok=3 ucredit=-1 lcredit=-1 dcredit=-1 ocredit=-1\nEOF\n</code></pre>"},{"location":"configuration/security/#ssh-security","title":"SSH Security","text":"<pre><code># Harden SSH configuration\nsudo tee /etc/ssh/sshd_config.d/99-prs-security.conf &lt;&lt; 'EOF'\n# Protocol and encryption\nProtocol 2\nCiphers chacha20-poly1305@openssh.com,aes256-gcm@openssh.com,aes128-gcm@openssh.com,aes256-ctr,aes192-ctr,aes128-ctr\nMACs hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha2-256,hmac-sha2-512\nKexAlgorithms curve25519-sha256@libssh.org,diffie-hellman-group16-sha512,diffie-hellman-group18-sha512\n\n# Authentication\nPermitRootLogin no\nPasswordAuthentication no\nPubkeyAuthentication yes\nAuthenticationMethods publickey\nMaxAuthTries 3\nMaxSessions 2\n\n# Network restrictions\nAllowUsers prs-deploy\nAllowGroups sudo\nDenyUsers root\nClientAliveInterval 300\nClientAliveCountMax 2\n\n# Logging\nLogLevel VERBOSE\nSyslogFacility AUTH\n\n# Disable dangerous features\nPermitEmptyPasswords no\nPermitUserEnvironment no\nAllowAgentForwarding no\nAllowTcpForwarding no\nX11Forwarding no\nPrintMotd no\nEOF\n\n# Restart SSH service\nsudo systemctl restart sshd\n</code></pre>"},{"location":"configuration/security/#monitoring-and-auditing","title":"Monitoring and Auditing","text":""},{"location":"configuration/security/#security-monitoring","title":"Security Monitoring","text":"<pre><code># Install security monitoring tools\nsudo apt install aide rkhunter chkrootkit\n\n# Configure AIDE (Advanced Intrusion Detection Environment)\nsudo aideinit\nsudo mv /var/lib/aide/aide.db.new /var/lib/aide/aide.db\n\n# Create AIDE monitoring script\ncat &gt; /opt/prs-deployment/scripts/security-monitor.sh &lt;&lt; 'EOF'\n#!/bin/bash\nLOG_FILE=\"/var/log/prs-security.log\"\n\n# Run AIDE check\necho \"$(date): Running AIDE integrity check\" &gt;&gt; \"$LOG_FILE\"\nif ! aide --check; then\n    echo \"$(date): AIDE detected file system changes\" &gt;&gt; \"$LOG_FILE\"\n    # Send alert\nfi\n\n# Run rootkit check\necho \"$(date): Running rootkit scan\" &gt;&gt; \"$LOG_FILE\"\nif ! rkhunter --check --skip-keypress; then\n    echo \"$(date): rkhunter detected potential issues\" &gt;&gt; \"$LOG_FILE\"\n    # Send alert\nfi\n\n# Check for failed login attempts\nFAILED_LOGINS=$(grep \"Failed password\" /var/log/auth.log | wc -l)\nif [ $FAILED_LOGINS -gt 10 ]; then\n    echo \"$(date): High number of failed login attempts: $FAILED_LOGINS\" &gt;&gt; \"$LOG_FILE\"\n    # Send alert\nfi\nEOF\n\nchmod +x /opt/prs-deployment/scripts/security-monitor.sh\n\n# Add to crontab for daily security checks\n(crontab -l 2&gt;/dev/null; echo \"0 3 * * * /opt/prs-deployment/scripts/security-monitor.sh\") | crontab -\n</code></pre>"},{"location":"configuration/security/#audit-logging","title":"Audit Logging","text":"<pre><code># Install and configure auditd\nsudo apt install auditd audispd-plugins\n\n# Configure audit rules\nsudo tee /etc/audit/rules.d/prs-audit.rules &lt;&lt; 'EOF'\n# Monitor file access\n-w /etc/passwd -p wa -k identity\n-w /etc/group -p wa -k identity\n-w /etc/shadow -p wa -k identity\n-w /etc/sudoers -p wa -k identity\n\n# Monitor system calls\n-a always,exit -F arch=b64 -S adjtimex -S settimeofday -k time-change\n-a always,exit -F arch=b32 -S adjtimex -S settimeofday -S stime -k time-change\n\n# Monitor network configuration\n-w /etc/issue -p wa -k system-locale\n-w /etc/issue.net -p wa -k system-locale\n-w /etc/hosts -p wa -k system-locale\n-w /etc/network -p wa -k system-locale\n\n# Monitor Docker\n-w /var/lib/docker -p wa -k docker\n-w /etc/docker -p wa -k docker\n\n# Monitor PRS application\n-w /opt/prs-deployment -p wa -k prs-config\n-w /mnt/ssd -p wa -k prs-data\n-w /mnt/hdd -p wa -k prs-data\nEOF\n\n# Restart auditd\nsudo systemctl restart auditd\nsudo systemctl enable auditd\n</code></pre>"},{"location":"configuration/security/#compliance-and-backup-security","title":"Compliance and Backup Security","text":""},{"location":"configuration/security/#backup-encryption","title":"Backup Encryption","text":"<pre><code># Encrypt database backups\ncat &gt; /opt/prs-deployment/scripts/encrypted-backup.sh &lt;&lt; 'EOF'\n#!/bin/bash\nBACKUP_DIR=\"/mnt/hdd/postgres-backups/encrypted\"\nGPG_RECIPIENT=\"backup@your-domain.com\"\nDATE=$(date +%Y%m%d_%H%M%S)\n\n# Create encrypted backup\ndocker exec prs-onprem-postgres-timescale pg_dump -U prs_admin -d prs_production | \\\n  gzip | \\\n  gpg --trust-model always --encrypt -r \"$GPG_RECIPIENT\" &gt; \\\n  \"$BACKUP_DIR/prs_encrypted_backup_${DATE}.sql.gz.gpg\"\n\n# Verify backup\nif [ -f \"$BACKUP_DIR/prs_encrypted_backup_${DATE}.sql.gz.gpg\" ]; then\n    echo \"$(date): Encrypted backup created successfully\" &gt;&gt; /var/log/prs-backup.log\nelse\n    echo \"$(date): Encrypted backup failed\" &gt;&gt; /var/log/prs-backup.log\nfi\nEOF\n\nchmod +x /opt/prs-deployment/scripts/encrypted-backup.sh\n</code></pre>"},{"location":"configuration/security/#security-compliance-checklist","title":"Security Compliance Checklist","text":"<pre><code># Create compliance validation script\ncat &gt; /opt/prs-deployment/scripts/compliance-check.sh &lt;&lt; 'EOF'\n#!/bin/bash\nREPORT_FILE=\"/tmp/prs-compliance-report.txt\"\n\necho \"PRS Security Compliance Report - $(date)\" &gt; \"$REPORT_FILE\"\necho \"================================================\" &gt;&gt; \"$REPORT_FILE\"\n\n# Check firewall status\nif sudo ufw status | grep -q \"Status: active\"; then\n    echo \"\u2713 Firewall is active\" &gt;&gt; \"$REPORT_FILE\"\nelse\n    echo \"\u2717 Firewall is not active\" &gt;&gt; \"$REPORT_FILE\"\nfi\n\n# Check fail2ban status\nif sudo systemctl is-active fail2ban &gt;/dev/null; then\n    echo \"\u2713 Fail2Ban is running\" &gt;&gt; \"$REPORT_FILE\"\nelse\n    echo \"\u2717 Fail2Ban is not running\" &gt;&gt; \"$REPORT_FILE\"\nfi\n\n# Check SSL certificate validity\nif openssl x509 -in /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt -checkend 2592000 &gt;/dev/null; then\n    echo \"\u2713 SSL certificate is valid for 30+ days\" &gt;&gt; \"$REPORT_FILE\"\nelse\n    echo \"\u2717 SSL certificate expires within 30 days\" &gt;&gt; \"$REPORT_FILE\"\nfi\n\n# Check audit daemon\nif sudo systemctl is-active auditd &gt;/dev/null; then\n    echo \"\u2713 Audit daemon is running\" &gt;&gt; \"$REPORT_FILE\"\nelse\n    echo \"\u2717 Audit daemon is not running\" &gt;&gt; \"$REPORT_FILE\"\nfi\n\n# Check for security updates\nUPDATES=$(apt list --upgradable 2&gt;/dev/null | grep -c security)\nif [ \"$UPDATES\" -eq 0 ]; then\n    echo \"\u2713 No security updates pending\" &gt;&gt; \"$REPORT_FILE\"\nelse\n    echo \"\u2717 $UPDATES security updates pending\" &gt;&gt; \"$REPORT_FILE\"\nfi\n\ncat \"$REPORT_FILE\"\nEOF\n\nchmod +x /opt/prs-deployment/scripts/compliance-check.sh\n</code></pre> <p>Security Hardened</p> <p>Your PRS deployment now has enterprise-grade security with comprehensive protection against common threats and compliance with security best practices.</p> <p>Regular Security Reviews</p> <p>Perform monthly security reviews using the compliance check script and update security configurations based on new threats and vulnerabilities.</p> <p>Security Maintenance</p> <p>Security is an ongoing process. Regularly update systems, monitor logs, and review access controls to maintain a strong security posture.</p>"},{"location":"database/backup/","title":"Database Backup","text":""},{"location":"database/backup/#overview","title":"Overview","text":"<p>This guide covers comprehensive database backup strategies for the PRS on-premises deployment, including automated backups, point-in-time recovery, and disaster recovery procedures.</p>"},{"location":"database/backup/#backup-strategy","title":"Backup Strategy","text":""},{"location":"database/backup/#backup-types","title":"Backup Types","text":""},{"location":"database/backup/#full-backups","title":"Full Backups","text":"<ul> <li>Frequency: Daily at 2:00 AM</li> <li>Retention: 30 days local, 90 days offsite</li> <li>Size: Complete database dump</li> <li>Recovery Time: 15-30 minutes</li> </ul>"},{"location":"database/backup/#incremental-backups","title":"Incremental Backups","text":"<ul> <li>Frequency: Every 6 hours</li> <li>Retention: 7 days</li> <li>Size: Changes since last backup</li> <li>Recovery Time: 5-15 minutes</li> </ul>"},{"location":"database/backup/#wal-archiving","title":"WAL Archiving","text":"<ul> <li>Frequency: Continuous</li> <li>Retention: 7 days</li> <li>Size: Transaction logs</li> <li>Recovery Time: Point-in-time recovery</li> </ul>"},{"location":"database/backup/#backup-architecture","title":"Backup Architecture","text":"<pre><code>graph TB\n    subgraph \"Production Database\"\n        DB[TimescaleDB&lt;br/&gt;PostgreSQL 15] --&gt; WAL[WAL Files&lt;br/&gt;Continuous]\n        DB --&gt; FULL[Full Backup&lt;br/&gt;Daily]\n        DB --&gt; INCR[Incremental Backup&lt;br/&gt;6 Hours]\n    end\n\n    subgraph \"Local Storage\"\n        WAL --&gt; WALDIR[/mnt/hdd/wal-archive]\n        FULL --&gt; FULLDIR[/mnt/hdd/postgres-backups/daily]\n        INCR --&gt; INCRDIR[/mnt/hdd/postgres-backups/incremental]\n    end\n\n    subgraph \"Backup Processing\"\n        FULLDIR --&gt; COMPRESS[Compression&lt;br/&gt;gzip -9]\n        INCRDIR --&gt; COMPRESS\n        COMPRESS --&gt; ENCRYPT[Encryption&lt;br/&gt;GPG]\n        ENCRYPT --&gt; VERIFY[Integrity Check&lt;br/&gt;SHA256]\n    end\n\n    subgraph \"Offsite Storage\"\n        VERIFY --&gt; OFFSITE[Remote Backup&lt;br/&gt;rsync/S3]\n        VERIFY --&gt; TAPE[Tape Backup&lt;br/&gt;Monthly]\n    end\n\n    style DB fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style COMPRESS fill:#e8f5e8,stroke:#4caf50,stroke-width:2px\n    style ENCRYPT fill:#fff3e0,stroke:#ff9800,stroke-width:2px\n    style OFFSITE fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px</code></pre>"},{"location":"database/backup/#automated-backup-configuration","title":"Automated Backup Configuration","text":""},{"location":"database/backup/#backup-scripts","title":"Backup Scripts","text":""},{"location":"database/backup/#full-backup-script","title":"Full Backup Script","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/backup-full.sh\n\nset -euo pipefail\n\n# Configuration\nBACKUP_DIR=\"/mnt/hdd/postgres-backups/daily\"\nRETENTION_DAYS=30\nCOMPRESSION_LEVEL=9\nENCRYPT_KEY=\"backup@your-domain.com\"\n\n# Database connection\nPGHOST=\"localhost\"\nPGPORT=\"5432\"\nPGUSER=\"prs_admin\"\nPGDATABASE=\"prs_production\"\n\n# Logging\nLOG_FILE=\"/var/log/prs-backup.log\"\nDATE=$(date +%Y%m%d_%H%M%S)\nBACKUP_FILE=\"$BACKUP_DIR/prs_full_backup_${DATE}.sql\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\n# Create backup directory\nmkdir -p \"$BACKUP_DIR\"\n\nlog_message \"Starting full database backup\"\n\n# Pre-backup checks\nlog_message \"Checking database connectivity\"\nif ! docker exec prs-onprem-postgres-timescale pg_isready -U \"$PGUSER\"; then\n    log_message \"ERROR: Database not ready\"\n    exit 1\nfi\n\n# Check available space\nAVAILABLE_SPACE=$(df \"$BACKUP_DIR\" | awk 'NR==2 {print $4}')\nREQUIRED_SPACE=5000000  # 5GB in KB\n\nif [ \"$AVAILABLE_SPACE\" -lt \"$REQUIRED_SPACE\" ]; then\n    log_message \"ERROR: Insufficient disk space for backup\"\n    exit 1\nfi\n\n# Create backup\nlog_message \"Creating database backup: $BACKUP_FILE\"\ndocker exec prs-onprem-postgres-timescale pg_dump \\\n    -U \"$PGUSER\" \\\n    -d \"$PGDATABASE\" \\\n    --verbose \\\n    --format=custom \\\n    --compress=9 \\\n    --no-owner \\\n    --no-privileges \\\n    &gt; \"$BACKUP_FILE\"\n\nif [ $? -eq 0 ]; then\n    log_message \"Database backup completed successfully\"\nelse\n    log_message \"ERROR: Database backup failed\"\n    exit 1\nfi\n\n# Compress backup\nlog_message \"Compressing backup\"\ngzip -\"$COMPRESSION_LEVEL\" \"$BACKUP_FILE\"\nBACKUP_FILE=\"${BACKUP_FILE}.gz\"\n\n# Generate checksum\nlog_message \"Generating checksum\"\nsha256sum \"$BACKUP_FILE\" &gt; \"${BACKUP_FILE}.sha256\"\n\n# Encrypt backup (optional)\nif command -v gpg &gt;/dev/null 2&gt;&amp;1 &amp;&amp; gpg --list-keys \"$ENCRYPT_KEY\" &gt;/dev/null 2&gt;&amp;1; then\n    log_message \"Encrypting backup\"\n    gpg --trust-model always --encrypt -r \"$ENCRYPT_KEY\" \"$BACKUP_FILE\"\n    rm \"$BACKUP_FILE\"\n    BACKUP_FILE=\"${BACKUP_FILE}.gpg\"\nfi\n\n# Verify backup integrity\nlog_message \"Verifying backup integrity\"\nBACKUP_SIZE=$(stat -c%s \"$BACKUP_FILE\")\nif [ \"$BACKUP_SIZE\" -gt 1000000 ]; then  # At least 1MB\n    log_message \"Backup verification successful (Size: $(numfmt --to=iec $BACKUP_SIZE))\"\nelse\n    log_message \"ERROR: Backup file too small, possible corruption\"\n    exit 1\nfi\n\n# Cleanup old backups\nlog_message \"Cleaning up old backups (retention: $RETENTION_DAYS days)\"\nfind \"$BACKUP_DIR\" -name \"prs_full_backup_*.sql*\" -mtime +$RETENTION_DAYS -delete\n\n# Log backup completion\nlog_message \"Full backup completed: $BACKUP_FILE\"\nlog_message \"Backup size: $(numfmt --to=iec $BACKUP_SIZE)\"\n\n# Send notification (optional)\nif command -v mail &gt;/dev/null 2&gt;&amp;1; then\n    echo \"Database backup completed successfully at $(date)\" | \\\n    mail -s \"PRS Database Backup Success\" admin@your-domain.com\nfi\n</code></pre>"},{"location":"database/backup/#incremental-backup-script","title":"Incremental Backup Script","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/backup-incremental.sh\n\nset -euo pipefail\n\n# Configuration\nBACKUP_DIR=\"/mnt/hdd/postgres-backups/incremental\"\nBASE_BACKUP_DIR=\"/mnt/hdd/postgres-backups/daily\"\nRETENTION_DAYS=7\n\n# Logging\nLOG_FILE=\"/var/log/prs-backup.log\"\nDATE=$(date +%Y%m%d_%H%M%S)\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\n# Create backup directory\nmkdir -p \"$BACKUP_DIR\"\n\nlog_message \"Starting incremental backup\"\n\n# Find latest full backup\nLATEST_FULL=$(ls -t \"$BASE_BACKUP_DIR\"/prs_full_backup_*.sql* 2&gt;/dev/null | head -1)\nif [ -z \"$LATEST_FULL\" ]; then\n    log_message \"ERROR: No full backup found, running full backup first\"\n    /opt/prs-deployment/scripts/backup-full.sh\n    exit 0\nfi\n\nlog_message \"Base backup: $LATEST_FULL\"\n\n# Create incremental backup using WAL files\nINCREMENTAL_FILE=\"$BACKUP_DIR/prs_incremental_backup_${DATE}.tar\"\n\nlog_message \"Creating incremental backup: $INCREMENTAL_FILE\"\n\n# Archive WAL files since last backup\nLAST_BACKUP_TIME=$(stat -c %Y \"$LATEST_FULL\")\nfind /mnt/hdd/wal-archive -name \"*.wal\" -newer \"$LATEST_FULL\" -print0 | \\\ntar --null -czf \"$INCREMENTAL_FILE\" --files-from=-\n\nif [ $? -eq 0 ]; then\n    log_message \"Incremental backup completed successfully\"\nelse\n    log_message \"ERROR: Incremental backup failed\"\n    exit 1\nfi\n\n# Generate checksum\nsha256sum \"$INCREMENTAL_FILE\" &gt; \"${INCREMENTAL_FILE}.sha256\"\n\n# Cleanup old incremental backups\nfind \"$BACKUP_DIR\" -name \"prs_incremental_backup_*.tar*\" -mtime +$RETENTION_DAYS -delete\n\nlog_message \"Incremental backup completed: $INCREMENTAL_FILE\"\n</code></pre>"},{"location":"database/backup/#wal-archiving-configuration","title":"WAL Archiving Configuration","text":""},{"location":"database/backup/#postgresql-configuration","title":"PostgreSQL Configuration","text":"<pre><code>-- Enable WAL archiving\nALTER SYSTEM SET wal_level = 'replica';\nALTER SYSTEM SET archive_mode = 'on';\nALTER SYSTEM SET archive_command = 'cp %p /mnt/hdd/wal-archive/%f';\nALTER SYSTEM SET archive_timeout = '300s';\nALTER SYSTEM SET max_wal_senders = 3;\nALTER SYSTEM SET wal_keep_segments = 64;\n\n-- Reload configuration\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"database/backup/#wal-archive-management","title":"WAL Archive Management","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/wal-archive-cleanup.sh\n\nWAL_ARCHIVE_DIR=\"/mnt/hdd/wal-archive\"\nRETENTION_DAYS=7\nLOG_FILE=\"/var/log/prs-backup.log\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nlog_message \"Starting WAL archive cleanup\"\n\n# Remove old WAL files\nDELETED_COUNT=$(find \"$WAL_ARCHIVE_DIR\" -name \"*.wal\" -mtime +$RETENTION_DAYS -delete -print | wc -l)\n\nlog_message \"Deleted $DELETED_COUNT old WAL files\"\n\n# Check WAL archive size\nARCHIVE_SIZE=$(du -sh \"$WAL_ARCHIVE_DIR\" | cut -f1)\nlog_message \"Current WAL archive size: $ARCHIVE_SIZE\"\n\n# Alert if archive is too large\nARCHIVE_SIZE_BYTES=$(du -sb \"$WAL_ARCHIVE_DIR\" | cut -f1)\nMAX_SIZE_BYTES=$((10 * 1024 * 1024 * 1024))  # 10GB\n\nif [ \"$ARCHIVE_SIZE_BYTES\" -gt \"$MAX_SIZE_BYTES\" ]; then\n    log_message \"WARNING: WAL archive size exceeds 10GB\"\n    echo \"WAL archive size is $ARCHIVE_SIZE, exceeding 10GB limit\" | \\\n    mail -s \"WAL Archive Size Warning\" admin@your-domain.com\nfi\n</code></pre>"},{"location":"database/backup/#backup-scheduling","title":"Backup Scheduling","text":""},{"location":"database/backup/#cron-configuration","title":"Cron Configuration","text":"<pre><code># Add backup jobs to crontab\n(crontab -l 2&gt;/dev/null; cat &lt;&lt; 'EOF'\n# PRS Database Backup Schedule\n\n# Full backup daily at 2:00 AM\n0 2 * * * /opt/prs-deployment/scripts/backup-full.sh\n\n# Incremental backup every 6 hours\n0 */6 * * * /opt/prs-deployment/scripts/backup-incremental.sh\n\n# WAL archive cleanup daily at 3:00 AM\n0 3 * * * /opt/prs-deployment/scripts/wal-archive-cleanup.sh\n\n# Backup verification daily at 4:00 AM\n0 4 * * * /opt/prs-deployment/scripts/backup-verify.sh\n\n# Offsite backup daily at 5:00 AM\n0 5 * * * /opt/prs-deployment/scripts/backup-offsite.sh\nEOF\n) | crontab -\n</code></pre>"},{"location":"database/backup/#backup-verification","title":"Backup Verification","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/backup-verify.sh\n\nBACKUP_DIR=\"/mnt/hdd/postgres-backups/daily\"\nLOG_FILE=\"/var/log/prs-backup.log\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nlog_message \"Starting backup verification\"\n\n# Find latest backup\nLATEST_BACKUP=$(ls -t \"$BACKUP_DIR\"/prs_full_backup_*.sql* 2&gt;/dev/null | head -1)\n\nif [ -z \"$LATEST_BACKUP\" ]; then\n    log_message \"ERROR: No backup found for verification\"\n    exit 1\nfi\n\nlog_message \"Verifying backup: $LATEST_BACKUP\"\n\n# Verify checksum\nif [ -f \"${LATEST_BACKUP}.sha256\" ]; then\n    if sha256sum -c \"${LATEST_BACKUP}.sha256\" &gt;/dev/null 2&gt;&amp;1; then\n        log_message \"Checksum verification passed\"\n    else\n        log_message \"ERROR: Checksum verification failed\"\n        exit 1\n    fi\nelse\n    log_message \"WARNING: No checksum file found\"\nfi\n\n# Test backup restore (to temporary database)\nlog_message \"Testing backup restore\"\n\n# Create test database\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -c \"DROP DATABASE IF EXISTS prs_backup_test;\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -c \"CREATE DATABASE prs_backup_test;\"\n\n# Restore backup to test database\nif [[ \"$LATEST_BACKUP\" == *.gpg ]]; then\n    # Decrypt and restore\n    gpg --quiet --decrypt \"$LATEST_BACKUP\" | \\\n    gunzip | \\\n    docker exec -i prs-onprem-postgres-timescale pg_restore -U prs_admin -d prs_backup_test --clean --if-exists\nelse\n    # Direct restore\n    gunzip -c \"$LATEST_BACKUP\" | \\\n    docker exec -i prs-onprem-postgres-timescale pg_restore -U prs_admin -d prs_backup_test --clean --if-exists\nfi\n\nif [ $? -eq 0 ]; then\n    log_message \"Backup restore test successful\"\n\n    # Verify data integrity\n    TABLE_COUNT=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_backup_test -t -c \"SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';\" | xargs)\n    log_message \"Restored $TABLE_COUNT tables\"\n\n    if [ \"$TABLE_COUNT\" -gt 10 ]; then\n        log_message \"Data integrity check passed\"\n    else\n        log_message \"WARNING: Low table count, possible incomplete restore\"\n    fi\nelse\n    log_message \"ERROR: Backup restore test failed\"\n    exit 1\nfi\n\n# Cleanup test database\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -c \"DROP DATABASE prs_backup_test;\"\n\nlog_message \"Backup verification completed successfully\"\n</code></pre>"},{"location":"database/backup/#point-in-time-recovery","title":"Point-in-Time Recovery","text":""},{"location":"database/backup/#recovery-preparation","title":"Recovery Preparation","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/prepare-recovery.sh\n\nRECOVERY_TIME=\"$1\"\nRECOVERY_DIR=\"/tmp/prs-recovery-$(date +%Y%m%d_%H%M%S)\"\n\nif [ -z \"$RECOVERY_TIME\" ]; then\n    echo \"Usage: $0 'YYYY-MM-DD HH:MM:SS'\"\n    echo \"Example: $0 '2024-08-22 14:30:00'\"\n    exit 1\nfi\n\necho \"Preparing point-in-time recovery to: $RECOVERY_TIME\"\necho \"Recovery directory: $RECOVERY_DIR\"\n\n# Create recovery directory\nmkdir -p \"$RECOVERY_DIR\"\n\n# Find appropriate base backup\nRECOVERY_TIMESTAMP=$(date -d \"$RECOVERY_TIME\" +%s)\nBASE_BACKUP=\"\"\n\nfor backup in $(ls -t /mnt/hdd/postgres-backups/daily/prs_full_backup_*.sql*); do\n    BACKUP_DATE=$(echo \"$backup\" | grep -o '[0-9]\\{8\\}_[0-9]\\{6\\}')\n    BACKUP_TIMESTAMP=$(date -d \"${BACKUP_DATE:0:8} ${BACKUP_DATE:9:2}:${BACKUP_DATE:11:2}:${BACKUP_DATE:13:2}\" +%s)\n\n    if [ \"$BACKUP_TIMESTAMP\" -le \"$RECOVERY_TIMESTAMP\" ]; then\n        BASE_BACKUP=\"$backup\"\n        break\n    fi\ndone\n\nif [ -z \"$BASE_BACKUP\" ]; then\n    echo \"ERROR: No suitable base backup found for recovery time\"\n    exit 1\nfi\n\necho \"Using base backup: $BASE_BACKUP\"\n\n# Copy base backup\ncp \"$BASE_BACKUP\" \"$RECOVERY_DIR/\"\n\n# Collect required WAL files\necho \"Collecting WAL files for recovery...\"\nWAL_START_TIME=$(stat -c %Y \"$BASE_BACKUP\")\n\nfind /mnt/hdd/wal-archive -name \"*.wal\" -newer \"$BASE_BACKUP\" -exec cp {} \"$RECOVERY_DIR/\" \\;\n\necho \"Recovery preparation completed\"\necho \"Recovery files available in: $RECOVERY_DIR\"\necho \"\"\necho \"To perform recovery:\"\necho \"1. Stop the database\"\necho \"2. Restore base backup\"\necho \"3. Configure recovery.conf\"\necho \"4. Start database in recovery mode\"\n</code></pre>"},{"location":"database/backup/#recovery-execution","title":"Recovery Execution","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/execute-recovery.sh\n\nRECOVERY_TIME=\"$1\"\nRECOVERY_DIR=\"$2\"\n\nif [ -z \"$RECOVERY_TIME\" ] || [ -z \"$RECOVERY_DIR\" ]; then\n    echo \"Usage: $0 'YYYY-MM-DD HH:MM:SS' /path/to/recovery/dir\"\n    exit 1\nfi\n\necho \"WARNING: This will replace the current database!\"\necho \"Recovery time: $RECOVERY_TIME\"\necho \"Recovery directory: $RECOVERY_DIR\"\necho \"\"\nread -p \"Are you sure you want to continue? (yes/no): \" confirm\n\nif [ \"$confirm\" != \"yes\" ]; then\n    echo \"Recovery cancelled\"\n    exit 0\nfi\n\n# Stop database\necho \"Stopping database...\"\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml stop postgres\n\n# Backup current data directory\necho \"Backing up current data directory...\"\nsudo mv /mnt/ssd/postgresql-hot /mnt/ssd/postgresql-hot.backup.$(date +%Y%m%d_%H%M%S)\n\n# Create new data directory\nsudo mkdir -p /mnt/ssd/postgresql-hot\nsudo chown 999:999 /mnt/ssd/postgresql-hot\n\n# Initialize new database cluster\ndocker run --rm \\\n    -v /mnt/ssd/postgresql-hot:/var/lib/postgresql/data \\\n    -e POSTGRES_DB=prs_production \\\n    -e POSTGRES_USER=prs_admin \\\n    -e POSTGRES_PASSWORD=\"$POSTGRES_PASSWORD\" \\\n    timescale/timescaledb:latest-pg15 \\\n    initdb\n\n# Restore base backup\necho \"Restoring base backup...\"\nBASE_BACKUP=$(ls \"$RECOVERY_DIR\"/prs_full_backup_*.sql*)\n\nif [[ \"$BASE_BACKUP\" == *.gpg ]]; then\n    gpg --quiet --decrypt \"$BASE_BACKUP\" | gunzip &gt; /tmp/restore.sql\nelse\n    gunzip -c \"$BASE_BACKUP\" &gt; /tmp/restore.sql\nfi\n\n# Start database temporarily for restore\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml up -d postgres\n\n# Wait for database to be ready\nsleep 30\nwhile ! docker exec prs-onprem-postgres-timescale pg_isready -U prs_admin; do\n    echo \"Waiting for database...\"\n    sleep 5\ndone\n\n# Restore data\ndocker exec -i prs-onprem-postgres-timescale pg_restore -U prs_admin -d prs_production --clean --if-exists &lt; /tmp/restore.sql\n\n# Stop database for WAL recovery\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml stop postgres\n\n# Configure recovery\ncat &gt; /tmp/recovery.conf &lt;&lt; EOF\nrestore_command = 'cp $RECOVERY_DIR/%f %p'\nrecovery_target_time = '$RECOVERY_TIME'\nrecovery_target_action = 'promote'\nEOF\n\nsudo cp /tmp/recovery.conf /mnt/ssd/postgresql-hot/\n\n# Start database in recovery mode\necho \"Starting database in recovery mode...\"\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml up -d postgres\n\n# Monitor recovery\necho \"Monitoring recovery progress...\"\nwhile docker exec prs-onprem-postgres-timescale test -f /var/lib/postgresql/data/recovery.conf; do\n    echo \"Recovery in progress...\"\n    sleep 10\ndone\n\necho \"Point-in-time recovery completed!\"\necho \"Database recovered to: $RECOVERY_TIME\"\n\n# Cleanup\nrm -f /tmp/restore.sql /tmp/recovery.conf\n</code></pre>"},{"location":"database/backup/#offsite-backup","title":"Offsite Backup","text":""},{"location":"database/backup/#remote-backup-configuration","title":"Remote Backup Configuration","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/backup-offsite.sh\n\n# Configuration\nLOCAL_BACKUP_DIR=\"/mnt/hdd/postgres-backups\"\nREMOTE_HOST=\"backup-server.your-domain.com\"\nREMOTE_USER=\"backup\"\nREMOTE_DIR=\"/backup/prs-production\"\nLOG_FILE=\"/var/log/prs-backup.log\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nlog_message \"Starting offsite backup sync\"\n\n# Sync daily backups\nlog_message \"Syncing daily backups\"\nrsync -avz --delete \\\n    --include=\"prs_full_backup_*\" \\\n    --exclude=\"*\" \\\n    \"$LOCAL_BACKUP_DIR/daily/\" \\\n    \"$REMOTE_USER@$REMOTE_HOST:$REMOTE_DIR/daily/\"\n\nif [ $? -eq 0 ]; then\n    log_message \"Daily backup sync completed\"\nelse\n    log_message \"ERROR: Daily backup sync failed\"\nfi\n\n# Sync incremental backups (last 7 days only)\nlog_message \"Syncing incremental backups\"\nfind \"$LOCAL_BACKUP_DIR/incremental\" -name \"prs_incremental_backup_*\" -mtime -7 | \\\nrsync -avz --files-from=- / \\\n    \"$REMOTE_USER@$REMOTE_HOST:$REMOTE_DIR/incremental/\"\n\n# Verify remote backups\nlog_message \"Verifying remote backups\"\nREMOTE_COUNT=$(ssh \"$REMOTE_USER@$REMOTE_HOST\" \"find $REMOTE_DIR/daily -name 'prs_full_backup_*' | wc -l\")\nLOCAL_COUNT=$(find \"$LOCAL_BACKUP_DIR/daily\" -name \"prs_full_backup_*\" | wc -l)\n\nif [ \"$REMOTE_COUNT\" -eq \"$LOCAL_COUNT\" ]; then\n    log_message \"Remote backup verification successful ($REMOTE_COUNT files)\"\nelse\n    log_message \"WARNING: Remote backup count mismatch (Local: $LOCAL_COUNT, Remote: $REMOTE_COUNT)\"\nfi\n\nlog_message \"Offsite backup sync completed\"\n</code></pre> <p>Backup System Ready</p> <p>Your PRS database now has comprehensive backup coverage with automated daily backups, point-in-time recovery, and offsite storage.</p> <p>Recovery Testing</p> <p>Regularly test backup restoration procedures to ensure backups are valid and recovery processes work correctly.</p> <p>Backup Monitoring</p> <p>Monitor backup jobs daily and ensure adequate storage space is available for backup retention requirements.</p>"},{"location":"database/maintenance/","title":"Database Maintenance","text":""},{"location":"database/maintenance/#overview","title":"Overview","text":"<p>This guide covers comprehensive database maintenance procedures for the PRS on-premises deployment, including routine maintenance, performance optimization, and preventive care.</p>"},{"location":"database/maintenance/#maintenance-schedule","title":"Maintenance Schedule","text":""},{"location":"database/maintenance/#daily-maintenance-automated","title":"Daily Maintenance (Automated)","text":"Task Time Duration Purpose Statistics Update 01:00 5 min Query optimization Full Backup 02:00 30 min Data protection WAL Archive Cleanup 03:00 5 min Storage management Performance Check 04:00 5 min Health monitoring Log Rotation 05:00 2 min Log management"},{"location":"database/maintenance/#weekly-maintenance-semi-automated","title":"Weekly Maintenance (Semi-automated)","text":"Task Day Duration Purpose VACUUM ANALYZE Sunday 01:00 60 min Table optimization Index Maintenance Sunday 02:30 30 min Index optimization Compression Review Sunday 03:00 15 min Storage optimization Security Audit Sunday 03:30 15 min Security review"},{"location":"database/maintenance/#monthly-maintenance-manual","title":"Monthly Maintenance (Manual)","text":"Task Schedule Duration Purpose Full REINDEX 1st Sunday 02:00 2 hours Index rebuilding Capacity Planning 2nd Sunday 30 min Growth analysis Performance Review 3rd Sunday 45 min Optimization review Disaster Recovery Test 4th Sunday 60 min Recovery validation"},{"location":"database/maintenance/#daily-maintenance-procedures","title":"Daily Maintenance Procedures","text":""},{"location":"database/maintenance/#automated-daily-tasks","title":"Automated Daily Tasks","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/daily-maintenance.sh\n\nset -euo pipefail\n\nLOG_FILE=\"/var/log/prs-maintenance.log\"\nDATE=$(date +%Y%m%d_%H%M%S)\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nlog_message \"Starting daily database maintenance\"\n\n# 1. Update table statistics\nlog_message \"Updating table statistics\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nANALYZE notifications;\nANALYZE audit_logs;\nANALYZE requisitions;\nANALYZE purchase_orders;\nANALYZE users;\nANALYZE departments;\n\"\n\n# 2. Check database health\nlog_message \"Checking database health\"\nHEALTH_CHECK=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\nSELECT \n    CASE \n        WHEN pg_is_in_recovery() THEN 'RECOVERY'\n        WHEN EXISTS (SELECT 1 FROM pg_stat_activity WHERE state = 'active' AND query_start &lt; now() - interval '1 hour') THEN 'SLOW_QUERIES'\n        ELSE 'HEALTHY'\n    END;\n\" | xargs)\n\nlog_message \"Database health status: $HEALTH_CHECK\"\n\n# 3. Monitor table bloat\nlog_message \"Checking table bloat\"\nBLOATED_TABLES=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\nSELECT COUNT(*) FROM pg_stat_user_tables \nWHERE n_dead_tup &gt; 1000 \nAND n_dead_tup::float / NULLIF(n_live_tup + n_dead_tup, 0) &gt; 0.1;\n\" | xargs)\n\nif [ \"$BLOATED_TABLES\" -gt 0 ]; then\n    log_message \"WARNING: $BLOATED_TABLES tables have significant bloat\"\n    # Schedule vacuum for bloated tables\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n    SELECT 'VACUUM ' || schemaname || '.' || tablename || ';' as vacuum_cmd\n    FROM pg_stat_user_tables \n    WHERE n_dead_tup &gt; 1000 \n    AND n_dead_tup::float / NULLIF(n_live_tup + n_dead_tup, 0) &gt; 0.1;\n    \" | grep VACUUM | while read cmd; do\n        log_message \"Executing: $cmd\"\n        docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"$cmd\"\n    done\nfi\n\n# 4. Check connection count\nACTIVE_CONNECTIONS=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\nSELECT count(*) FROM pg_stat_activity WHERE state = 'active';\n\" | xargs)\n\nlog_message \"Active connections: $ACTIVE_CONNECTIONS\"\n\nif [ \"$ACTIVE_CONNECTIONS\" -gt 100 ]; then\n    log_message \"WARNING: High connection count detected\"\nfi\n\n# 5. Monitor disk usage\nSSD_USAGE=$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\nHDD_USAGE=$(df /mnt/hdd | awk 'NR==2 {print $5}' | sed 's/%//')\n\nlog_message \"Storage usage - SSD: ${SSD_USAGE}%, HDD: ${HDD_USAGE}%\"\n\nif [ \"$SSD_USAGE\" -gt 85 ]; then\n    log_message \"WARNING: SSD usage high (${SSD_USAGE}%)\"\n    # Trigger data movement\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n    SELECT move_chunk(chunk_name, 'hdd_cold')\n    FROM timescaledb_information.chunks \n    WHERE range_start &lt; NOW() - INTERVAL '14 days'\n    AND tablespace_name = 'ssd_hot'\n    LIMIT 5;\n    \"\nfi\n\nlog_message \"Daily maintenance completed\"\n</code></pre>"},{"location":"database/maintenance/#statistics-maintenance","title":"Statistics Maintenance","text":"<pre><code>-- Comprehensive statistics update\nANALYZE VERBOSE notifications;\nANALYZE VERBOSE audit_logs;\nANALYZE VERBOSE requisitions;\nANALYZE VERBOSE purchase_orders;\n\n-- Update statistics for specific columns with high cardinality\nALTER TABLE notifications ALTER COLUMN user_id SET STATISTICS 1000;\nALTER TABLE requisitions ALTER COLUMN department_id SET STATISTICS 1000;\nALTER TABLE audit_logs ALTER COLUMN action SET STATISTICS 500;\n\n-- Re-analyze after statistics target changes\nANALYZE notifications (user_id);\nANALYZE requisitions (department_id);\nANALYZE audit_logs (action);\n\n-- Check statistics freshness\nSELECT \n    schemaname,\n    tablename,\n    last_analyze,\n    last_autoanalyze,\n    n_mod_since_analyze\nFROM pg_stat_user_tables\nWHERE last_analyze &lt; NOW() - INTERVAL '1 day'\n   OR n_mod_since_analyze &gt; 1000\nORDER BY n_mod_since_analyze DESC;\n</code></pre>"},{"location":"database/maintenance/#weekly-maintenance-procedures","title":"Weekly Maintenance Procedures","text":""},{"location":"database/maintenance/#vacuum-and-analyze","title":"Vacuum and Analyze","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/weekly-maintenance.sh\n\nset -euo pipefail\n\nLOG_FILE=\"/var/log/prs-maintenance.log\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nlog_message \"Starting weekly database maintenance\"\n\n# 1. Comprehensive VACUUM ANALYZE\nlog_message \"Starting VACUUM ANALYZE\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSET maintenance_work_mem = '1GB';\nVACUUM (ANALYZE, VERBOSE) notifications;\nVACUUM (ANALYZE, VERBOSE) audit_logs;\nVACUUM (ANALYZE, VERBOSE) requisitions;\nVACUUM (ANALYZE, VERBOSE) purchase_orders;\nVACUUM (ANALYZE, VERBOSE) users;\nVACUUM (ANALYZE, VERBOSE) departments;\n\"\n\n# 2. Check for unused indexes\nlog_message \"Checking for unused indexes\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    pg_size_pretty(pg_relation_size(indexrelid)) as size,\n    idx_scan\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\n  AND pg_relation_size(indexrelid) &gt; 1024*1024  -- Larger than 1MB\nORDER BY pg_relation_size(indexrelid) DESC;\n\"\n\n# 3. Reindex small tables\nlog_message \"Reindexing small tables\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nREINDEX TABLE users;\nREINDEX TABLE departments;\nREINDEX TABLE categories;\nREINDEX TABLE vendors;\n\"\n\n# 4. Update TimescaleDB compression\nlog_message \"Updating TimescaleDB compression\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT compress_chunk(chunk_name) \nFROM timescaledb_information.chunks \nWHERE range_start &lt; NOW() - INTERVAL '7 days'\nAND NOT is_compressed\nAND hypertable_name IN ('notifications', 'audit_logs')\nLIMIT 10;\n\"\n\n# 5. Check compression effectiveness\nCOMPRESSION_STATS=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\nSELECT \n    round(\n        AVG((before_compression_total_bytes::numeric - after_compression_total_bytes::numeric) \n        / before_compression_total_bytes::numeric * 100), 2\n    )\nFROM timescaledb_information.compressed_hypertable_stats;\n\" | xargs)\n\nlog_message \"Average compression ratio: ${COMPRESSION_STATS}%\"\n\nlog_message \"Weekly maintenance completed\"\n</code></pre>"},{"location":"database/maintenance/#index-maintenance","title":"Index Maintenance","text":"<pre><code>-- Check index bloat\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    pg_size_pretty(pg_relation_size(indexrelid)) as index_size,\n    idx_scan,\n    idx_tup_read,\n    idx_tup_fetch\nFROM pg_stat_user_indexes\nWHERE pg_relation_size(indexrelid) &gt; 100*1024*1024  -- Larger than 100MB\nORDER BY pg_relation_size(indexrelid) DESC;\n\n-- Rebuild indexes with low usage\nREINDEX INDEX CONCURRENTLY idx_notifications_user_time;\nREINDEX INDEX CONCURRENTLY idx_requisitions_dept_status_time;\nREINDEX INDEX CONCURRENTLY idx_audit_logs_user_action_time;\n\n-- Check for duplicate indexes\nSELECT \n    a.schemaname,\n    a.tablename,\n    a.indexname as index1,\n    b.indexname as index2,\n    a.indexdef,\n    b.indexdef\nFROM pg_indexes a\nJOIN pg_indexes b ON a.tablename = b.tablename \n    AND a.schemaname = b.schemaname\n    AND a.indexname &lt; b.indexname\nWHERE a.indexdef = b.indexdef;\n</code></pre>"},{"location":"database/maintenance/#monthly-maintenance-procedures","title":"Monthly Maintenance Procedures","text":""},{"location":"database/maintenance/#full-database-maintenance","title":"Full Database Maintenance","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/monthly-maintenance.sh\n\nset -euo pipefail\n\nLOG_FILE=\"/var/log/prs-maintenance.log\"\nMAINTENANCE_WINDOW=\"4 hours\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nlog_message \"Starting monthly database maintenance (${MAINTENANCE_WINDOW} window)\"\n\n# 1. Create maintenance notification\nlog_message \"Creating maintenance notification\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nINSERT INTO system_notifications (message, type, created_at) \nVALUES ('Database maintenance in progress. Performance may be affected.', 'maintenance', NOW());\n\"\n\n# 2. Full REINDEX (during maintenance window)\nlog_message \"Starting full REINDEX\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSET maintenance_work_mem = '2GB';\nREINDEX (VERBOSE) DATABASE prs_production;\n\"\n\n# 3. Update all table statistics with high detail\nlog_message \"Updating detailed statistics\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nALTER TABLE notifications ALTER COLUMN user_id SET STATISTICS 1000;\nALTER TABLE notifications ALTER COLUMN type SET STATISTICS 500;\nALTER TABLE requisitions ALTER COLUMN department_id SET STATISTICS 1000;\nALTER TABLE requisitions ALTER COLUMN status SET STATISTICS 500;\nALTER TABLE audit_logs ALTER COLUMN user_id SET STATISTICS 1000;\nALTER TABLE audit_logs ALTER COLUMN action SET STATISTICS 500;\n\nANALYZE VERBOSE;\n\"\n\n# 4. Comprehensive compression update\nlog_message \"Comprehensive compression update\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT compress_chunk(chunk_name) \nFROM timescaledb_information.chunks \nWHERE range_start &lt; NOW() - INTERVAL '30 days'\nAND NOT is_compressed;\n\"\n\n# 5. Data movement optimization\nlog_message \"Optimizing data movement\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT move_chunk(chunk_name, 'hdd_cold')\nFROM timescaledb_information.chunks \nWHERE range_start &lt; NOW() - INTERVAL '30 days'\nAND tablespace_name = 'ssd_hot';\n\"\n\n# 6. Generate maintenance report\nlog_message \"Generating maintenance report\"\nREPORT_FILE=\"/tmp/monthly-maintenance-report-$(date +%Y%m).txt\"\n\ncat &gt; \"$REPORT_FILE\" &lt;&lt; EOF\nMonthly Database Maintenance Report\nGenerated: $(date)\n\nDatabase Size:\n$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"SELECT pg_size_pretty(pg_database_size('prs_production'));\")\n\nTable Sizes:\n$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT \n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\nFROM pg_tables \nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\nLIMIT 10;\n\")\n\nCompression Statistics:\n$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT \n    hypertable_name,\n    pg_size_pretty(before_compression_total_bytes) as before_compression,\n    pg_size_pretty(after_compression_total_bytes) as after_compression,\n    round(\n        (before_compression_total_bytes::numeric - after_compression_total_bytes::numeric) \n        / before_compression_total_bytes::numeric * 100, 2\n    ) as compression_ratio_percent\nFROM timescaledb_information.compressed_hypertable_stats;\n\")\n\nStorage Distribution:\n$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT \n    tablespace_name,\n    COUNT(*) as chunk_count,\n    pg_size_pretty(SUM(chunk_size)) as total_size\nFROM timescaledb_information.chunks\nGROUP BY tablespace_name;\n\")\nEOF\n\nlog_message \"Maintenance report generated: $REPORT_FILE\"\n\n# 7. Clear maintenance notification\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nDELETE FROM system_notifications WHERE type = 'maintenance';\n\"\n\nlog_message \"Monthly maintenance completed\"\n\n# Send report via email\nif command -v mail &gt;/dev/null 2&gt;&amp;1; then\n    mail -s \"Monthly Database Maintenance Report\" admin@your-domain.com &lt; \"$REPORT_FILE\"\nfi\n</code></pre>"},{"location":"database/maintenance/#performance-monitoring-and-tuning","title":"Performance Monitoring and Tuning","text":""},{"location":"database/maintenance/#performance-metrics-collection","title":"Performance Metrics Collection","text":"<pre><code>-- Create performance monitoring function\nCREATE OR REPLACE FUNCTION collect_performance_metrics()\nRETURNS TABLE (\n    metric_name text,\n    metric_value text,\n    collected_at timestamp\n) AS $$\nBEGIN\n    RETURN QUERY\n    SELECT \n        'database_size'::text,\n        pg_size_pretty(pg_database_size('prs_production')),\n        NOW()\n    UNION ALL\n    SELECT \n        'active_connections'::text,\n        COUNT(*)::text,\n        NOW()\n    FROM pg_stat_activity \n    WHERE state = 'active'\n    UNION ALL\n    SELECT \n        'cache_hit_ratio'::text,\n        round(100.0 * sum(blks_hit) / nullif(sum(blks_hit) + sum(blks_read), 0), 2)::text || '%',\n        NOW()\n    FROM pg_stat_database\n    WHERE datname = 'prs_production'\n    UNION ALL\n    SELECT \n        'transactions_per_second'::text,\n        round(sum(xact_commit + xact_rollback) / extract(epoch from (now() - stats_reset)), 2)::text,\n        NOW()\n    FROM pg_stat_database\n    WHERE datname = 'prs_production';\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Collect metrics\nSELECT * FROM collect_performance_metrics();\n</code></pre>"},{"location":"database/maintenance/#automated-performance-tuning","title":"Automated Performance Tuning","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/auto-tune-performance.sh\n\nLOG_FILE=\"/var/log/prs-maintenance.log\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\n# Check and adjust work_mem based on query patterns\nCOMPLEX_QUERIES=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\nSELECT COUNT(*) FROM pg_stat_statements \nWHERE query LIKE '%ORDER BY%' \n   OR query LIKE '%GROUP BY%' \n   OR query LIKE '%JOIN%'\nAND calls &gt; 100;\n\" | xargs)\n\nif [ \"$COMPLEX_QUERIES\" -gt 50 ]; then\n    log_message \"High number of complex queries detected, increasing work_mem\"\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n    ALTER SYSTEM SET work_mem = '64MB';\n    SELECT pg_reload_conf();\n    \"\nfi\n\n# Check and adjust shared_buffers based on cache hit ratio\nCACHE_HIT_RATIO=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\nSELECT round(100.0 * sum(blks_hit) / nullif(sum(blks_hit) + sum(blks_read), 0), 2)\nFROM pg_stat_database WHERE datname = 'prs_production';\n\" | xargs)\n\nif (( $(echo \"$CACHE_HIT_RATIO &lt; 95\" | bc -l) )); then\n    log_message \"Low cache hit ratio ($CACHE_HIT_RATIO%), considering shared_buffers increase\"\n    # Note: This requires restart, so just log for manual review\nfi\n\n# Optimize autovacuum settings based on table activity\nHIGH_ACTIVITY_TABLES=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\nSELECT COUNT(*) FROM pg_stat_user_tables \nWHERE n_tup_ins + n_tup_upd + n_tup_del &gt; 10000;\n\" | xargs)\n\nif [ \"$HIGH_ACTIVITY_TABLES\" -gt 5 ]; then\n    log_message \"High table activity detected, adjusting autovacuum settings\"\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n    ALTER SYSTEM SET autovacuum_naptime = '30s';\n    ALTER SYSTEM SET autovacuum_vacuum_threshold = 25;\n    ALTER SYSTEM SET autovacuum_analyze_threshold = 25;\n    SELECT pg_reload_conf();\n    \"\nfi\n</code></pre>"},{"location":"database/maintenance/#maintenance-automation","title":"Maintenance Automation","text":""},{"location":"database/maintenance/#cron-schedule-setup","title":"Cron Schedule Setup","text":"<pre><code># Setup comprehensive maintenance schedule\n(crontab -l 2&gt;/dev/null; cat &lt;&lt; 'EOF'\n# PRS Database Maintenance Schedule\n\n# Daily maintenance at 1:00 AM\n0 1 * * * /opt/prs-deployment/scripts/daily-maintenance.sh\n\n# Weekly maintenance on Sunday at 1:00 AM\n0 1 * * 0 /opt/prs-deployment/scripts/weekly-maintenance.sh\n\n# Monthly maintenance on first Sunday at 2:00 AM\n0 2 1-7 * 0 /opt/prs-deployment/scripts/monthly-maintenance.sh\n\n# Performance monitoring every 4 hours\n0 */4 * * * /opt/prs-deployment/scripts/auto-tune-performance.sh\n\n# Log rotation daily at 6:00 AM\n0 6 * * * /opt/prs-deployment/scripts/rotate-logs.sh\nEOF\n) | crontab -\n</code></pre>"},{"location":"database/maintenance/#maintenance-monitoring","title":"Maintenance Monitoring","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/maintenance-monitor.sh\n\nMAINTENANCE_LOG=\"/var/log/prs-maintenance.log\"\nALERT_EMAIL=\"admin@your-domain.com\"\n\n# Check if maintenance completed successfully\nif ! grep -q \"maintenance completed\" \"$MAINTENANCE_LOG\" | tail -1; then\n    echo \"Database maintenance may have failed. Check logs.\" | \\\n    mail -s \"Maintenance Alert\" \"$ALERT_EMAIL\"\nfi\n\n# Check for maintenance warnings\nWARNING_COUNT=$(grep -c \"WARNING\" \"$MAINTENANCE_LOG\" | tail -100)\nif [ \"$WARNING_COUNT\" -gt 5 ]; then\n    echo \"High number of maintenance warnings detected: $WARNING_COUNT\" | \\\n    mail -s \"Maintenance Warnings\" \"$ALERT_EMAIL\"\nfi\n\n# Generate maintenance summary\nLAST_MAINTENANCE=$(grep \"maintenance completed\" \"$MAINTENANCE_LOG\" | tail -1)\necho \"Last successful maintenance: $LAST_MAINTENANCE\"\n</code></pre> <p>Maintenance Configured</p> <p>Your PRS database now has comprehensive automated maintenance procedures to ensure optimal performance and reliability.</p> <p>Maintenance Windows</p> <p>Schedule intensive maintenance tasks during low-usage periods to minimize impact on users.</p> <p>Monitoring Required</p> <p>Always monitor maintenance job execution and review logs regularly to ensure all tasks complete successfully.</p>"},{"location":"database/performance/","title":"Database Performance","text":""},{"location":"database/performance/#overview","title":"Overview","text":"<p>This guide covers database performance optimization, monitoring, and tuning for the PRS on-premises deployment to achieve optimal query performance and system responsiveness.</p>"},{"location":"database/performance/#performance-architecture","title":"Performance Architecture","text":""},{"location":"database/performance/#timescaledb-performance-features","title":"TimescaleDB Performance Features","text":"<pre><code>graph TB\n    subgraph \"Query Layer\"\n        APP[Application Queries] --&gt; PLANNER[Query Planner]\n        PLANNER --&gt; OPTIMIZER[TimescaleDB Optimizer]\n    end\n\n    subgraph \"Storage Optimization\"\n        OPTIMIZER --&gt; CHUNKS[Chunk Exclusion]\n        OPTIMIZER --&gt; COMPRESS[Compressed Chunks]\n        OPTIMIZER --&gt; PARALLEL[Parallel Queries]\n    end\n\n    subgraph \"Storage Tiers\"\n        CHUNKS --&gt; SSD[SSD Hot Data&lt;br/&gt;0-30 days&lt;br/&gt;&lt;50ms response]\n        CHUNKS --&gt; HDD[HDD Cold Data&lt;br/&gt;30+ days&lt;br/&gt;&lt;2s response]\n        COMPRESS --&gt; HDD\n    end\n\n    subgraph \"Caching Layer\"\n        SSD --&gt; SHARED[Shared Buffers&lt;br/&gt;2GB RAM]\n        HDD --&gt; SHARED\n        SHARED --&gt; OS[OS Page Cache&lt;br/&gt;4GB RAM]\n    end\n\n    style OPTIMIZER fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style COMPRESS fill:#e8f5e8,stroke:#4caf50,stroke-width:2px\n    style SSD fill:#fff3e0,stroke:#ff9800,stroke-width:2px\n    style HDD fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px</code></pre>"},{"location":"database/performance/#performance-targets","title":"Performance Targets","text":"Metric Target Current Optimization Query Response &lt;200ms 50-150ms \u2713 Optimized Concurrent Users 100+ 150+ \u2713 Exceeded Throughput 500 QPS 800+ QPS \u2713 Exceeded Cache Hit Ratio &gt;95% 98%+ \u2713 Optimized Index Usage &gt;90% 95%+ \u2713 Optimized"},{"location":"database/performance/#query-performance-optimization","title":"Query Performance Optimization","text":""},{"location":"database/performance/#index-strategy","title":"Index Strategy","text":""},{"location":"database/performance/#primary-indexes","title":"Primary Indexes","text":"<pre><code>-- Time-based indexes for hypertables\nCREATE INDEX CONCURRENTLY idx_notifications_time \nON notifications (created_at DESC);\n\nCREATE INDEX CONCURRENTLY idx_audit_logs_time \nON audit_logs (created_at DESC);\n\nCREATE INDEX CONCURRENTLY idx_requisitions_time \nON requisitions (created_at DESC);\n\n-- Composite indexes for common queries\nCREATE INDEX CONCURRENTLY idx_notifications_user_time \nON notifications (user_id, created_at DESC);\n\nCREATE INDEX CONCURRENTLY idx_requisitions_dept_status_time \nON requisitions (department_id, status, created_at DESC);\n\nCREATE INDEX CONCURRENTLY idx_audit_logs_user_action_time \nON audit_logs (user_id, action, created_at DESC);\n</code></pre>"},{"location":"database/performance/#partial-indexes-for-hot-data","title":"Partial Indexes for Hot Data","text":"<pre><code>-- Indexes for frequently accessed recent data\nCREATE INDEX CONCURRENTLY idx_notifications_recent \nON notifications (user_id, created_at DESC) \nWHERE created_at &gt;= NOW() - INTERVAL '30 days';\n\nCREATE INDEX CONCURRENTLY idx_requisitions_active \nON requisitions (department_id, status, created_at DESC) \nWHERE status IN ('pending', 'approved', 'processing');\n\nCREATE INDEX CONCURRENTLY idx_purchase_orders_recent \nON purchase_orders (vendor_id, status, created_at DESC) \nWHERE created_at &gt;= NOW() - INTERVAL '90 days';\n</code></pre>"},{"location":"database/performance/#expression-indexes","title":"Expression Indexes","text":"<pre><code>-- Indexes on computed values\nCREATE INDEX CONCURRENTLY idx_requisitions_total_amount \nON requisitions ((total_amount::numeric));\n\nCREATE INDEX CONCURRENTLY idx_notifications_read_status \nON notifications ((CASE WHEN read_at IS NULL THEN 'unread' ELSE 'read' END));\n\n-- Full-text search indexes\nCREATE INDEX CONCURRENTLY idx_requisitions_search \nON requisitions USING gin(to_tsvector('english', description || ' ' || notes));\n</code></pre>"},{"location":"database/performance/#query-optimization","title":"Query Optimization","text":""},{"location":"database/performance/#optimized-query-patterns","title":"Optimized Query Patterns","text":"<pre><code>-- Efficient time-range queries with chunk exclusion\nSELECT r.id, r.description, r.total_amount, r.status\nFROM requisitions r\nWHERE r.created_at &gt;= '2024-08-01'::date\n  AND r.created_at &lt; '2024-09-01'::date\n  AND r.department_id = 5\n  AND r.status = 'approved'\nORDER BY r.created_at DESC\nLIMIT 50;\n\n-- Aggregation queries with time bucketing\nSELECT \n    time_bucket('1 day', created_at) as day,\n    department_id,\n    COUNT(*) as requisition_count,\n    SUM(total_amount) as total_amount\nFROM requisitions\nWHERE created_at &gt;= NOW() - INTERVAL '30 days'\nGROUP BY day, department_id\nORDER BY day DESC;\n\n-- Efficient joins with proper indexing\nSELECT \n    r.id,\n    r.description,\n    u.name as requester_name,\n    d.name as department_name\nFROM requisitions r\nJOIN users u ON r.user_id = u.id\nJOIN departments d ON r.department_id = d.id\nWHERE r.created_at &gt;= NOW() - INTERVAL '7 days'\n  AND r.status = 'pending'\nORDER BY r.created_at DESC;\n</code></pre>"},{"location":"database/performance/#query-performance-analysis","title":"Query Performance Analysis","text":"<pre><code>-- Analyze query performance\nEXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) \nSELECT r.id, r.description, r.total_amount\nFROM requisitions r\nWHERE r.created_at &gt;= NOW() - INTERVAL '30 days'\n  AND r.department_id = 5\nORDER BY r.created_at DESC\nLIMIT 20;\n\n-- Check index usage\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,\n    idx_tup_read,\n    idx_tup_fetch\nFROM pg_stat_user_indexes\nWHERE idx_scan &gt; 0\nORDER BY idx_scan DESC;\n\n-- Identify slow queries\nSELECT \n    query,\n    calls,\n    total_time,\n    mean_time,\n    rows,\n    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent\nFROM pg_stat_statements\nWHERE calls &gt; 100\nORDER BY total_time DESC\nLIMIT 20;\n</code></pre>"},{"location":"database/performance/#memory-configuration","title":"Memory Configuration","text":""},{"location":"database/performance/#postgresql-memory-settings","title":"PostgreSQL Memory Settings","text":"<pre><code>-- Memory configuration for 16GB system\nALTER SYSTEM SET shared_buffers = '2GB';\nALTER SYSTEM SET effective_cache_size = '4GB';\nALTER SYSTEM SET work_mem = '32MB';\nALTER SYSTEM SET maintenance_work_mem = '512MB';\n\n-- WAL and checkpoint settings\nALTER SYSTEM SET wal_buffers = '32MB';\nALTER SYSTEM SET checkpoint_completion_target = 0.9;\nALTER SYSTEM SET max_wal_size = '2GB';\nALTER SYSTEM SET min_wal_size = '512MB';\n\n-- Connection and worker settings\nALTER SYSTEM SET max_connections = 150;\nALTER SYSTEM SET max_worker_processes = 32;\nALTER SYSTEM SET max_parallel_workers = 16;\nALTER SYSTEM SET max_parallel_workers_per_gather = 4;\n\n-- TimescaleDB specific settings\nALTER SYSTEM SET timescaledb.max_background_workers = 16;\n\n-- Reload configuration\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"database/performance/#memory-usage-monitoring","title":"Memory Usage Monitoring","text":"<pre><code>-- Monitor memory usage\nSELECT \n    name,\n    setting,\n    unit,\n    context\nFROM pg_settings \nWHERE name IN (\n    'shared_buffers',\n    'effective_cache_size',\n    'work_mem',\n    'maintenance_work_mem',\n    'wal_buffers'\n);\n\n-- Check buffer cache hit ratio\nSELECT \n    'Buffer Cache Hit Ratio' as metric,\n    round(\n        100.0 * sum(blks_hit) / nullif(sum(blks_hit) + sum(blks_read), 0), 2\n    ) as percentage\nFROM pg_stat_database;\n\n-- Monitor memory context usage\nSELECT \n    name,\n    pg_size_pretty(total_bytes) as total_size,\n    pg_size_pretty(used_bytes) as used_size,\n    pg_size_pretty(free_bytes) as free_size\nFROM pg_backend_memory_contexts\nWHERE backend_type = 'client backend'\nORDER BY total_bytes DESC;\n</code></pre>"},{"location":"database/performance/#storage-performance","title":"Storage Performance","text":""},{"location":"database/performance/#ssd-optimization","title":"SSD Optimization","text":"<pre><code># SSD optimization settings\necho noop | sudo tee /sys/block/sda/queue/scheduler\necho noop | sudo tee /sys/block/sdb/queue/scheduler\n\n# Disable swap for better performance\nsudo swapoff -a\nsudo sysctl vm.swappiness=1\n\n# Optimize file system\nsudo mount -o remount,noatime,nodiratime /mnt/ssd\n</code></pre>"},{"location":"database/performance/#storage-performance-configuration","title":"Storage Performance Configuration","text":"<pre><code>-- Storage optimization settings\nALTER SYSTEM SET random_page_cost = 1.1;  -- SSD optimized\nALTER SYSTEM SET effective_io_concurrency = 200;  -- SSD concurrent I/O\nALTER SYSTEM SET seq_page_cost = 1.0;\n\n-- Checkpoint optimization\nALTER SYSTEM SET checkpoint_timeout = '15min';\nALTER SYSTEM SET checkpoint_completion_target = 0.9;\nALTER SYSTEM SET checkpoint_warning = '30s';\n\n-- Background writer optimization\nALTER SYSTEM SET bgwriter_delay = '200ms';\nALTER SYSTEM SET bgwriter_lru_maxpages = 100;\nALTER SYSTEM SET bgwriter_lru_multiplier = 2.0;\n\n-- Reload configuration\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"database/performance/#io-performance-monitoring","title":"I/O Performance Monitoring","text":"<pre><code>#!/bin/bash\n# Monitor I/O performance\n\n# Install monitoring tools\nsudo apt install iotop iostat sysstat\n\n# Monitor real-time I/O\niostat -x 1\n\n# Monitor disk usage by process\nsudo iotop -o\n\n# Check I/O statistics\nsar -d 1 10\n\n# Monitor specific devices\niostat -x 1 /dev/md0 /dev/md1\n</code></pre>"},{"location":"database/performance/#timescaledb-optimization","title":"TimescaleDB Optimization","text":""},{"location":"database/performance/#chunk-management","title":"Chunk Management","text":"<pre><code>-- Optimize chunk intervals\nSELECT add_dimension('notifications', 'user_id', number_partitions =&gt; 4);\nSELECT add_dimension('audit_logs', 'user_id', number_partitions =&gt; 4);\n\n-- Set optimal chunk time intervals\nSELECT set_chunk_time_interval('notifications', INTERVAL '1 day');\nSELECT set_chunk_time_interval('audit_logs', INTERVAL '1 day');\nSELECT set_chunk_time_interval('requisitions', INTERVAL '7 days');\n\n-- Check chunk statistics\nSELECT \n    hypertable_name,\n    chunk_name,\n    range_start,\n    range_end,\n    is_compressed,\n    pg_size_pretty(chunk_size) as size\nFROM timescaledb_information.chunks\nORDER BY hypertable_name, range_start DESC;\n</code></pre>"},{"location":"database/performance/#compression-optimization","title":"Compression Optimization","text":"<pre><code>-- Configure compression for optimal performance\nALTER TABLE notifications SET (\n    timescaledb.compress,\n    timescaledb.compress_segmentby = 'user_id',\n    timescaledb.compress_orderby = 'created_at DESC'\n);\n\nALTER TABLE audit_logs SET (\n    timescaledb.compress,\n    timescaledb.compress_segmentby = 'user_id, action',\n    timescaledb.compress_orderby = 'created_at DESC'\n);\n\nALTER TABLE requisitions SET (\n    timescaledb.compress,\n    timescaledb.compress_segmentby = 'department_id, status',\n    timescaledb.compress_orderby = 'created_at DESC'\n);\n\n-- Add compression policies\nSELECT add_compression_policy('notifications', INTERVAL '7 days');\nSELECT add_compression_policy('audit_logs', INTERVAL '7 days');\nSELECT add_compression_policy('requisitions', INTERVAL '30 days');\n\n-- Monitor compression effectiveness\nSELECT \n    hypertable_name,\n    pg_size_pretty(before_compression_total_bytes) as before_compression,\n    pg_size_pretty(after_compression_total_bytes) as after_compression,\n    round(\n        (before_compression_total_bytes::numeric - after_compression_total_bytes::numeric) \n        / before_compression_total_bytes::numeric * 100, 2\n    ) as compression_ratio_percent\nFROM timescaledb_information.compressed_hypertable_stats;\n</code></pre>"},{"location":"database/performance/#data-movement-policies","title":"Data Movement Policies","text":"<pre><code>-- Configure automatic data movement to optimize performance\nSELECT add_move_chunk_policy('notifications', INTERVAL '30 days', 'hdd_cold');\nSELECT add_move_chunk_policy('audit_logs', INTERVAL '30 days', 'hdd_cold');\nSELECT add_move_chunk_policy('requisitions', INTERVAL '30 days', 'hdd_cold');\n\n-- Move history tables faster\nSELECT add_move_chunk_policy('requisition_canvass_histories', INTERVAL '14 days', 'hdd_cold');\nSELECT add_move_chunk_policy('requisition_item_histories', INTERVAL '14 days', 'hdd_cold');\n\n-- Monitor data movement\nSELECT \n    hypertable_name,\n    tablespace_name,\n    COUNT(*) as chunk_count,\n    pg_size_pretty(SUM(chunk_size)) as total_size\nFROM timescaledb_information.chunks\nGROUP BY hypertable_name, tablespace_name\nORDER BY hypertable_name, tablespace_name;\n</code></pre>"},{"location":"database/performance/#connection-pool-optimization","title":"Connection Pool Optimization","text":""},{"location":"database/performance/#application-connection-pool","title":"Application Connection Pool","text":"<pre><code>// Optimized connection pool configuration for on-premises\nconst poolConfig = {\n  host: process.env.POSTGRES_HOST,\n  database: process.env.POSTGRES_DB,\n  user: process.env.POSTGRES_USER,\n  password: process.env.POSTGRES_PASSWORD,\n  port: process.env.POSTGRES_PORT,\n\n  // Pool settings optimized for on-premises deployment\n  min: 5,                    // Minimum connections\n  max: 20,                   // Maximum connections (increased from cloud)\n  acquire: 30000,            // Connection timeout (30s)\n  idle: 10000,               // Idle timeout (10s)\n  evict: 20000,              // Eviction timeout (20s)\n\n  // Performance optimizations\n  dialectOptions: {\n    statement_timeout: 30000,  // 30 second query timeout\n    idle_in_transaction_session_timeout: 60000,  // 1 minute idle transaction timeout\n  },\n\n  // Retry configuration\n  retry: {\n    max: 3,\n    timeout: 5000,\n    match: [/ECONNRESET/, /ETIMEDOUT/, /ENOTFOUND/]\n  },\n\n  // Logging for performance monitoring\n  logging: (sql, timing) =&gt; {\n    if (timing &gt; 1000) {  // Log slow queries\n      console.warn(`Slow query (${timing}ms): ${sql.substring(0, 100)}...`);\n    }\n  }\n};\n</code></pre>"},{"location":"database/performance/#pgbouncer-configuration-optional","title":"PgBouncer Configuration (Optional)","text":"<pre><code># /etc/pgbouncer/pgbouncer.ini\n[databases]\nprs_production = host=localhost port=5432 dbname=prs_production\n\n[pgbouncer]\nlisten_port = 6432\nlisten_addr = 127.0.0.1\nauth_type = md5\nauth_file = /etc/pgbouncer/userlist.txt\n\n# Pool settings optimized for performance\npool_mode = transaction\nmax_client_conn = 200\ndefault_pool_size = 25\nreserve_pool_size = 5\nmax_db_connections = 50\n\n# Performance settings\nserver_round_robin = 1\nignore_startup_parameters = extra_float_digits\n\n# Timeouts\nserver_connect_timeout = 15\nserver_login_retry = 15\nquery_timeout = 0\nquery_wait_timeout = 120\nclient_idle_timeout = 0\nserver_idle_timeout = 600\nserver_lifetime = 3600\n\n# Logging\nlog_connections = 1\nlog_disconnections = 1\nlog_pooler_errors = 1\n</code></pre>"},{"location":"database/performance/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"database/performance/#real-time-performance-monitoring","title":"Real-time Performance Monitoring","text":"<pre><code>-- Create performance monitoring views\nCREATE OR REPLACE VIEW v_performance_overview AS\nSELECT \n    'Database Size' as metric,\n    pg_size_pretty(pg_database_size('prs_production')) as value\nUNION ALL\nSELECT \n    'Active Connections',\n    COUNT(*)::text\nFROM pg_stat_activity \nWHERE state = 'active'\nUNION ALL\nSELECT \n    'Cache Hit Ratio',\n    round(100.0 * sum(blks_hit) / nullif(sum(blks_hit) + sum(blks_read), 0), 2)::text || '%'\nFROM pg_stat_database\nWHERE datname = 'prs_production'\nUNION ALL\nSELECT \n    'Transactions/sec',\n    round(sum(xact_commit + xact_rollback) / extract(epoch from (now() - stats_reset)), 2)::text\nFROM pg_stat_database\nWHERE datname = 'prs_production';\n\n-- Monitor slow queries\nCREATE OR REPLACE VIEW v_slow_queries AS\nSELECT \n    query,\n    calls,\n    total_time,\n    mean_time,\n    rows,\n    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent\nFROM pg_stat_statements\nWHERE mean_time &gt; 100  -- Queries taking more than 100ms on average\nORDER BY total_time DESC;\n\n-- Monitor table statistics\nCREATE OR REPLACE VIEW v_table_performance AS\nSELECT \n    schemaname,\n    tablename,\n    n_tup_ins as inserts,\n    n_tup_upd as updates,\n    n_tup_del as deletes,\n    n_live_tup as live_tuples,\n    n_dead_tup as dead_tuples,\n    round(100.0 * n_dead_tup / nullif(n_live_tup + n_dead_tup, 0), 2) as dead_ratio,\n    last_vacuum,\n    last_autovacuum,\n    last_analyze,\n    last_autoanalyze\nFROM pg_stat_user_tables\nORDER BY n_live_tup DESC;\n</code></pre>"},{"location":"database/performance/#performance-alerting","title":"Performance Alerting","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/performance-monitor.sh\n\nLOG_FILE=\"/var/log/prs-performance.log\"\nALERT_EMAIL=\"admin@your-domain.com\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\n# Check database performance metrics\ncheck_performance() {\n    # Check cache hit ratio\n    CACHE_HIT_RATIO=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT round(100.0 * sum(blks_hit) / nullif(sum(blks_hit) + sum(blks_read), 0), 2)\n    FROM pg_stat_database WHERE datname = 'prs_production';\n    \" | xargs)\n\n    if (( $(echo \"$CACHE_HIT_RATIO &lt; 95\" | bc -l) )); then\n        log_message \"WARNING: Low cache hit ratio: $CACHE_HIT_RATIO%\"\n        echo \"Cache hit ratio is $CACHE_HIT_RATIO%\" | mail -s \"Database Performance Alert\" \"$ALERT_EMAIL\"\n    fi\n\n    # Check active connections\n    ACTIVE_CONNECTIONS=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT count(*) FROM pg_stat_activity WHERE state = 'active';\n    \" | xargs)\n\n    if [ \"$ACTIVE_CONNECTIONS\" -gt 100 ]; then\n        log_message \"WARNING: High active connections: $ACTIVE_CONNECTIONS\"\n        echo \"Active connections: $ACTIVE_CONNECTIONS\" | mail -s \"Database Connection Alert\" \"$ALERT_EMAIL\"\n    fi\n\n    # Check for long-running queries\n    LONG_QUERIES=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT count(*) FROM pg_stat_activity \n    WHERE state = 'active' \n    AND query_start &lt; now() - interval '5 minutes'\n    AND query NOT LIKE '%pg_stat_activity%';\n    \" | xargs)\n\n    if [ \"$LONG_QUERIES\" -gt 0 ]; then\n        log_message \"WARNING: $LONG_QUERIES long-running queries detected\"\n        echo \"$LONG_QUERIES long-running queries detected\" | mail -s \"Long Query Alert\" \"$ALERT_EMAIL\"\n    fi\n}\n\n# Main execution\ncheck_performance\n</code></pre>"},{"location":"database/performance/#performance-tuning-checklist","title":"Performance Tuning Checklist","text":""},{"location":"database/performance/#daily-performance-tasks","title":"Daily Performance Tasks","text":"<pre><code>#!/bin/bash\n# Daily performance maintenance\n\n# Update table statistics\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"ANALYZE;\"\n\n# Check for bloated tables\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT \n    schemaname,\n    tablename,\n    n_dead_tup,\n    n_live_tup,\n    round(100.0 * n_dead_tup / nullif(n_live_tup + n_dead_tup, 0), 2) as dead_ratio\nFROM pg_stat_user_tables\nWHERE n_dead_tup &gt; 1000\nORDER BY dead_ratio DESC;\n\"\n\n# Check index usage\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,\n    idx_tup_read\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\nORDER BY pg_relation_size(indexrelid) DESC;\n\"\n</code></pre>"},{"location":"database/performance/#weekly-performance-tasks","title":"Weekly Performance Tasks","text":"<pre><code>#!/bin/bash\n# Weekly performance optimization\n\n# Vacuum and analyze all tables\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"VACUUM ANALYZE;\"\n\n# Reindex if needed\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"REINDEX DATABASE prs_production;\"\n\n# Update TimescaleDB statistics\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT compress_chunk(chunk_name) \nFROM timescaledb_information.chunks \nWHERE range_start &lt; NOW() - INTERVAL '7 days'\nAND NOT is_compressed\nAND hypertable_name IN ('notifications', 'audit_logs');\n\"\n</code></pre> <p>Performance Optimized</p> <p>Your PRS database is now optimized for high-performance operations with proper indexing, memory configuration, and monitoring.</p> <p>Continuous Monitoring</p> <p>Regularly monitor performance metrics and adjust configuration based on actual usage patterns and query performance.</p> <p>Performance Testing</p> <p>Always test performance changes in a staging environment before applying to production to ensure optimal results.</p>"},{"location":"database/timescaledb/","title":"TimescaleDB Production Guide","text":""},{"location":"database/timescaledb/#overview","title":"Overview","text":"<p>TimescaleDB provides the foundation for PRS's zero-deletion data policy with intelligent storage tiering, automatic compression, and unlimited scalability.</p>"},{"location":"database/timescaledb/#architecture","title":"Architecture","text":""},{"location":"database/timescaledb/#storage-strategy","title":"Storage Strategy","text":"<pre><code>graph TB\n    subgraph \"Application Layer\"\n        APP[PRS Application] --&gt; QUERY[SQL Queries]\n    end\n\n    subgraph \"TimescaleDB Engine\"\n        QUERY --&gt; PLANNER[Query Planner]\n        PLANNER --&gt; HOT[Hot Chunks&lt;br/&gt;0-30 days]\n        PLANNER --&gt; COLD[Cold Chunks&lt;br/&gt;30+ days]\n    end\n\n    subgraph \"Storage Tiers\"\n        HOT --&gt; SSD[SSD Storage&lt;br/&gt;ssd_hot tablespace&lt;br/&gt;/mnt/ssd/postgresql-hot]\n        COLD --&gt; HDD[HDD Storage&lt;br/&gt;hdd_cold tablespace&lt;br/&gt;/mnt/hdd/postgresql-cold]\n    end\n\n    subgraph \"Data Lifecycle\"\n        NEW[New Data] --&gt; SSD\n        SSD --&gt; COMPRESS[Auto Compress&lt;br/&gt;7-30 days]\n        COMPRESS --&gt; MOVE[Auto Move&lt;br/&gt;30+ days]\n        MOVE --&gt; HDD\n    end\n\n    style SSD fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style HDD fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px\n    style COMPRESS fill:#fff3e0,stroke:#ff9800,stroke-width:2px\n    style MOVE fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px</code></pre>"},{"location":"database/timescaledb/#configuration","title":"Configuration","text":"<p>The PRS system uses 38 hypertables with optimized compression policies based on the actual migration implementation:</p> Table Category Tables Chunk Interval Priority Storage Strategy Core Business <code>requisitions</code>, <code>purchase_orders</code>, <code>delivery_receipts</code>, <code>delivery_receipt_items</code>, <code>rs_payment_requests</code>, <code>canvass_requisitions</code> 1 month CORE Performance-focused High-Volume <code>audit_logs</code>, <code>notifications</code>, <code>notes</code>, <code>comments</code> 1 week HIGH_VOLUME Aggressive compression Critical Workflow <code>requisition_badges</code>, <code>requisition_approvers</code>, <code>attachments</code>, <code>histories</code> 1 month/1 week CRITICAL Balanced optimization History Tables <code>*_histories</code> tables (8 tables) 1 week HIGH/MEDIUM Audit trail optimization Workflow Tables <code>*_approvers</code>, <code>*_items</code>, <code>canvass_*</code> 1 month MEDIUM/HIGH Long-term optimization <p>Complete list of 38 hypertables: - Core: <code>requisitions</code>, <code>purchase_orders</code>, <code>delivery_receipts</code>, <code>delivery_receipt_items</code>, <code>rs_payment_requests</code>, <code>canvass_requisitions</code> - High-volume: <code>audit_logs</code>, <code>notifications</code>, <code>notes</code>, <code>comments</code>, <code>force_close_logs</code> - Critical: <code>requisition_badges</code>, <code>requisition_approvers</code>, <code>attachments</code>, <code>histories</code> - History tables: <code>requisition_canvass_histories</code>, <code>requisition_item_histories</code>, <code>requisition_order_histories</code>, <code>requisition_delivery_histories</code>, <code>requisition_payment_histories</code>, <code>requisition_return_histories</code>, <code>non_requisition_histories</code>, <code>invoice_report_histories</code>, <code>delivery_receipt_items_history</code> - Workflow: <code>canvass_item_suppliers</code>, <code>canvass_approvers</code>, <code>requisition_item_lists</code>, <code>canvass_items</code>, <code>purchase_order_items</code>, <code>purchase_order_approvers</code>, <code>non_requisitions</code>, <code>rs_payment_request_approvers</code>, <code>non_requisition_approvers</code>, <code>non_requisition_items</code>, <code>delivery_receipt_invoices</code>, <code>invoice_reports</code>, <code>gate_passes</code>, <code>purchase_order_cancelled_items</code></p>"},{"location":"database/timescaledb/#configuration_1","title":"Configuration","text":""},{"location":"database/timescaledb/#settings-16gb-ram-optimized","title":"Settings (16GB RAM Optimized)","text":"<pre><code>-- Memory Settings\nshared_buffers = 2GB                    -- 33% of allocated RAM\neffective_cache_size = 4GB              -- 67% of allocated RAM\nwork_mem = 32MB                         -- For complex queries\nmaintenance_work_mem = 512MB            -- For maintenance operations\n\n-- TimescaleDB Settings\ntimescaledb.max_background_workers = 16\nmax_worker_processes = 32\nmax_parallel_workers = 16\nmax_parallel_workers_per_gather = 4\n\n-- Connection Settings\nmax_connections = 150                   -- 5x increase from cloud\n\n-- Storage Settings\nrandom_page_cost = 1.1                 -- SSD optimization\neffective_io_concurrency = 200         -- SSD concurrent I/O\ncheckpoint_completion_target = 0.9     -- Smooth checkpoints\nwal_buffers = 32MB                     -- WAL buffer size\n\n-- Tablespace Configuration\ntemp_tablespaces = ssd_hot\ndefault_tablespace = ssd_hot\n</code></pre>"},{"location":"database/timescaledb/#extension-setup","title":"Extension Setup","text":"<pre><code>-- Enable TimescaleDB extension\nCREATE EXTENSION IF NOT EXISTS timescaledb;\n\n-- Configure telemetry (disabled for on-premises)\nALTER SYSTEM SET timescaledb.telemetry = 'off';\n\n-- Reload configuration\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"database/timescaledb/#tablespace-management","title":"Tablespace Management","text":""},{"location":"database/timescaledb/#storage-tablespaces","title":"Storage Tablespaces","text":"<p>Change directory ownership first <pre><code>docker exec prs-onprem-postgres-timescale chown -R postgres:postgres /mnt/ssd/postgresql-hot /mnt/hdd/postgresql-cold\n</code></pre></p> <pre><code>-- Create tablespaces for tiered storage\nCREATE TABLESPACE ssd_hot LOCATION '/mnt/ssd/postgresql-hot';\nCREATE TABLESPACE hdd_cold LOCATION '/mnt/hdd/postgresql-cold';\n\n-- Set default tablespace for new chunks\nALTER DATABASE prs SET default_tablespace = ssd_hot;\nALTER SYSTEM SET default_tablespace = 'ssd_hot';\n\n-- Reload configuration\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"database/timescaledb/#usage-monitoring","title":"Usage Monitoring","text":"<pre><code>-- Check tablespace usage\nSELECT\n    spcname as tablespace_name,\n    pg_size_pretty(pg_tablespace_size(spcname)) as size,\n    CASE\n        WHEN spcname = 'ssd_hot' THEN 'SSD Storage'\n        WHEN spcname = 'hdd_cold' THEN 'HDD Storage'\n        ELSE 'Default'\n    END as storage_type\nFROM pg_tablespace;\n</code></pre>"},{"location":"database/timescaledb/#compression-management","title":"Compression Management","text":""},{"location":"database/timescaledb/#compression-policies","title":"Compression Policies","text":"<pre><code>-- High-volume tables - compress after 7 days\nSELECT add_compression_policy('notifications', INTERVAL '7 days');\nSELECT add_compression_policy('audit_logs', INTERVAL '7 days');\nSELECT add_compression_policy('histories', INTERVAL '7 days');\nSELECT add_compression_policy('comments', INTERVAL '7 days');\n\n-- History tables - compress after 14 days\nSELECT add_compression_policy('requisition_canvass_histories', INTERVAL '14 days');\nSELECT add_compression_policy('requisition_item_histories', INTERVAL '14 days');\nSELECT add_compression_policy('requisition_order_histories', INTERVAL '14 days');\n\n-- Business tables - compress after 30 days\nSELECT add_compression_policy('requisitions', INTERVAL '30 days');\nSELECT add_compression_policy('purchase_orders', INTERVAL '30 days');\nSELECT add_compression_policy('delivery_receipts', INTERVAL '30 days');\n</code></pre>"},{"location":"database/timescaledb/#monitoring","title":"Monitoring","text":"<pre><code>-- Check compression statistics\nSELECT\n    hypertable_name,\n    pg_size_pretty(before_compression_total_bytes) as before_compression,\n    pg_size_pretty(after_compression_total_bytes) as after_compression,\n    round(\n        (before_compression_total_bytes::numeric - after_compression_total_bytes::numeric)\n        / before_compression_total_bytes::numeric * 100, 2\n    ) as compression_ratio_percent\nFROM timescaledb_information.compressed_hypertable_stats;\n</code></pre>"},{"location":"database/timescaledb/#data-movement-policies","title":"Data Movement Policies","text":""},{"location":"database/timescaledb/#data-tiering","title":"Data Tiering","text":"<pre><code>-- Move chunks older than 30 days to HDD storage\nSELECT add_move_chunk_policy('notifications', INTERVAL '30 days', 'hdd_cold');\nSELECT add_move_chunk_policy('audit_logs', INTERVAL '30 days', 'hdd_cold');\nSELECT add_move_chunk_policy('requisitions', INTERVAL '30 days', 'hdd_cold');\nSELECT add_move_chunk_policy('purchase_orders', INTERVAL '30 days', 'hdd_cold');\n\n-- Move history tables after 14 days (faster archival)\nSELECT add_move_chunk_policy('requisition_canvass_histories', INTERVAL '14 days', 'hdd_cold');\nSELECT add_move_chunk_policy('requisition_item_histories', INTERVAL '14 days', 'hdd_cold');\n</code></pre>"},{"location":"database/timescaledb/#data-movement","title":"Data Movement","text":"<pre><code>-- Move specific chunk to different tablespace\nSELECT move_chunk(\n    chunk =&gt; '_timescaledb_internal._hyper_1_1_chunk',\n    destination_tablespace =&gt; 'hdd_cold'\n);\n\n-- Move all old chunks for a table\nSELECT move_chunk(chunk_name, 'hdd_cold')\nFROM timescaledb_information.chunks\nWHERE hypertable_name = 'notifications'\nAND range_start &lt; NOW() - INTERVAL '30 days'\nAND tablespace_name = 'ssd_hot';\n</code></pre>"},{"location":"database/timescaledb/#performance-optimization","title":"Performance Optimization","text":""},{"location":"database/timescaledb/#performance","title":"Performance","text":"<pre><code>-- Time-based queries are automatically optimized\nSELECT COUNT(*) FROM notifications\nWHERE created_at &gt;= NOW() - INTERVAL '30 days';\n-- Execution time: ~50ms (SSD data)\n\nSELECT COUNT(*) FROM notifications\nWHERE created_at &gt;= NOW() - INTERVAL '6 months';\n-- Execution time: ~2s (includes HDD data)\n</code></pre>"},{"location":"database/timescaledb/#optimization","title":"Optimization","text":"<pre><code>-- Create time-based indexes for better performance\nCREATE INDEX CONCURRENTLY idx_notifications_time_user\nON notifications (created_at DESC, user_id);\n\n-- Create partial indexes for hot data\nCREATE INDEX CONCURRENTLY idx_notifications_recent\nON notifications (user_id, created_at DESC)\nWHERE created_at &gt;= NOW() - INTERVAL '30 days';\n</code></pre>"},{"location":"database/timescaledb/#aggregates","title":"Aggregates","text":"<pre><code>-- Create continuous aggregate for daily summaries\nCREATE MATERIALIZED VIEW daily_activity_summary\nWITH (timescaledb.continuous) AS\nSELECT\n    time_bucket('1 day', created_at) AS day,\n    COUNT(*) as total_activities,\n    COUNT(DISTINCT user_id) as unique_users\nFROM notifications\nGROUP BY day;\n\n-- Add refresh policy\nSELECT add_continuous_aggregate_policy(\n    'daily_activity_summary',\n    start_offset =&gt; INTERVAL '1 day',\n    end_offset =&gt; INTERVAL '1 hour',\n    schedule_interval =&gt; INTERVAL '1 hour'\n);\n</code></pre>"},{"location":"database/timescaledb/#monitoring-and-health-checks","title":"Monitoring and Health Checks","text":""},{"location":"database/timescaledb/#information","title":"Information","text":"<pre><code>-- View chunk distribution across storage tiers\nSELECT\n    hypertable_name,\n    chunk_name,\n    chunk_schema,\n    range_start,\n    range_end,\n    pg_size_pretty(chunk_size) as size,\n    is_compressed,\n    tablespace_name,\n    CASE\n        WHEN tablespace_name = 'ssd_hot' THEN 'SSD'\n        WHEN tablespace_name = 'hdd_cold' THEN 'HDD'\n        ELSE 'Default'\n    END as storage_tier\nFROM timescaledb_information.chunks\nWHERE hypertable_name = 'notifications'\nORDER BY range_start DESC\nLIMIT 20;\n</code></pre>"},{"location":"database/timescaledb/#job-status","title":"Job Status","text":"<pre><code>-- Check compression and movement job status\nSELECT\n    job_id,\n    application_name,\n    schedule_interval,\n    max_runtime,\n    max_retries,\n    retry_period,\n    last_run_started_at,\n    last_successful_finish,\n    last_run_status,\n    total_runs,\n    total_successes,\n    total_failures\nFROM timescaledb_information.jobs\nWHERE application_name LIKE '%compression%'\n   OR application_name LIKE '%move%';\n</code></pre>"},{"location":"database/timescaledb/#usage-analysis","title":"Usage Analysis","text":"<pre><code>-- Analyze storage usage by hypertable\nSELECT\n    hypertable_name,\n    num_chunks,\n    pg_size_pretty(table_size) as table_size,\n    pg_size_pretty(index_size) as index_size,\n    pg_size_pretty(total_size) as total_size,\n    ROUND(\n        (table_size::numeric / (1024^3)), 2\n    ) as table_size_gb\nFROM timescaledb_information.hypertables\nORDER BY table_size DESC;\n</code></pre>"},{"location":"database/timescaledb/#zero-deletion-policy","title":"Zero-Deletion Policy","text":""},{"location":"database/timescaledb/#policy-configuration","title":"Policy Configuration","text":"<pre><code>-- Verify no retention policies exist (zero-deletion compliance)\nSELECT * FROM timescaledb_information.drop_chunks_policies;\n-- Should return empty result set\n\n-- If retention policies exist, remove them\nSELECT remove_retention_policy('table_name');\n</code></pre>"},{"location":"database/timescaledb/#lifecycle-without-deletion","title":"Lifecycle Without Deletion","text":"<p>Instead of deleting data, the system uses:</p> <ol> <li>Compression: Reduces storage by 60-80%</li> <li>Data Movement: Moves old data to cheaper HDD storage</li> <li>Archival: Long-term storage on HDD with full accessibility</li> </ol>"},{"location":"database/timescaledb/#maintenance-operations","title":"Maintenance Operations","text":""},{"location":"database/timescaledb/#maintenance","title":"Maintenance","text":"<pre><code>-- Update table statistics\nANALYZE notifications;\nANALYZE audit_logs;\nANALYZE requisitions;\n\n-- Check for failed background jobs\nSELECT * FROM timescaledb_information.job_stats\nWHERE last_run_success = false;\n</code></pre>"},{"location":"database/timescaledb/#maintenance_1","title":"Maintenance","text":"<pre><code>-- Vacuum and analyze all hypertables\nSELECT format('VACUUM ANALYZE %I;', hypertable_name)\nFROM timescaledb_information.hypertables;\n\n-- Update statistics for all hypertables\nSELECT update_stats(hypertable_name)\nFROM timescaledb_information.hypertables;\n</code></pre>"},{"location":"database/timescaledb/#procedures","title":"Procedures","text":""},{"location":"database/timescaledb/#compression-ssd-full","title":"Compression (SSD Full)","text":"<pre><code>-- Emergency compress all eligible chunks\nSELECT compress_chunk(chunk_name)\nFROM timescaledb_information.chunks\nWHERE range_start &lt; NOW() - INTERVAL '3 days'\nAND NOT is_compressed\nAND tablespace_name = 'ssd_hot';\n</code></pre>"},{"location":"database/timescaledb/#data-movement-ssd-critical","title":"Data Movement (SSD Critical)","text":"<pre><code>-- Move older chunks to HDD immediately\nSELECT move_chunk(chunk_name, 'hdd_cold')\nFROM timescaledb_information.chunks\nWHERE range_start &lt; NOW() - INTERVAL '14 days'\nAND tablespace_name = 'ssd_hot';\n</code></pre> <p>Automatic Operation</p> <p>TimescaleDB handles all data lifecycle management automatically. The application never needs to know which storage tier contains the data - queries work transparently across both SSD and HDD storage.</p> <p>Performance Characteristics</p> <ul> <li>Recent Data (SSD): &lt;50ms query time</li> <li>Historical Data (HDD): &lt;2s query time</li> <li>Compressed Data: 60-80% space savings</li> <li>Zero Data Loss: Complete compliance with zero-deletion policy</li> </ul>"},{"location":"database/timescaledb/#production-operations","title":"Production Operations","text":"<p>For production maintenance and automation:</p> <ul> <li>TimescaleDB Automation Guide - Complete automation strategy and setup</li> <li>TimescaleDB Quick Reference - Daily operations and troubleshooting</li> <li>Routine Maintenance - Integration with overall maintenance procedures</li> </ul>"},{"location":"database/timescaledb/#key-production-commands","title":"Key Production Commands","text":"<pre><code># One-time setup\n./deploy-onprem.sh optimize-timescaledb\n\n# Weekly automation (cron)\n0 2 * * 0 /opt/prs/prs-deployment/scripts/deploy-onprem.sh weekly-maintenance\n\n# Status monitoring\n./deploy-onprem.sh timescaledb-status\n</code></pre> <p>Fully Automated</p> <p>TimescaleDB requires minimal manual intervention in production. The automation handles compression, storage tiering, and data lifecycle management automatically.</p>"},{"location":"deployment/custom-domain/","title":"Custom Domain Implementation Guide","text":""},{"location":"deployment/custom-domain/#overview","title":"Overview","text":"<p>YES, it is absolutely possible to use custom domains like <code>prs.citylandcondo.com</code> with automated Let's Encrypt SSL certificates, even with GoDaddy hosting (no API access) and office-network-only accessibility.</p> <p>This guide covers the complete implementation of custom domains for PRS on-premises deployment, including the proven solution for office-network-only setups with professional SSL certificates.</p>"},{"location":"deployment/custom-domain/#implementation-summary","title":"Implementation Summary","text":""},{"location":"deployment/custom-domain/#solution-http-01-challenge-with-temporary-port-forwarding","title":"Solution: HTTP-01 Challenge with Temporary Port Forwarding","text":"<p>This solution works by temporarily exposing port 80 during certificate generation (every ~90 days) while maintaining security for daily operations.</p>"},{"location":"deployment/custom-domain/#key-benefits","title":"Key Benefits","text":"<ul> <li>Automated SSL: Let's Encrypt certificates with 90-day auto-renewal</li> <li>GoDaddy Compatible: Works without API access - only requires DNS A record</li> <li>Office Network Only: Domain only accessible from internal network</li> <li>Minimal Maintenance: Quarterly renewal process (5-10 minutes)</li> <li>Professional Appearance: Custom branded domain for users</li> <li>Security Maintained: HTTPS with valid certificates</li> </ul>"},{"location":"deployment/custom-domain/#domain-requirements","title":"Domain Requirements","text":""},{"location":"deployment/custom-domain/#prerequisites-for-custom-domain","title":"Prerequisites for Custom Domain","text":"<ul> <li>Static Public IP: Required for SSL certificate validation</li> <li>DNS Control: Administrative access to domain DNS management</li> <li>Router Access: Ability to configure temporary port forwarding</li> <li>Office Network: Internal network configured (192.168.0.0/20)</li> </ul>"},{"location":"deployment/custom-domain/#supported-domain-configurations","title":"Supported Domain Configurations","text":"<ul> <li>GoDaddy Domains: <code>*.citylandcondo.com</code> (primary use case)</li> <li>Other Registrars: Any domain with DNS control</li> <li>Subdomains: <code>prs.your-company.com</code></li> <li>Internal Domains: <code>prs.company.local</code> (self-signed certificates only)</li> </ul>"},{"location":"deployment/custom-domain/#implementation-components","title":"Implementation Components","text":""},{"location":"deployment/custom-domain/#automated-scripts-available","title":"Automated Scripts Available","text":"<p>The PRS deployment includes these custom domain automation scripts:</p> <ul> <li><code>ssl-automation-citylandcondo.sh</code> - Complete SSL certificate automation</li> <li><code>ssl-renewal-monitor.sh</code> - Daily monitoring and alerting</li> <li><code>setup-custom-domain.sh</code> - One-click setup wizard</li> </ul>"},{"location":"deployment/custom-domain/#system-components-updated","title":"System Components Updated","text":""},{"location":"deployment/custom-domain/#nginx-configuration","title":"Nginx Configuration","text":"<ul> <li>HTTP to HTTPS redirect</li> <li>Custom domain handling</li> <li>IP fallback access</li> <li>Security headers added</li> </ul>"},{"location":"deployment/custom-domain/#monitoring-system","title":"Monitoring System","text":"<ul> <li>Daily certificate status checks</li> <li>Email alerts 30 days before expiry</li> <li>Grafana webhook integration</li> <li>Automated renewal reminders</li> </ul>"},{"location":"deployment/custom-domain/#dns-configuration","title":"DNS Configuration","text":""},{"location":"deployment/custom-domain/#for-godaddy-domains-recommended","title":"For GoDaddy Domains (Recommended)","text":""},{"location":"deployment/custom-domain/#primary-dns-a-record","title":"Primary DNS A Record","text":"<pre><code># GoDaddy DNS Management\nRecord Type: A\nName: prs\nValue: [Your office public IP address]\nTTL: 1 Hour (3600 seconds)\n</code></pre> <p>Public IP Required</p> <p>The DNS A record must point to your office's public IP address, not the internal server IP (192.168.0.100). This is required for SSL certificate validation.</p>"},{"location":"deployment/custom-domain/#optional-internal-dns","title":"Optional Internal DNS","text":"<pre><code># Internal DNS Server (for performance)\nprs.citylandcondo.com.     IN  A    192.168.0.100\n</code></pre>"},{"location":"deployment/custom-domain/#for-other-domain-registrars","title":"For Other Domain Registrars","text":""},{"location":"deployment/custom-domain/#standard-a-record-configuration","title":"Standard A Record Configuration","text":"<pre><code># Primary domain A record\nprs.your-company.com.     IN  A    [Office Public IP]\n\n# Subdomain A record\nprocurement.your-company.com.  IN  A    [Office Public IP]\n\n# WWW alias (optional)\nwww.prs.your-company.com.      IN  CNAME  prs.your-company.com.\n</code></pre>"},{"location":"deployment/custom-domain/#quick-implementation","title":"Quick Implementation","text":""},{"location":"deployment/custom-domain/#one-command-setup","title":"One-Command Setup","text":"<p>For <code>*.citylandcondo.com</code> domains:</p> <pre><code># Run the automated setup script\n./setup-custom-domain.sh\n</code></pre> <p>This script handles: 1. Prerequisites check 2. Package installation 3. DNS configuration guidance 4. SSL certificate generation 5. Service configuration 6. Monitoring setup</p>"},{"location":"deployment/custom-domain/#manual-implementation-steps","title":"Manual Implementation Steps","text":""},{"location":"deployment/custom-domain/#step-1-dns-configuration-one-time","title":"Step 1: DNS Configuration (One-time)","text":"<ol> <li>Add A record in GoDaddy: <code>prs</code> \u2192 <code>[Office Public IP]</code></li> <li>Wait 5-10 minutes for DNS propagation</li> <li>Verify with: <code>nslookup prs.citylandcondo.com 8.8.8.8</code></li> </ol>"},{"location":"deployment/custom-domain/#step-2-ssl-certificate-generation","title":"Step 2: SSL Certificate Generation","text":"<ol> <li>Enable port 80 forwarding temporarily</li> <li>Run: <code>./ssl-automation-citylandcondo.sh</code></li> <li>Disable port 80 forwarding after completion</li> </ol>"},{"location":"deployment/custom-domain/#step-3-service-configuration","title":"Step 3: Service Configuration","text":"<ol> <li>Update environment variables</li> <li>Restart services: <code>./deploy-onprem.sh restart</code></li> <li>Test access from office devices</li> </ol>"},{"location":"deployment/custom-domain/#operational-process","title":"Operational Process","text":""},{"location":"deployment/custom-domain/#initial-setup-one-time-30-minutes","title":"Initial Setup (One-time, ~30 minutes)","text":"<ol> <li>Configure DNS A record in domain registrar</li> <li>Run setup script or manual configuration</li> <li>Follow guided prompts</li> <li>Test access from office devices</li> </ol>"},{"location":"deployment/custom-domain/#quarterly-renewal-5-10-minutes","title":"Quarterly Renewal (~5-10 minutes)","text":"<ol> <li>Receive automated email alert</li> <li>Enable port 80 forwarding temporarily</li> <li>Run: <code>./ssl-automation-citylandcondo.sh renew</code></li> <li>Disable port 80 forwarding</li> <li>Verify certificate renewal</li> </ol>"},{"location":"deployment/custom-domain/#daily-operations-automated","title":"Daily Operations (Automated)","text":"<ul> <li>Certificate monitoring at 2:00 AM</li> <li>Health checks and alerts</li> <li>Log rotation and maintenance</li> </ul>"},{"location":"deployment/custom-domain/#dns-validation","title":"DNS Validation","text":"<pre><code># Verify DNS resolution\nnslookup prs.your-company.com\ndig prs.your-company.com A\ndig prs.your-company.com AAAA\n\n# Test from different DNS servers\ndig @8.8.8.8 prs.your-company.com\ndig @1.1.1.1 prs.your-company.com\n\n# Check DNS propagation\nfor server in 8.8.8.8 1.1.1.1 208.67.222.222; do\n    echo \"Testing $server:\"\n    dig @$server prs.your-company.com A +short\ndone\n</code></pre>"},{"location":"deployment/custom-domain/#ssl-certificate-configuration","title":"SSL Certificate Configuration","text":""},{"location":"deployment/custom-domain/#lets-encrypt-certificate","title":"Let's Encrypt Certificate","text":""},{"location":"deployment/custom-domain/#automated-certificate-generation","title":"Automated Certificate Generation","text":"<pre><code># Update domain in SSL automation script\ncd /opt/prs-deployment/scripts\ncp ssl-automation-citylandcondo.sh ssl-automation-custom.sh\n\n# Edit the script for your domain\nsed -i 's/citylandcondo.com/your-company.com/g' ssl-automation-custom.sh\nsed -i 's/cityland@citylandcondo.com/admin@your-company.com/g' ssl-automation-custom.sh\n\n# Run certificate generation\n./ssl-automation-custom.sh\n</code></pre>"},{"location":"deployment/custom-domain/#manual-lets-encrypt-setup","title":"Manual Let's Encrypt Setup","text":"<pre><code># Stop nginx temporarily\ndocker-compose -f ../02-docker-configuration/docker-compose.onprem.yml stop nginx\n\n# Generate certificate\nsudo certbot certonly \\\n  --standalone \\\n  --email admin@your-company.com \\\n  --agree-tos \\\n  --no-eff-email \\\n  --domains prs.your-company.com \\\n  --domains www.prs.your-company.com\n\n# Copy certificates\nsudo cp /etc/letsencrypt/live/prs.your-company.com/fullchain.pem \\\n  /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt\n\nsudo cp /etc/letsencrypt/live/prs.your-company.com/privkey.pem \\\n  /opt/prs-deployment/02-docker-configuration/ssl/private.key\n\nsudo cp /etc/letsencrypt/live/prs.your-company.com/chain.pem \\\n  /opt/prs-deployment/02-docker-configuration/ssl/ca-bundle.crt\n\n# Start nginx\ndocker-compose -f ../02-docker-configuration/docker-compose.onprem.yml start nginx\n</code></pre>"},{"location":"deployment/custom-domain/#commercial-ssl-certificate","title":"Commercial SSL Certificate","text":""},{"location":"deployment/custom-domain/#certificate-installation","title":"Certificate Installation","text":"<pre><code># Copy your commercial certificate files\nsudo cp /path/to/your-domain.crt /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt\nsudo cp /path/to/your-domain.key /opt/prs-deployment/02-docker-configuration/ssl/private.key\nsudo cp /path/to/ca-bundle.crt /opt/prs-deployment/02-docker-configuration/ssl/ca-bundle.crt\n\n# Set proper permissions\nsudo chmod 644 /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt\nsudo chmod 600 /opt/prs-deployment/02-docker-configuration/ssl/private.key\nsudo chmod 644 /opt/prs-deployment/02-docker-configuration/ssl/ca-bundle.crt\n</code></pre>"},{"location":"deployment/custom-domain/#certificate-validation","title":"Certificate Validation","text":"<pre><code># Verify certificate matches domain\nopenssl x509 -in /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt -text -noout | grep -A1 \"Subject Alternative Name\"\n\n# Check certificate chain\nopenssl verify -CAfile /opt/prs-deployment/02-docker-configuration/ssl/ca-bundle.crt \\\n  /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt\n\n# Test certificate expiration\nopenssl x509 -in /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt -noout -dates\n</code></pre>"},{"location":"deployment/custom-domain/#application-configuration","title":"Application Configuration","text":""},{"location":"deployment/custom-domain/#environment-configuration","title":"Environment Configuration","text":"<pre><code># Update .env file with custom domain\ncd /opt/prs-deployment/02-docker-configuration\ncp .env .env.backup\n\n# Update domain settings\nsed -i 's/DOMAIN=.*/DOMAIN=prs.your-company.com/' .env\nsed -i 's/SERVER_IP=.*/SERVER_IP=192.168.0.100/' .env\nsed -i 's/SSL_EMAIL=.*/SSL_EMAIL=admin@your-company.com/' .env\n\n# Update API URLs\nsed -i 's|VITE_APP_API_URL=.*|VITE_APP_API_URL=https://prs.your-company.com/api|' .env\nsed -i 's|VITE_APP_BASE_URL=.*|VITE_APP_BASE_URL=https://prs.your-company.com|' .env\n\n# Update notification settings\nsed -i 's/NOTIFICATION_FROM_EMAIL=.*/NOTIFICATION_FROM_EMAIL=noreply@your-company.com/' .env\nsed -i 's/NOTIFICATION_FROM_NAME=.*/NOTIFICATION_FROM_NAME=PRS System/' .env\n</code></pre>"},{"location":"deployment/custom-domain/#nginx-configuration_1","title":"Nginx Configuration","text":"<pre><code># Custom domain nginx configuration\nserver {\n    listen 80;\n    server_name prs.your-company.com www.prs.your-company.com;\n    return 301 https://prs.your-company.com$request_uri;\n}\n\nserver {\n    listen 443 ssl http2;\n    server_name www.prs.your-company.com;\n\n    ssl_certificate /etc/nginx/ssl/certificate.crt;\n    ssl_certificate_key /etc/nginx/ssl/private.key;\n\n    return 301 https://prs.your-company.com$request_uri;\n}\n\nserver {\n    listen 443 ssl http2;\n    server_name prs.your-company.com;\n\n    # SSL Configuration\n    ssl_certificate /etc/nginx/ssl/certificate.crt;\n    ssl_certificate_key /etc/nginx/ssl/private.key;\n    ssl_trusted_certificate /etc/nginx/ssl/ca-bundle.crt;\n\n    # SSL Security Settings\n    ssl_protocols TLSv1.2 TLSv1.3;\n    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384;\n    ssl_prefer_server_ciphers off;\n    ssl_session_cache shared:SSL:10m;\n    ssl_session_timeout 10m;\n\n    # Security Headers\n    add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains; preload\" always;\n    add_header X-Frame-Options DENY always;\n    add_header X-Content-Type-Options nosniff always;\n    add_header X-XSS-Protection \"1; mode=block\" always;\n    add_header Referrer-Policy \"strict-origin-when-cross-origin\" always;\n\n    # Custom domain specific headers\n    add_header X-Served-By \"PRS-OnPrem\" always;\n    add_header X-Domain \"prs.your-company.com\" always;\n\n    # Application routes\n    location / {\n        proxy_pass http://frontend:3000;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header X-Forwarded-Host $host;\n    }\n\n    location /api/ {\n        proxy_pass http://backend:4000;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header X-Forwarded-Host $host;\n    }\n\n    # Health check endpoint\n    location /health {\n        access_log off;\n        return 200 \"healthy\\n\";\n        add_header Content-Type text/plain;\n    }\n}\n</code></pre>"},{"location":"deployment/custom-domain/#database-configuration","title":"Database Configuration","text":"<pre><code>-- Update database configuration for custom domain\nUPDATE system_settings\nSET value = 'https://prs.your-company.com'\nWHERE key = 'base_url';\n\nUPDATE system_settings\nSET value = 'prs.your-company.com'\nWHERE key = 'domain_name';\n\nUPDATE system_settings\nSET value = 'noreply@your-company.com'\nWHERE key = 'notification_from_email';\n\n-- Update email templates with new domain\nUPDATE email_templates\nSET content = REPLACE(content, 'citylandcondo.com', 'your-company.com');\n\n-- Update notification settings\nUPDATE notification_settings\nSET webhook_url = REPLACE(webhook_url, 'citylandcondo.com', 'your-company.com')\nWHERE webhook_url IS NOT NULL;\n</code></pre>"},{"location":"deployment/custom-domain/#multi-domain-support","title":"Multi-Domain Support","text":""},{"location":"deployment/custom-domain/#multiple-domain-configuration","title":"Multiple Domain Configuration","text":"<pre><code># Support multiple domains\nserver {\n    listen 443 ssl http2;\n    server_name prs.your-company.com procurement.your-company.com prs.yourcompany.local;\n\n    # SSL Configuration (wildcard or multi-domain certificate)\n    ssl_certificate /etc/nginx/ssl/certificate.crt;\n    ssl_certificate_key /etc/nginx/ssl/private.key;\n\n    # Domain-specific routing\n    set $backend_host backend:4000;\n    set $frontend_host frontend:3000;\n\n    # Custom routing based on domain\n    if ($host = \"prs.yourcompany.local\") {\n        set $backend_host backend-dev:4000;\n        set $frontend_host frontend-dev:3000;\n    }\n\n    location / {\n        proxy_pass http://$frontend_host;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n\n    location /api/ {\n        proxy_pass http://$backend_host;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre>"},{"location":"deployment/custom-domain/#domain-specific-environment-variables","title":"Domain-Specific Environment Variables","text":"<pre><code># Create domain-specific environment files\ncat &gt; .env.production &lt;&lt; 'EOF'\nDOMAIN=prs.your-company.com\nVITE_APP_API_URL=https://prs.your-company.com/api\nVITE_APP_BASE_URL=https://prs.your-company.com\nNOTIFICATION_FROM_EMAIL=noreply@your-company.com\nEOF\n\ncat &gt; .env.staging &lt;&lt; 'EOF'\nDOMAIN=prs-staging.your-company.com\nVITE_APP_API_URL=https://prs-staging.your-company.com/api\nVITE_APP_BASE_URL=https://prs-staging.your-company.com\nNOTIFICATION_FROM_EMAIL=staging@your-company.com\nEOF\n\ncat &gt; .env.development &lt;&lt; 'EOF'\nDOMAIN=prs.yourcompany.local\nVITE_APP_API_URL=https://prs.yourcompany.local/api\nVITE_APP_BASE_URL=https://prs.yourcompany.local\nNOTIFICATION_FROM_EMAIL=dev@yourcompany.local\nEOF\n</code></pre>"},{"location":"deployment/custom-domain/#domain-validation-and-testing","title":"Domain Validation and Testing","text":""},{"location":"deployment/custom-domain/#automated-domain-testing","title":"Automated Domain Testing","text":"<pre><code>#!/bin/bash\n# domain-validation.sh\n\nDOMAIN=\"prs.your-company.com\"\nLOG_FILE=\"/tmp/domain-validation-$(date +%Y%m%d_%H%M%S).log\"\n\necho \"Domain Validation Report for $DOMAIN\" | tee \"$LOG_FILE\"\necho \"======================================\" | tee -a \"$LOG_FILE\"\n\n# Test DNS resolution\necho \"Testing DNS resolution...\" | tee -a \"$LOG_FILE\"\nif nslookup \"$DOMAIN\" &gt; /dev/null 2&gt;&amp;1; then\n    echo \"\u2713 DNS resolution successful\" | tee -a \"$LOG_FILE\"\n    IP=$(dig +short \"$DOMAIN\")\n    echo \"  Resolved IP: $IP\" | tee -a \"$LOG_FILE\"\nelse\n    echo \"\u2717 DNS resolution failed\" | tee -a \"$LOG_FILE\"\nfi\n\n# Test HTTP redirect\necho \"Testing HTTP to HTTPS redirect...\" | tee -a \"$LOG_FILE\"\nHTTP_STATUS=$(curl -s -o /dev/null -w \"%{http_code}\" \"http://$DOMAIN\")\nif [ \"$HTTP_STATUS\" = \"301\" ] || [ \"$HTTP_STATUS\" = \"302\" ]; then\n    echo \"\u2713 HTTP redirect working ($HTTP_STATUS)\" | tee -a \"$LOG_FILE\"\nelse\n    echo \"\u2717 HTTP redirect not working ($HTTP_STATUS)\" | tee -a \"$LOG_FILE\"\nfi\n\n# Test HTTPS access\necho \"Testing HTTPS access...\" | tee -a \"$LOG_FILE\"\nif curl -f -s \"https://$DOMAIN/health\" &gt; /dev/null; then\n    echo \"\u2713 HTTPS access successful\" | tee -a \"$LOG_FILE\"\nelse\n    echo \"\u2717 HTTPS access failed\" | tee -a \"$LOG_FILE\"\nfi\n\n# Test SSL certificate\necho \"Testing SSL certificate...\" | tee -a \"$LOG_FILE\"\nif openssl s_client -connect \"$DOMAIN:443\" -servername \"$DOMAIN\" &lt; /dev/null 2&gt;/dev/null | grep -q \"Verify return code: 0\"; then\n    echo \"\u2713 SSL certificate valid\" | tee -a \"$LOG_FILE\"\n\n    # Get certificate details\n    CERT_EXPIRY=$(openssl s_client -connect \"$DOMAIN:443\" -servername \"$DOMAIN\" &lt; /dev/null 2&gt;/dev/null | openssl x509 -noout -enddate | cut -d= -f2)\n    echo \"  Certificate expires: $CERT_EXPIRY\" | tee -a \"$LOG_FILE\"\nelse\n    echo \"\u2717 SSL certificate invalid\" | tee -a \"$LOG_FILE\"\nfi\n\n# Test API endpoints\necho \"Testing API endpoints...\" | tee -a \"$LOG_FILE\"\nif curl -f -s \"https://$DOMAIN/api/health\" &gt; /dev/null; then\n    echo \"\u2713 API endpoints accessible\" | tee -a \"$LOG_FILE\"\nelse\n    echo \"\u2717 API endpoints not accessible\" | tee -a \"$LOG_FILE\"\nfi\n\n# Test application loading\necho \"Testing application loading...\" | tee -a \"$LOG_FILE\"\nRESPONSE_TIME=$(curl -w \"%{time_total}\" -o /dev/null -s \"https://$DOMAIN/\")\nif (( $(echo \"$RESPONSE_TIME &lt; 5.0\" | bc -l) )); then\n    echo \"\u2713 Application loads in ${RESPONSE_TIME}s\" | tee -a \"$LOG_FILE\"\nelse\n    echo \"\u2717 Application slow to load (${RESPONSE_TIME}s)\" | tee -a \"$LOG_FILE\"\nfi\n\necho \"Domain validation completed. Report saved to: $LOG_FILE\"\n</code></pre>"},{"location":"deployment/custom-domain/#ssl-certificate-monitoring","title":"SSL Certificate Monitoring","text":"<pre><code>#!/bin/bash\n# ssl-monitor.sh\n\nDOMAIN=\"prs.your-company.com\"\nDAYS_WARNING=30\n\n# Check certificate expiration\nCERT_FILE=\"/opt/prs-deployment/02-docker-configuration/ssl/certificate.crt\"\nif [ -f \"$CERT_FILE\" ]; then\n    EXPIRY_DATE=$(openssl x509 -in \"$CERT_FILE\" -noout -enddate | cut -d= -f2)\n    EXPIRY_EPOCH=$(date -d \"$EXPIRY_DATE\" +%s)\n    CURRENT_EPOCH=$(date +%s)\n    DAYS_UNTIL_EXPIRY=$(( (EXPIRY_EPOCH - CURRENT_EPOCH) / 86400 ))\n\n    if [ $DAYS_UNTIL_EXPIRY -lt $DAYS_WARNING ]; then\n        echo \"WARNING: SSL certificate for $DOMAIN expires in $DAYS_UNTIL_EXPIRY days\"\n\n        # Send notification email\n        echo \"SSL certificate for $DOMAIN expires in $DAYS_UNTIL_EXPIRY days\" | \\\n        mail -s \"SSL Certificate Expiration Warning\" admin@your-company.com\n\n        # Auto-renew if Let's Encrypt\n        if [ -f \"/opt/prs-deployment/scripts/ssl-automation-custom.sh\" ]; then\n            echo \"Attempting automatic renewal...\"\n            /opt/prs-deployment/scripts/ssl-automation-custom.sh --renew\n        fi\n    else\n        echo \"SSL certificate for $DOMAIN is valid for $DAYS_UNTIL_EXPIRY more days\"\n    fi\nelse\n    echo \"ERROR: SSL certificate file not found: $CERT_FILE\"\nfi\n</code></pre>"},{"location":"deployment/custom-domain/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/custom-domain/#common-domain-issues","title":"Common Domain Issues","text":""},{"location":"deployment/custom-domain/#dns-not-resolving","title":"DNS Not Resolving","text":"<pre><code># Check DNS configuration\ndig prs.your-company.com A\ndig prs.your-company.com AAAA\n\n# Test different DNS servers\ndig @8.8.8.8 prs.your-company.com\ndig @1.1.1.1 prs.your-company.com\n\n# Check DNS propagation\nnslookup prs.your-company.com 8.8.8.8\n</code></pre>"},{"location":"deployment/custom-domain/#ssl-certificate-issues","title":"SSL Certificate Issues","text":"<pre><code># Check certificate validity\nopenssl x509 -in /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt -text -noout\n\n# Test SSL connection\nopenssl s_client -connect prs.your-company.com:443 -servername prs.your-company.com\n\n# Check certificate chain\ncurl -vI https://prs.your-company.com/\n</code></pre>"},{"location":"deployment/custom-domain/#application-not-loading","title":"Application Not Loading","text":"<pre><code># Check nginx configuration\ndocker exec prs-onprem-nginx nginx -t\n\n# Check nginx logs\ndocker logs prs-onprem-nginx\n\n# Test internal connectivity\ndocker exec prs-onprem-nginx curl http://frontend:3000\ndocker exec prs-onprem-nginx curl http://backend:4000/health\n</code></pre>"},{"location":"deployment/custom-domain/#domain-migration","title":"Domain Migration","text":"<pre><code>#!/bin/bash\n# migrate-domain.sh\n\nOLD_DOMAIN=\"$1\"\nNEW_DOMAIN=\"$2\"\n\nif [ -z \"$OLD_DOMAIN\" ] || [ -z \"$NEW_DOMAIN\" ]; then\n    echo \"Usage: $0 &lt;old-domain&gt; &lt;new-domain&gt;\"\n    exit 1\nfi\n\necho \"Migrating from $OLD_DOMAIN to $NEW_DOMAIN\"\n\n# Update environment files\nsed -i \"s/$OLD_DOMAIN/$NEW_DOMAIN/g\" /opt/prs-deployment/02-docker-configuration/.env\n\n# Update database\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nUPDATE system_settings SET value = REPLACE(value, '$OLD_DOMAIN', '$NEW_DOMAIN');\nUPDATE email_templates SET content = REPLACE(content, '$OLD_DOMAIN', '$NEW_DOMAIN');\nUPDATE notification_settings SET webhook_url = REPLACE(webhook_url, '$OLD_DOMAIN', '$NEW_DOMAIN');\n\"\n\n# Restart services\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml restart\n\necho \"Domain migration completed. Please update DNS and SSL certificates.\"\n</code></pre> <p>Custom Domain Configured</p> <p>Your PRS deployment is now accessible via your custom domain with proper SSL security and optimized configuration.</p> <p>DNS Propagation</p> <p>DNS changes can take up to 48 hours to propagate globally. Use online DNS propagation checkers to monitor the status.</p> <p>Certificate Management</p> <p>Ensure SSL certificates are properly configured and monitored for expiration to maintain secure access to your domain.</p>"},{"location":"deployment/it-coordination/","title":"IT Network Admin Coordination Guide","text":""},{"location":"deployment/it-coordination/#overview","title":"Overview","text":"<p>This guide provides specific instructions for IT network administrators to support PRS deployment with custom domain SSL certificates, particularly for <code>*.citylandcondo.com</code> domains using GoDaddy DNS management.</p>"},{"location":"deployment/it-coordination/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure you have: - Administrative access to GoDaddy DNS management - Router/firewall configuration access for port forwarding - Office public IP address (static IP required) - Internal network configured (192.168.0.0/20)</p>"},{"location":"deployment/it-coordination/#required-it-coordination-tasks","title":"Required IT Coordination Tasks","text":""},{"location":"deployment/it-coordination/#task-1-dns-configuration-one-time-setup","title":"Task 1: DNS Configuration (One-time Setup)","text":""},{"location":"deployment/it-coordination/#godaddy-dns-management","title":"GoDaddy DNS Management","text":"<ol> <li>Login to GoDaddy:</li> <li>Access your GoDaddy account</li> <li>Navigate to \"My Products\" \u2192 \"DNS\"</li> <li> <p>Select the <code>citylandcondo.com</code> domain</p> </li> <li> <p>Create DNS A Record:    <pre><code>Record Type: A\nName: prs\nValue: [Your office public IP address]\nTTL: 1 Hour (3600 seconds)\n</code></pre></p> </li> </ol> <p>!!! warning \"Public IP Required\"        Replace <code>[Your office public IP address]</code> with your actual static public IP. This is absolutely required for SSL certificate validation.</p> <ol> <li>Verify DNS Configuration:    <pre><code># Test DNS resolution\nnslookup prs.citylandcondo.com 8.8.8.8\n\n# Should return your office public IP\ndig prs.citylandcondo.com +short\n</code></pre></li> </ol>"},{"location":"deployment/it-coordination/#optional-internal-dns-configuration","title":"Optional: Internal DNS Configuration","text":"<p>For better performance, configure internal DNS:</p> <pre><code>Record Type: A\nName: prs.citylandcondo.com\nValue: 192.168.0.100\n</code></pre> <p>Benefits: - Faster access for office users - Reduced external traffic - Better performance</p>"},{"location":"deployment/it-coordination/#task-2-port-forwarding-configuration","title":"Task 2: Port Forwarding Configuration","text":""},{"location":"deployment/it-coordination/#when-port-forwarding-is-needed","title":"When Port Forwarding is Needed","text":"<ul> <li>Initial Setup: During first SSL certificate generation</li> <li>Renewals: Every 90 days (automated email notifications)</li> <li>Duration: 5-10 minutes each time</li> </ul>"},{"location":"deployment/it-coordination/#port-forwarding-rules","title":"Port Forwarding Rules","text":"<p>For SSL Certificate Generation (Temporary): <pre><code>External Port: 80\nInternal IP: 192.168.0.100\nInternal Port: 80\nProtocol: TCP\n</code></pre></p> <p>For HTTPS Access (Optional - Permanent): <pre><code>External Port: 443\nInternal IP: 192.168.0.100\nInternal Port: 443\nProtocol: TCP\n</code></pre></p>"},{"location":"deployment/it-coordination/#port-forwarding-process","title":"Port Forwarding Process","text":"<ol> <li>Enable Forwarding:</li> <li>Add port 80 forwarding rule before certificate generation</li> <li> <p>Notify technical team when ready</p> </li> <li> <p>Certificate Generation:</p> </li> <li>Technical team runs SSL automation script</li> <li> <p>Process takes 2-5 minutes</p> </li> <li> <p>Disable Forwarding:</p> </li> <li>Remove port 80 forwarding rule after completion</li> <li>Confirm with technical team</li> </ol>"},{"location":"deployment/it-coordination/#task-3-network-security-configuration","title":"Task 3: Network Security Configuration","text":""},{"location":"deployment/it-coordination/#firewall-rules","title":"Firewall Rules","text":"<p>Ensure these rules are configured:</p> <pre><code># Allow internal network access\nAllow from 192.168.0.0/20 to 192.168.0.100:80,443\n\n# Allow temporary external access for SSL (when needed)\nAllow from any to [public-ip]:80 (temporary only)\n\n# Block all other external access\nDeny from any to [public-ip]:* (default)\n</code></pre>"},{"location":"deployment/it-coordination/#security-considerations","title":"Security Considerations","text":"<ul> <li>Port 80 forwarding only during certificate generation</li> <li>HTTPS (port 443) can remain permanently forwarded if desired</li> <li>Internal network access always maintained</li> <li>External access blocked except during SSL validation</li> </ul>"},{"location":"deployment/it-coordination/#implementation-timeline","title":"Implementation Timeline","text":""},{"location":"deployment/it-coordination/#phase-1-initial-setup-day-1","title":"Phase 1: Initial Setup (Day 1)","text":"<p>IT Admin Tasks (15 minutes): 1. Create GoDaddy DNS A record 2. Configure internal DNS (optional) 3. Verify DNS resolution 4. Confirm office public IP with technical team</p> <p>Coordination: - Provide office public IP to technical team - Confirm DNS propagation (5-10 minutes)</p>"},{"location":"deployment/it-coordination/#phase-2-ssl-certificate-generation-day-2","title":"Phase 2: SSL Certificate Generation (Day 2)","text":"<p>IT Admin Tasks (10 minutes): 1. Enable port 80 forwarding when notified 2. Monitor certificate generation process 3. Disable port 80 forwarding when complete</p> <p>Coordination: - Technical team provides 15-minute advance notice - Be available during certificate generation window - Confirm forwarding disabled after completion</p>"},{"location":"deployment/it-coordination/#phase-3-ongoing-operations","title":"Phase 3: Ongoing Operations","text":"<p>Quarterly Renewal Process: 1. Receive automated email alert (30 days before expiry) 2. Schedule 10-minute maintenance window 3. Enable port 80 forwarding during renewal 4. Disable forwarding after completion</p>"},{"location":"deployment/it-coordination/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/it-coordination/#dns-issues","title":"DNS Issues","text":"<p>Problem: DNS not resolving <pre><code># Check DNS propagation\ndig prs.citylandcondo.com @8.8.8.8\ndig prs.citylandcondo.com @1.1.1.1\n\n# Check TTL and propagation\ndig prs.citylandcondo.com +trace\n</code></pre></p> <p>Solution: Wait 5-10 minutes for DNS propagation</p>"},{"location":"deployment/it-coordination/#port-forwarding-issues","title":"Port Forwarding Issues","text":"<p>Problem: SSL certificate generation fails 1. Verify port 80 forwarding is active 2. Test external connectivity:    <pre><code># From external network\ntelnet [office-public-ip] 80\n</code></pre> 3. Check firewall rules 4. Confirm internal server is responding</p>"},{"location":"deployment/it-coordination/#network-connectivity-issues","title":"Network Connectivity Issues","text":"<p>Problem: Internal access not working 1. Verify internal DNS configuration 2. Check internal firewall rules 3. Test direct IP access: <code>https://192.168.0.100</code></p>"},{"location":"deployment/it-coordination/#contact-information","title":"Contact Information","text":""},{"location":"deployment/it-coordination/#technical-team-contacts","title":"Technical Team Contacts","text":"<ul> <li>Primary: [Technical Lead Contact]</li> <li>Secondary: [Backup Contact]</li> <li>Emergency: [Emergency Contact]</li> </ul>"},{"location":"deployment/it-coordination/#notification-preferences","title":"Notification Preferences","text":"<ul> <li>Email: [IT Admin Email]</li> <li>Phone: [IT Admin Phone] (for urgent issues)</li> <li>Preferred Time: [Business Hours]</li> </ul>"},{"location":"deployment/it-coordination/#security-compliance","title":"Security Compliance","text":""},{"location":"deployment/it-coordination/#access-control","title":"Access Control","text":"<ul> <li>DNS Management: Restricted to authorized IT staff</li> <li>Port Forwarding: Temporary access only</li> <li>Certificate Files: Secured on internal server</li> </ul>"},{"location":"deployment/it-coordination/#audit-trail","title":"Audit Trail","text":"<ul> <li>DNS Changes: Logged in GoDaddy admin panel</li> <li>Port Forwarding: Logged in router/firewall</li> <li>SSL Renewals: Logged in monitoring system</li> </ul>"},{"location":"deployment/it-coordination/#compliance-requirements","title":"Compliance Requirements","text":"<ul> <li>Data Security: All traffic encrypted with valid SSL</li> <li>Network Security: External access minimized</li> <li>Change Management: All changes documented and approved</li> </ul> <p>Coordination Complete</p> <p>With proper IT coordination, the PRS system can maintain professional SSL certificates while ensuring office network security.</p> <p>Automation Benefits</p> <p>After initial setup, the process is largely automated with minimal IT intervention required (quarterly, 5-10 minutes).</p>"},{"location":"deployment/process/","title":"Deployment Process","text":""},{"location":"deployment/process/#overview","title":"Overview","text":"<p>This guide covers the complete deployment process for the PRS on-premises production environment, from initial setup to final validation.</p>"},{"location":"deployment/process/#pre-deployment-checklist","title":"Pre-Deployment Checklist","text":""},{"location":"deployment/process/#validation","title":"Validation","text":"<ul> <li>[ ] RAM: 16GB available and recognized</li> <li>[ ] SSD: 470GB RAID1 mounted at <code>/mnt/ssd</code></li> <li>[ ] HDD: 2.4TB RAID5 mounted at <code>/mnt/hdd</code></li> <li>[ ] Network: 1 Gbps interface configured</li> <li>[ ] UPS: Backup power system operational</li> <li>[ ] Firewall: Network security configured</li> </ul>"},{"location":"deployment/process/#prerequisites","title":"Prerequisites","text":"<ul> <li>[ ] Operating System: Ubuntu 20.04+ or CentOS 8+</li> <li>[ ] Docker: Version 20.10+ installed</li> <li>[ ] Docker Compose: Version 2.0+ installed</li> <li>[ ] Git: Version 2.25+ installed</li> <li>[ ] SSL Certificates: Valid certificates available</li> </ul>"},{"location":"deployment/process/#deployment-steps","title":"Deployment Steps","text":""},{"location":"deployment/process/#1-environment-preparation","title":"1: Environment Preparation","text":""},{"location":"deployment/process/#deployment-repository","title":"Deployment Repository","text":"<pre><code># Clone the deployment repository\ncd /opt\nsudo git clone https://github.com/your-org/prs-deployment.git\nsudo chown -R $USER:$USER /opt/prs-deployment\ncd /opt/prs-deployment\n</code></pre>"},{"location":"deployment/process/#storage-directories","title":"Storage Directories","text":"<pre><code># Run storage setup script\ncd scripts\nsudo ./setup-storage.sh\n\n# Verify storage structure\nls -la /mnt/ssd/\nls -la /mnt/hdd/\n</code></pre> <p>Expected output: <pre><code>/mnt/ssd/:\n\u251c\u2500\u2500 postgresql-hot/\n\u251c\u2500\u2500 redis-data/\n\u251c\u2500\u2500 uploads/\n\u251c\u2500\u2500 logs/\n\u251c\u2500\u2500 nginx-cache/\n\u251c\u2500\u2500 prometheus-data/\n\u2514\u2500\u2500 grafana-data/\n\n/mnt/hdd/:\n\u251c\u2500\u2500 postgresql-cold/\n\u251c\u2500\u2500 postgres-backups/\n\u251c\u2500\u2500 app-logs-archive/\n\u2514\u2500\u2500 prometheus-archive/\n</code></pre></p>"},{"location":"deployment/process/#2-environment-configuration","title":"2: Environment Configuration","text":""},{"location":"deployment/process/#environment-variables","title":"Environment Variables","text":"<pre><code># Copy and customize environment file\ncp 02-docker-configuration/.env.example 02-docker-configuration/.env\n\n# Edit environment variables\nnano 02-docker-configuration/.env\n</code></pre> <p>Key environment variables to configure:</p> <pre><code># Domain and Network\nDOMAIN=your-domain.com\nSERVER_IP=192.168.0.100\nNETWORK_SUBNET=192.168.0.0/20\n\n# Database Configuration\nPOSTGRES_DB=prs_production\nPOSTGRES_USER=prs_admin\nPOSTGRES_PASSWORD=secure_password_here\n\n# Application Secrets\nJWT_SECRET=your_jwt_secret_here\nENCRYPTION_KEY=your_encryption_key_here\nOTP_KEY=your_otp_key_here\n\n# External API Integration\nCITYLAND_API_URL=https://your-api-endpoint.com\nCITYLAND_API_USERNAME=api_username\nCITYLAND_API_PASSWORD=api_password\n\n# SSL Configuration\nSSL_EMAIL=admin@your-domain.com\n</code></pre>"},{"location":"deployment/process/#repository-paths","title":"Repository Paths","text":"<pre><code># Setup repository configuration\ncp scripts/repo-config.example.sh scripts/repo-config.sh\nnano scripts/repo-config.sh\n</code></pre> <p>Update repository paths: <pre><code># Repository configuration\nexport REPOS_BASE_DIR=\"/opt/prs\"\nexport BACKEND_REPO_NAME=\"prs-backend-a\"\nexport FRONTEND_REPO_NAME=\"prs-frontend-a\"\nexport BACKEND_REPO_URL=\"https://github.com/your-org/prs-backend-a.git\"\nexport FRONTEND_REPO_URL=\"https://github.com/your-org/prs-frontend-a.git\"\n</code></pre></p>"},{"location":"deployment/process/#3-application-deployment","title":"3: Application Deployment","text":""},{"location":"deployment/process/#deployment-script","title":"Deployment Script","text":"<pre><code># Make deployment script executable\nchmod +x scripts/deploy-onprem.sh\n\n# Run deployment for production environment\n./scripts/deploy-onprem.sh prod\n</code></pre> <p>The deployment script will:</p> <ol> <li>Validate Environment: Check prerequisites and configuration</li> <li>Clone Repositories: Download application source code</li> <li>Build Images: Create Docker images for all services</li> <li>Setup Database: Initialize TimescaleDB with proper configuration</li> <li>Start Services: Launch all application services</li> <li>Configure SSL: Setup SSL certificates and HTTPS</li> <li>Run Health Checks: Validate deployment success</li> </ol>"},{"location":"deployment/process/#deployment-progress","title":"Deployment Progress","text":"<pre><code># Watch deployment logs\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml logs -f\n\n# Check service status\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml ps\n</code></pre>"},{"location":"deployment/process/#4-database-initialization","title":"4: Database Initialization","text":""},{"location":"deployment/process/#setup","title":"Setup","text":"<pre><code># Connect to database\ndocker exec -it prs-onprem-postgres-timescale psql -U prs_admin -d prs_production\n\n# Create tablespaces for dual storage\nCREATE TABLESPACE ssd_hot LOCATION '/mnt/ssd/postgresql-hot';\nCREATE TABLESPACE hdd_cold LOCATION '/mnt/hdd/postgresql-cold';\n\n# Enable TimescaleDB extension\nCREATE EXTENSION IF NOT EXISTS timescaledb;\n\n# Configure telemetry\nALTER SYSTEM SET timescaledb.telemetry = 'off';\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"deployment/process/#database-migrations","title":"Database Migrations","text":"<pre><code># Access backend container\ndocker exec -it prs-onprem-backend bash\n\n# Run database migrations\nnpm run migrate\n\n# Setup TimescaleDB hypertables and compression\nnpm run setup:timescaledb\n</code></pre>"},{"location":"deployment/process/#5-ssl-configuration","title":"5: SSL Configuration","text":""},{"location":"deployment/process/#ssl-setup","title":"SSL Setup","text":"<pre><code># Run SSL automation script\n./scripts/ssl-automation-citylandcondo.sh\n</code></pre>"},{"location":"deployment/process/#ssl-configuration-if-needed","title":"SSL Configuration (if needed)","text":"<pre><code># Copy SSL certificates to proper location\nsudo cp /path/to/your/certificate.crt 02-docker-configuration/ssl/\nsudo cp /path/to/your/private.key 02-docker-configuration/ssl/\nsudo cp /path/to/your/ca-bundle.crt 02-docker-configuration/ssl/\n\n# Update nginx configuration\nnano 02-docker-configuration/nginx/sites-enabled/default.conf\n</code></pre>"},{"location":"deployment/process/#6-service-validation","title":"6: Service Validation","text":""},{"location":"deployment/process/#check-script","title":"Check Script","text":"<pre><code># Run comprehensive health checks\n./scripts/system-health-check.sh\n</code></pre>"},{"location":"deployment/process/#service-verification","title":"Service Verification","text":"<pre><code># Check all services are running\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml ps\n\n# Test database connectivity\ndocker exec prs-onprem-postgres-timescale pg_isready -U prs_admin\n\n# Test Redis connectivity\ndocker exec prs-onprem-redis redis-cli -a $REDIS_PASSWORD ping\n\n# Test application endpoints\ncurl -k https://your-domain.com/api/health\ncurl -k https://your-domain.com/\n</code></pre>"},{"location":"deployment/process/#deployment-validation","title":"Deployment Validation","text":""},{"location":"deployment/process/#testing","title":"Testing","text":""},{"location":"deployment/process/#testing_1","title":"Testing","text":"<pre><code># Install testing tools\nsudo apt install apache2-utils\n\n# Test concurrent connections\nab -n 1000 -c 10 https://your-domain.com/\n\n# Test API endpoints\nab -n 500 -c 5 https://your-domain.com/api/health\n</code></pre>"},{"location":"deployment/process/#performance","title":"Performance","text":"<pre><code>-- Test query performance\nEXPLAIN ANALYZE SELECT COUNT(*) FROM notifications \nWHERE created_at &gt;= NOW() - INTERVAL '30 days';\n\n-- Check TimescaleDB status\nSELECT * FROM timescaledb_information.hypertables;\n\n-- Verify compression policies\nSELECT * FROM timescaledb_information.compression_settings;\n</code></pre>"},{"location":"deployment/process/#validation_1","title":"Validation","text":""},{"location":"deployment/process/#certificate-verification","title":"Certificate Verification","text":"<pre><code># Check SSL certificate\nopenssl s_client -connect your-domain.com:443 -servername your-domain.com\n\n# Verify certificate chain\ncurl -vI https://your-domain.com/\n</code></pre>"},{"location":"deployment/process/#scan","title":"Scan","text":"<pre><code># Run security hardening check\n./scripts/security-hardening-check.sh\n\n# Check open ports\nsudo netstat -tulpn | grep LISTEN\n</code></pre>"},{"location":"deployment/process/#post-deployment-configuration","title":"Post-Deployment Configuration","text":""},{"location":"deployment/process/#setup_1","title":"Setup","text":""},{"location":"deployment/process/#grafana","title":"Grafana","text":"<ol> <li>Access Grafana at <code>https://your-domain.com:3001</code></li> <li>Login with admin credentials</li> <li>Import PRS dashboards from <code>02-docker-configuration/config/grafana/dashboards/</code></li> <li>Configure data sources for Prometheus and TimescaleDB</li> </ol>"},{"location":"deployment/process/#alerting","title":"Alerting","text":"<pre><code># Configure alert rules\ncp 02-docker-configuration/config/prometheus/alerts.yml.example \\\n   02-docker-configuration/config/prometheus/alerts.yml\n\n# Restart Prometheus to load alerts\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml restart prometheus\n</code></pre>"},{"location":"deployment/process/#configuration","title":"Configuration","text":""},{"location":"deployment/process/#automated-backups","title":"Automated Backups","text":"<pre><code># Configure backup script\ncp scripts/backup-maintenance.sh.example scripts/backup-maintenance.sh\nchmod +x scripts/backup-maintenance.sh\n\n# Add to crontab for daily execution\necho \"0 2 * * * /opt/prs-deployment/scripts/backup-maintenance.sh\" | sudo crontab -\n</code></pre>"},{"location":"deployment/process/#backup-and-restore","title":"Backup and Restore","text":"<pre><code># Run manual backup\n./scripts/backup-maintenance.sh\n\n# Verify backup files\nls -la /mnt/hdd/postgres-backups/\nls -la /mnt/hdd/app-logs-archive/\n</code></pre>"},{"location":"deployment/process/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/process/#issues","title":"Issues","text":""},{"location":"deployment/process/#startup-failures","title":"Startup Failures","text":"<pre><code># Check service logs\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml logs service-name\n\n# Restart specific service\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml restart service-name\n</code></pre>"},{"location":"deployment/process/#connection-issues","title":"Connection Issues","text":"<pre><code># Check database status\ndocker exec prs-onprem-postgres-timescale pg_isready\n\n# Check database logs\ndocker logs prs-onprem-postgres-timescale\n\n# Test connection from backend\ndocker exec prs-onprem-backend npm run db:test\n</code></pre>"},{"location":"deployment/process/#certificate-issues","title":"Certificate Issues","text":"<pre><code># Regenerate SSL certificates\n./scripts/ssl-automation-citylandcondo.sh --force\n\n# Check certificate validity\nopenssl x509 -in 02-docker-configuration/ssl/certificate.crt -text -noout\n</code></pre>"},{"location":"deployment/process/#procedures","title":"Procedures","text":""},{"location":"deployment/process/#recovery","title":"Recovery","text":"<pre><code># Stop all services\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml down\n\n# Clean up containers and volumes (if needed)\ndocker system prune -f\n\n# Restart deployment\n./scripts/deploy-onprem.sh prod\n</code></pre>"},{"location":"deployment/process/#recovery_1","title":"Recovery","text":"<pre><code># Restore from backup\n./scripts/restore-database.sh /mnt/hdd/postgres-backups/latest-backup.sql\n\n# Verify data integrity\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"SELECT COUNT(*) FROM notifications;\"\n</code></pre>"},{"location":"deployment/process/#deployment-success-criteria","title":"Deployment Success Criteria","text":""},{"location":"deployment/process/#validation_2","title":"Validation","text":"<ul> <li>[ ] All services running and healthy</li> <li>[ ] Database accessible and optimized</li> <li>[ ] SSL certificates valid and configured</li> <li>[ ] Monitoring and alerting operational</li> <li>[ ] Backup procedures tested and working</li> </ul>"},{"location":"deployment/process/#validation_3","title":"Validation","text":"<ul> <li>[ ] Response time &lt;200ms for 95% of requests</li> <li>[ ] Support for 100+ concurrent users</li> <li>[ ] Database queries optimized for dual storage</li> <li>[ ] Storage tiers functioning correctly</li> </ul>"},{"location":"deployment/process/#validation_4","title":"Validation","text":"<ul> <li>[ ] SSL/TLS encryption enabled</li> <li>[ ] Firewall rules configured</li> <li>[ ] Security hardening applied</li> <li>[ ] Access controls implemented</li> </ul> <p>Deployment Complete</p> <p>Once all validation steps pass, your PRS on-premises deployment is ready for production use.</p> <p>Next Steps</p> <p>Proceed to Testing &amp; Validation for comprehensive system testing procedures.</p>"},{"location":"deployment/testing/","title":"Testing &amp; Validation","text":""},{"location":"deployment/testing/#overview","title":"Overview","text":"<p>This guide provides comprehensive testing procedures to validate your PRS on-premises deployment before production use.</p>"},{"location":"deployment/testing/#pre-production-testing","title":"Pre-Production Testing","text":""},{"location":"deployment/testing/#system-validation-tests","title":"System Validation Tests","text":""},{"location":"deployment/testing/#infrastructure-testing","title":"Infrastructure Testing","text":"<pre><code># System resource validation\n./scripts/system-health-check.sh --comprehensive\n\n# Storage performance testing\nsudo fio --name=ssd-test --filename=/mnt/ssd/test --size=1G --rw=randwrite --bs=4k --numjobs=4 --time_based --runtime=60\nsudo fio --name=hdd-test --filename=/mnt/hdd/test --size=1G --rw=randwrite --bs=64k --numjobs=2 --time_based --runtime=60\n\n# Network performance testing\niperf3 -c target-server -t 60 -P 4\n\n# Memory stress testing\nstress-ng --vm 4 --vm-bytes 75% --timeout 300s --metrics-brief\n</code></pre>"},{"location":"deployment/testing/#service-health-testing","title":"Service Health Testing","text":"<pre><code># Verify all services are running\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml ps\n\n# Check service health endpoints\ncurl -f https://your-domain.com/api/health\ncurl -f https://your-domain.com/health\n\n# Database connectivity test\ndocker exec prs-onprem-postgres-timescale pg_isready -U prs_admin\n\n# Redis connectivity test\ndocker exec prs-onprem-redis redis-cli -a $REDIS_PASSWORD ping\n</code></pre>"},{"location":"deployment/testing/#application-testing","title":"Application Testing","text":""},{"location":"deployment/testing/#api-endpoint-testing","title":"API Endpoint Testing","text":"<pre><code># Health check endpoint\ncurl -s https://your-domain.com/api/health | jq '.'\n# Expected: {\"status\": \"ok\", \"timestamp\": \"...\"}\n\n# Authentication endpoint\ncurl -X POST https://your-domain.com/api/auth/login \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"username\": \"test\", \"password\": \"test\"}'\n\n# Protected endpoint (requires authentication)\ncurl -H \"Authorization: Bearer $TOKEN\" https://your-domain.com/api/user/profile\n\n# File upload endpoint\ncurl -X POST https://your-domain.com/api/upload \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -F \"file=@test-file.pdf\"\n</code></pre>"},{"location":"deployment/testing/#database-testing","title":"Database Testing","text":"<pre><code>-- Connect to database\ndocker exec -it prs-onprem-postgres-timescale psql -U prs_admin -d prs_production\n\n-- Test basic operations\nINSERT INTO notifications (user_id, message, created_at) \nVALUES (1, 'Test notification', NOW());\n\nSELECT COUNT(*) FROM notifications;\n\n-- Test TimescaleDB functionality\nSELECT * FROM timescaledb_information.hypertables;\n\n-- Test compression\nSELECT * FROM timescaledb_information.compressed_hypertable_stats;\n\n-- Test data movement\nSELECT \n    hypertable_name,\n    tablespace_name,\n    COUNT(*) as chunk_count\nFROM timescaledb_information.chunks\nGROUP BY hypertable_name, tablespace_name;\n</code></pre>"},{"location":"deployment/testing/#performance-testing","title":"Performance Testing","text":""},{"location":"deployment/testing/#load-testing","title":"Load Testing","text":""},{"location":"deployment/testing/#application-load-testing","title":"Application Load Testing","text":"<pre><code># Install Apache Bench\nsudo apt install apache2-utils\n\n# Basic load test (100 requests, 10 concurrent)\nab -n 100 -c 10 https://your-domain.com/\n\n# API load test\nab -n 500 -c 20 -H \"Authorization: Bearer $TOKEN\" https://your-domain.com/api/health\n\n# Sustained load test (5 minutes)\nab -t 300 -c 50 https://your-domain.com/\n</code></pre>"},{"location":"deployment/testing/#database-load-testing","title":"Database Load Testing","text":"<pre><code># Install pgbench\nsudo apt install postgresql-client\n\n# Initialize pgbench\ndocker exec prs-onprem-postgres-timescale pgbench -i -s 10 prs_production\n\n# Run benchmark (10 clients, 1000 transactions each)\ndocker exec prs-onprem-postgres-timescale pgbench -c 10 -j 2 -t 1000 prs_production\n\n# Custom benchmark with application-like queries\ndocker exec prs-onprem-postgres-timescale pgbench -c 20 -j 4 -T 300 -f custom-queries.sql prs_production\n</code></pre>"},{"location":"deployment/testing/#concurrent-user-testing","title":"Concurrent User Testing","text":"<pre><code># Create concurrent user simulation script\ncat &gt; concurrent-user-test.sh &lt;&lt; 'EOF'\n#!/bin/bash\nUSERS=${1:-50}\nDURATION=${2:-300}\n\necho \"Testing $USERS concurrent users for $DURATION seconds\"\n\nfor i in $(seq 1 $USERS); do\n    (\n        while [ $SECONDS -lt $DURATION ]; do\n            curl -s https://your-domain.com/api/health &gt; /dev/null\n            sleep $(( RANDOM % 5 + 1 ))\n        done\n    ) &amp;\ndone\n\nwait\necho \"Concurrent user test completed\"\nEOF\n\nchmod +x concurrent-user-test.sh\n\n# Test 100 concurrent users for 5 minutes\n./concurrent-user-test.sh 100 300\n</code></pre>"},{"location":"deployment/testing/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"deployment/testing/#response-time-testing","title":"Response Time Testing","text":"<pre><code># Create response time test script\ncat &gt; response-time-test.sh &lt;&lt; 'EOF'\n#!/bin/bash\n\necho \"Testing response times...\"\n\n# Test various endpoints\nendpoints=(\n    \"/\"\n    \"/api/health\"\n    \"/api/auth/status\"\n    \"/api/dashboard\"\n)\n\nfor endpoint in \"${endpoints[@]}\"; do\n    echo \"Testing $endpoint:\"\n    curl -w \"Time: %{time_total}s, Size: %{size_download} bytes\\n\" \\\n         -o /dev/null -s https://your-domain.com$endpoint\ndone\nEOF\n\nchmod +x response-time-test.sh\n./response-time-test.sh\n</code></pre>"},{"location":"deployment/testing/#throughput-testing","title":"Throughput Testing","text":"<pre><code># Database throughput test\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT \n    schemaname,\n    tablename,\n    n_tup_ins + n_tup_upd + n_tup_del as total_operations,\n    (n_tup_ins + n_tup_upd + n_tup_del) / EXTRACT(EPOCH FROM (now() - stats_reset)) as ops_per_second\nFROM pg_stat_user_tables \nWHERE n_tup_ins + n_tup_upd + n_tup_del &gt; 0\nORDER BY ops_per_second DESC;\n\"\n\n# Application throughput monitoring\ndocker stats --format \"table {{.Container}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\\t{{.NetIO}}\"\n</code></pre>"},{"location":"deployment/testing/#security-testing","title":"Security Testing","text":""},{"location":"deployment/testing/#ssltls-testing","title":"SSL/TLS Testing","text":"<pre><code># Test SSL configuration\nopenssl s_client -connect your-domain.com:443 -servername your-domain.com\n\n# Check SSL certificate\ncurl -vI https://your-domain.com/\n\n# Test SSL Labs rating (external)\n# Visit: https://www.ssllabs.com/ssltest/analyze.html?d=your-domain.com\n\n# Test security headers\ncurl -I https://your-domain.com/ | grep -E \"(Strict-Transport-Security|X-Frame-Options|X-Content-Type-Options)\"\n</code></pre>"},{"location":"deployment/testing/#authentication-testing","title":"Authentication Testing","text":"<pre><code># Test authentication endpoints\n# Valid login\ncurl -X POST https://your-domain.com/api/auth/login \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"username\": \"admin\", \"password\": \"correct_password\"}'\n\n# Invalid login\ncurl -X POST https://your-domain.com/api/auth/login \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"username\": \"admin\", \"password\": \"wrong_password\"}'\n\n# Test rate limiting\nfor i in {1..10}; do\n    curl -X POST https://your-domain.com/api/auth/login \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\"username\": \"admin\", \"password\": \"wrong_password\"}'\ndone\n</code></pre>"},{"location":"deployment/testing/#access-control-testing","title":"Access Control Testing","text":"<pre><code># Test unauthorized access\ncurl -I https://your-domain.com/api/admin/users\n# Should return 401 Unauthorized\n\n# Test with valid token\ncurl -H \"Authorization: Bearer $VALID_TOKEN\" https://your-domain.com/api/user/profile\n# Should return 200 OK\n\n# Test with invalid token\ncurl -H \"Authorization: Bearer invalid_token\" https://your-domain.com/api/user/profile\n# Should return 401 Unauthorized\n</code></pre>"},{"location":"deployment/testing/#data-integrity-testing","title":"Data Integrity Testing","text":""},{"location":"deployment/testing/#backup-and-recovery-testing","title":"Backup and Recovery Testing","text":""},{"location":"deployment/testing/#database-backup-testing","title":"Database Backup Testing","text":"<pre><code># Create test data\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nINSERT INTO notifications (user_id, message, created_at) \nSELECT \n    (random() * 100)::int + 1,\n    'Test message ' || generate_series,\n    NOW() - (random() * interval '30 days')\nFROM generate_series(1, 1000);\n\"\n\n# Create backup\n./scripts/backup-maintenance.sh\n\n# Verify backup exists\nls -la /mnt/hdd/postgres-backups/daily/\n\n# Test backup integrity\nLATEST_BACKUP=$(ls -t /mnt/hdd/postgres-backups/daily/*.sql | head -1)\nsha256sum -c \"${LATEST_BACKUP}.sha256\"\n</code></pre>"},{"location":"deployment/testing/#recovery-testing","title":"Recovery Testing","text":"<pre><code># Create test database for recovery testing\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -c \"CREATE DATABASE prs_test;\"\n\n# Restore backup to test database\ndocker exec prs-onprem-postgres-timescale pg_restore -U prs_admin -d prs_test --clean --if-exists \"$LATEST_BACKUP\"\n\n# Verify data integrity\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_test -c \"SELECT COUNT(*) FROM notifications;\"\n\n# Cleanup test database\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -c \"DROP DATABASE prs_test;\"\n</code></pre>"},{"location":"deployment/testing/#data-consistency-testing","title":"Data Consistency Testing","text":"<pre><code>-- Test referential integrity\nSELECT \n    conname,\n    conrelid::regclass AS table_name,\n    confrelid::regclass AS referenced_table\nFROM pg_constraint \nWHERE contype = 'f';\n\n-- Test data consistency across tables\nSELECT \n    'users' as table_name,\n    COUNT(*) as record_count\nFROM users\nUNION ALL\nSELECT \n    'notifications',\n    COUNT(*)\nFROM notifications;\n\n-- Test TimescaleDB chunk consistency\nSELECT \n    hypertable_name,\n    COUNT(*) as total_chunks,\n    COUNT(*) FILTER (WHERE is_compressed) as compressed_chunks,\n    COUNT(*) FILTER (WHERE tablespace_name = 'ssd_hot') as ssd_chunks,\n    COUNT(*) FILTER (WHERE tablespace_name = 'hdd_cold') as hdd_chunks\nFROM timescaledb_information.chunks\nGROUP BY hypertable_name;\n</code></pre>"},{"location":"deployment/testing/#monitoring-testing","title":"Monitoring Testing","text":""},{"location":"deployment/testing/#metrics-collection-testing","title":"Metrics Collection Testing","text":"<pre><code># Test Prometheus metrics collection\ncurl -s http://localhost:9090/metrics | head -20\n\n# Test application metrics\ncurl -s https://your-domain.com/metrics | grep -E \"(http_requests|database_connections)\"\n\n# Test custom metrics\ncurl -s http://localhost:9100/metrics | grep -E \"(node_cpu|node_memory|node_disk)\"\n</code></pre>"},{"location":"deployment/testing/#alert-testing","title":"Alert Testing","text":"<pre><code># Test alert rules\ncurl -s http://localhost:9090/api/v1/rules | jq '.data.groups[].rules[] | select(.type==\"alerting\")'\n\n# Trigger test alert (high CPU)\nstress-ng --cpu $(nproc) --timeout 300s &amp;\n\n# Check alert status\ncurl -s http://localhost:9090/api/v1/alerts | jq '.data[] | select(.state==\"firing\")'\n</code></pre>"},{"location":"deployment/testing/#dashboard-testing","title":"Dashboard Testing","text":"<pre><code># Test Grafana API\ncurl -s -H \"Authorization: Bearer $GRAFANA_API_KEY\" http://localhost:3001/api/health\n\n# Test dashboard data\ncurl -s -H \"Authorization: Bearer $GRAFANA_API_KEY\" \\\n  \"http://localhost:3001/api/datasources/proxy/1/api/v1/query?query=up\"\n</code></pre>"},{"location":"deployment/testing/#user-acceptance-testing","title":"User Acceptance Testing","text":""},{"location":"deployment/testing/#functional-testing-checklist","title":"Functional Testing Checklist","text":""},{"location":"deployment/testing/#user-management","title":"User Management","text":"<ul> <li>[ ] User registration works</li> <li>[ ] User login/logout works</li> <li>[ ] Password reset works</li> <li>[ ] User profile updates work</li> <li>[ ] Role-based access control works</li> </ul>"},{"location":"deployment/testing/#core-functionality","title":"Core Functionality","text":"<ul> <li>[ ] Create requisition works</li> <li>[ ] Approve requisition works</li> <li>[ ] Generate purchase order works</li> <li>[ ] Upload documents works</li> <li>[ ] Search functionality works</li> <li>[ ] Reporting works</li> </ul>"},{"location":"deployment/testing/#performance-requirements","title":"Performance Requirements","text":"<ul> <li>[ ] Page load time &lt;2 seconds</li> <li>[ ] API response time &lt;500ms</li> <li>[ ] File upload works for 50MB files</li> <li>[ ] System supports 100+ concurrent users</li> <li>[ ] Database queries complete &lt;1 second</li> </ul>"},{"location":"deployment/testing/#browser-compatibility-testing","title":"Browser Compatibility Testing","text":"<pre><code># Test with different user agents\ncurl -H \"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" https://your-domain.com/\ncurl -H \"User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\" https://your-domain.com/\ncurl -H \"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\" https://your-domain.com/\n</code></pre>"},{"location":"deployment/testing/#automated-testing","title":"Automated Testing","text":""},{"location":"deployment/testing/#test-automation-script","title":"Test Automation Script","text":"<pre><code>#!/bin/bash\n# automated-test-suite.sh\n\nDOMAIN=\"your-domain.com\"\nLOG_FILE=\"/tmp/prs-test-results-$(date +%Y%m%d_%H%M%S).log\"\n\necho \"Starting PRS Automated Test Suite\" | tee \"$LOG_FILE\"\necho \"=================================\" | tee -a \"$LOG_FILE\"\n\n# Test 1: Service Health\necho \"Testing service health...\" | tee -a \"$LOG_FILE\"\nif curl -f -s https://$DOMAIN/api/health &gt; /dev/null; then\n    echo \"\u2713 API health check passed\" | tee -a \"$LOG_FILE\"\nelse\n    echo \"\u2717 API health check failed\" | tee -a \"$LOG_FILE\"\nfi\n\n# Test 2: Database Connectivity\necho \"Testing database connectivity...\" | tee -a \"$LOG_FILE\"\nif docker exec prs-onprem-postgres-timescale pg_isready -U prs_admin &gt; /dev/null; then\n    echo \"\u2713 Database connectivity passed\" | tee -a \"$LOG_FILE\"\nelse\n    echo \"\u2717 Database connectivity failed\" | tee -a \"$LOG_FILE\"\nfi\n\n# Test 3: Performance\necho \"Testing performance...\" | tee -a \"$LOG_FILE\"\nRESPONSE_TIME=$(curl -w \"%{time_total}\" -o /dev/null -s https://$DOMAIN/)\nif (( $(echo \"$RESPONSE_TIME &lt; 2.0\" | bc -l) )); then\n    echo \"\u2713 Response time test passed ($RESPONSE_TIME seconds)\" | tee -a \"$LOG_FILE\"\nelse\n    echo \"\u2717 Response time test failed ($RESPONSE_TIME seconds)\" | tee -a \"$LOG_FILE\"\nfi\n\n# Test 4: SSL Certificate\necho \"Testing SSL certificate...\" | tee -a \"$LOG_FILE\"\nif openssl s_client -connect $DOMAIN:443 -servername $DOMAIN &lt; /dev/null 2&gt;/dev/null | grep -q \"Verify return code: 0\"; then\n    echo \"\u2713 SSL certificate test passed\" | tee -a \"$LOG_FILE\"\nelse\n    echo \"\u2717 SSL certificate test failed\" | tee -a \"$LOG_FILE\"\nfi\n\necho \"Test suite completed. Results saved to: $LOG_FILE\"\n</code></pre>"},{"location":"deployment/testing/#test-results-documentation","title":"Test Results Documentation","text":""},{"location":"deployment/testing/#test-report-template","title":"Test Report Template","text":"<pre><code># PRS Deployment Test Report\n\n**Date**: $(date)\n**Environment**: Production\n**Tester**: [Name]\n\n## Test Summary\n- Total Tests: [number]\n- Passed: [number]\n- Failed: [number]\n- Success Rate: [percentage]\n\n## Performance Results\n- Average Response Time: [time]\n- Peak Concurrent Users: [number]\n- Database Query Performance: [time]\n- Storage Performance: [IOPS]\n\n## Security Results\n- SSL Rating: [A+/A/B/etc]\n- Authentication: [Pass/Fail]\n- Authorization: [Pass/Fail]\n- Security Headers: [Pass/Fail]\n\n## Issues Found\n1. [Issue description]\n   - Severity: [High/Medium/Low]\n   - Status: [Open/Resolved]\n   - Resolution: [Description]\n\n## Recommendations\n1. [Recommendation]\n2. [Recommendation]\n\n## Sign-off\n- Technical Lead: [Name] [Date]\n- System Administrator: [Name] [Date]\n- Business Owner: [Name] [Date]\n</code></pre> <p>Testing Complete</p> <p>Comprehensive testing ensures your PRS deployment meets performance, security, and reliability requirements before production use.</p> <p>Continuous Testing</p> <p>Implement automated testing as part of your deployment pipeline to catch issues early and maintain system quality.</p> <p>Production Readiness</p> <p>Complete all testing phases and resolve any issues before declaring the system production-ready.</p>"},{"location":"deployment/troubleshooting/","title":"Troubleshooting","text":""},{"location":"deployment/troubleshooting/#overview","title":"Overview","text":"<p>This guide provides comprehensive troubleshooting procedures for common issues in the PRS on-premises deployment.</p>"},{"location":"deployment/troubleshooting/#service-issues","title":"Service Issues","text":""},{"location":"deployment/troubleshooting/#services-wont-start","title":"Services Won't Start","text":""},{"location":"deployment/troubleshooting/#symptoms","title":"Symptoms","text":"<ul> <li>Docker containers fail to start</li> <li>Services show \"Exited\" status</li> <li>Application not accessible</li> </ul>"},{"location":"deployment/troubleshooting/#diagnosis","title":"Diagnosis","text":"<pre><code># Check service status\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml ps\n\n# Check specific service logs\ndocker logs prs-onprem-backend\ndocker logs prs-onprem-postgres-timescale\ndocker logs prs-onprem-nginx\n\n# Check Docker daemon status\nsudo systemctl status docker\n\n# Check available resources\nfree -h\ndf -h\n</code></pre>"},{"location":"deployment/troubleshooting/#solutions","title":"Solutions","text":"<p>Resource Issues: <pre><code># Check disk space\ndf -h\n# If disk full, clean up:\ndocker system prune -f\nsudo find /tmp -type f -mtime +7 -delete\n\n# Check memory\nfree -h\n# If memory low, restart services one by one:\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml restart backend\n</code></pre></p> <p>Permission Issues: <pre><code># Fix storage permissions\nsudo chown -R 999:999 /mnt/ssd/postgresql-hot /mnt/hdd/postgresql-cold\nsudo chown -R 999:999 /mnt/ssd/redis-data\nsudo chown -R 472:472 /mnt/ssd/grafana-data\n\n# Fix file permissions\nsudo chmod 644 02-docker-configuration/.env\nsudo chmod 600 02-docker-configuration/ssl/private.key\n</code></pre></p> <p>Configuration Issues: <pre><code># Validate environment file\ngrep -E \"(DOMAIN|POSTGRES_PASSWORD|JWT_SECRET)\" 02-docker-configuration/.env\n\n# Check Docker Compose syntax\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml config\n\n# Restart with fresh configuration\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml down\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml up -d\n</code></pre></p>"},{"location":"deployment/troubleshooting/#database-connection-issues","title":"Database Connection Issues","text":""},{"location":"deployment/troubleshooting/#symptoms_1","title":"Symptoms","text":"<ul> <li>Backend can't connect to database</li> <li>\"Connection refused\" errors</li> <li>Timeout errors</li> </ul>"},{"location":"deployment/troubleshooting/#diagnosis_1","title":"Diagnosis","text":"<pre><code># Check PostgreSQL status\ndocker exec prs-onprem-postgres-timescale pg_isready -U prs_admin\n\n# Check database logs\ndocker logs prs-onprem-postgres-timescale --tail 50\n\n# Test connection from backend\ndocker exec prs-onprem-backend npm run db:test\n\n# Check connection count\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT count(*) as connections, state \nFROM pg_stat_activity \nGROUP BY state;\n\"\n</code></pre>"},{"location":"deployment/troubleshooting/#solutions_1","title":"Solutions","text":"<p>Database Not Ready: <pre><code># Wait for database to be ready\nwhile ! docker exec prs-onprem-postgres-timescale pg_isready -U prs_admin; do\n    echo \"Waiting for PostgreSQL...\"\n    sleep 5\ndone\n\n# Restart database if needed\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml restart postgres\n</code></pre></p> <p>Connection Pool Exhausted: <pre><code># Kill idle connections\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT pg_terminate_backend(pid) \nFROM pg_stat_activity \nWHERE state = 'idle' \nAND query_start &lt; NOW() - INTERVAL '1 hour';\n\"\n\n# Restart backend to reset connection pool\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml restart backend\n</code></pre></p> <p>Network Issues: <pre><code># Check container network\ndocker network inspect prs_onprem_network\n\n# Test container connectivity\ndocker exec prs-onprem-backend ping prs-onprem-postgres-timescale\n\n# Restart networking\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml down\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml up -d\n</code></pre></p>"},{"location":"deployment/troubleshooting/#ssl-certificate-issues","title":"SSL Certificate Issues","text":""},{"location":"deployment/troubleshooting/#symptoms_2","title":"Symptoms","text":"<ul> <li>HTTPS not working</li> <li>Certificate warnings in browser</li> <li>SSL handshake failures</li> </ul>"},{"location":"deployment/troubleshooting/#diagnosis_2","title":"Diagnosis","text":"<pre><code># Check certificate files\nls -la 02-docker-configuration/ssl/\n\n# Check certificate validity\nopenssl x509 -in 02-docker-configuration/ssl/certificate.crt -text -noout\n\n# Check certificate expiration\nopenssl x509 -in 02-docker-configuration/ssl/certificate.crt -noout -dates\n\n# Test SSL connection\nopenssl s_client -connect your-domain.com:443 -servername your-domain.com\n</code></pre>"},{"location":"deployment/troubleshooting/#solutions_2","title":"Solutions","text":"<p>Missing Certificates: <pre><code># Generate new certificates\n./scripts/ssl-automation-citylandcondo.sh\n\n# Or copy existing certificates\nsudo cp /path/to/certificate.crt 02-docker-configuration/ssl/\nsudo cp /path/to/private.key 02-docker-configuration/ssl/\nsudo cp /path/to/ca-bundle.crt 02-docker-configuration/ssl/\n</code></pre></p> <p>Expired Certificates: <pre><code># Renew Let's Encrypt certificates\nsudo certbot renew --force-renewal\n\n# Copy renewed certificates\nsudo cp /etc/letsencrypt/live/your-domain.com/fullchain.pem 02-docker-configuration/ssl/certificate.crt\nsudo cp /etc/letsencrypt/live/your-domain.com/privkey.pem 02-docker-configuration/ssl/private.key\n\n# Restart nginx\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml restart nginx\n</code></pre></p> <p>Permission Issues: <pre><code># Fix certificate permissions\nsudo chmod 644 02-docker-configuration/ssl/certificate.crt\nsudo chmod 600 02-docker-configuration/ssl/private.key\nsudo chmod 644 02-docker-configuration/ssl/ca-bundle.crt\n\n# Restart nginx\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml restart nginx\n</code></pre></p>"},{"location":"deployment/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"deployment/troubleshooting/#slow-response-times","title":"Slow Response Times","text":""},{"location":"deployment/troubleshooting/#symptoms_3","title":"Symptoms","text":"<ul> <li>Pages load slowly (&gt;2 seconds)</li> <li>API responses take too long</li> <li>Users report performance issues</li> </ul>"},{"location":"deployment/troubleshooting/#diagnosis_3","title":"Diagnosis","text":"<pre><code># Test response times\ntime curl -s https://your-domain.com/api/health\nab -n 100 -c 10 https://your-domain.com/\n\n# Check system resources\nhtop\niostat -x 1\niotop\n\n# Check container resources\ndocker stats\n\n# Check database performance\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT query, calls, total_time, mean_time \nFROM pg_stat_statements \nORDER BY total_time DESC \nLIMIT 10;\n\"\n</code></pre>"},{"location":"deployment/troubleshooting/#solutions_3","title":"Solutions","text":"<p>High CPU Usage: <pre><code># Identify CPU-intensive processes\ntop -o %CPU\n\n# Check container CPU usage\ndocker stats --format \"table {{.Container}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\"\n\n# Restart high-CPU containers\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml restart backend\n</code></pre></p> <p>Memory Issues: <pre><code># Check memory usage\nfree -h\ndocker stats --format \"table {{.Container}}\\t{{.MemUsage}}\\t{{.MemPerc}}\"\n\n# Clear system cache\nsudo sync &amp;&amp; sudo sysctl vm.drop_caches=3\n\n# Restart memory-intensive services\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml restart backend frontend\n</code></pre></p> <p>Database Performance: <pre><code>-- Check slow queries\nSELECT query, calls, total_time, mean_time \nFROM pg_stat_statements \nORDER BY total_time DESC \nLIMIT 10;\n\n-- Update statistics\nANALYZE notifications;\nANALYZE audit_logs;\nANALYZE requisitions;\n\n-- Check for table bloat\nSELECT \n    schemaname,\n    tablename,\n    n_dead_tup,\n    n_live_tup\nFROM pg_stat_user_tables\nWHERE n_dead_tup &gt; 1000\nORDER BY n_dead_tup DESC;\n\n-- Vacuum if needed\nVACUUM ANALYZE notifications;\n</code></pre></p>"},{"location":"deployment/troubleshooting/#storage-issues","title":"Storage Issues","text":""},{"location":"deployment/troubleshooting/#symptoms_4","title":"Symptoms","text":"<ul> <li>\"No space left on device\" errors</li> <li>Slow file operations</li> <li>Database write failures</li> </ul>"},{"location":"deployment/troubleshooting/#diagnosis_4","title":"Diagnosis","text":"<pre><code># Check storage usage\ndf -h /mnt/ssd /mnt/hdd\n\n# Check inode usage\ndf -i /mnt/ssd /mnt/hdd\n\n# Check large files\nfind /mnt/ssd -type f -size +100M -exec ls -lh {} \\;\nfind /mnt/hdd -type f -size +1G -exec ls -lh {} \\;\n\n# Check RAID status\ncat /proc/mdstat\n</code></pre>"},{"location":"deployment/troubleshooting/#solutions_4","title":"Solutions","text":"<p>SSD Full (&gt;90%): <pre><code># Emergency cleanup\nfind /mnt/ssd/logs -name \"*.log\" -mtime +1 -exec gzip {} \\;\ndocker system prune -f\n\n# Move old data to HDD\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT move_chunk(chunk_name, 'hdd_cold')\nFROM timescaledb_information.chunks \nWHERE range_start &lt; NOW() - INTERVAL '14 days'\nAND tablespace_name = 'ssd_hot'\nLIMIT 10;\n\"\n\n# Compress old chunks\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT compress_chunk(chunk_name) \nFROM timescaledb_information.chunks \nWHERE range_start &lt; NOW() - INTERVAL '3 days'\nAND NOT is_compressed\nAND tablespace_name = 'ssd_hot';\n\"\n</code></pre></p> <p>HDD Full (&gt;85%): <pre><code># Clean old backups\nfind /mnt/hdd/postgres-backups -name \"*.sql\" -mtime +30 -delete\nfind /mnt/hdd/app-logs-archive -name \"*.log.gz\" -mtime +365 -delete\n\n# Compress uncompressed files\nfind /mnt/hdd -name \"*.log\" -mtime +7 -exec gzip {} \\;\n</code></pre></p> <p>RAID Issues: <pre><code># Check RAID status\ncat /proc/mdstat\nsudo mdadm --detail /dev/md0\nsudo mdadm --detail /dev/md1\n\n# If RAID degraded, check disk health\nsudo smartctl -a /dev/sda\nsudo smartctl -a /dev/sdb\n\n# Replace failed disk (if needed)\nsudo mdadm --manage /dev/md1 --add /dev/sdf\n</code></pre></p>"},{"location":"deployment/troubleshooting/#network-issues","title":"Network Issues","text":""},{"location":"deployment/troubleshooting/#connectivity-problems","title":"Connectivity Problems","text":""},{"location":"deployment/troubleshooting/#symptoms_5","title":"Symptoms","text":"<ul> <li>Can't access application from network</li> <li>Intermittent connection failures</li> <li>DNS resolution issues</li> </ul>"},{"location":"deployment/troubleshooting/#diagnosis_5","title":"Diagnosis","text":"<pre><code># Check network interfaces\nip addr show\nip route show\n\n# Test connectivity\nping 8.8.8.8\nping your-domain.com\nnslookup your-domain.com\n\n# Check firewall\nsudo ufw status verbose\nsudo iptables -L\n\n# Check port bindings\nsudo netstat -tulpn | grep LISTEN\n</code></pre>"},{"location":"deployment/troubleshooting/#solutions_5","title":"Solutions","text":"<p>Firewall Issues: <pre><code># Check and fix firewall rules\nsudo ufw status verbose\n\n# Allow required ports\nsudo ufw allow from 192.168.0.0/20 to any port 80\nsudo ufw allow from 192.168.0.0/20 to any port 443\n\n# Restart firewall\nsudo ufw disable &amp;&amp; sudo ufw enable\n</code></pre></p> <p>DNS Issues: <pre><code># Check DNS configuration\ncat /etc/resolv.conf\n\n# Test DNS resolution\nnslookup your-domain.com\ndig your-domain.com\n\n# Update DNS if needed\necho \"nameserver 8.8.8.8\" | sudo tee -a /etc/resolv.conf\n</code></pre></p> <p>Network Configuration: <pre><code># Check network configuration\ncat /etc/netplan/01-network-config.yaml\n\n# Apply network configuration\nsudo netplan apply\n\n# Restart networking\nsudo systemctl restart systemd-networkd\n</code></pre></p>"},{"location":"deployment/troubleshooting/#application-specific-issues","title":"Application-Specific Issues","text":""},{"location":"deployment/troubleshooting/#frontend-issues","title":"Frontend Issues","text":""},{"location":"deployment/troubleshooting/#symptoms_6","title":"Symptoms","text":"<ul> <li>Blank page or loading errors</li> <li>JavaScript errors in console</li> <li>API connection failures</li> </ul>"},{"location":"deployment/troubleshooting/#diagnosis_6","title":"Diagnosis","text":"<pre><code># Check frontend logs\ndocker logs prs-onprem-frontend\n\n# Check nginx logs\ndocker logs prs-onprem-nginx\n\n# Test frontend container\ndocker exec prs-onprem-frontend curl http://localhost:3000\n\n# Check browser console for errors\n</code></pre>"},{"location":"deployment/troubleshooting/#solutions_6","title":"Solutions","text":"<p>Build Issues: <pre><code># Rebuild frontend\ncd /opt/prs/prs-frontend-a\nnpm run build\n\n# Rebuild Docker image\ndocker build -t prs-frontend:latest -f ../prs-deployment/dockerfiles/Dockerfile.frontend .\n\n# Restart frontend\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml restart frontend\n</code></pre></p> <p>Configuration Issues: <pre><code># Check environment variables\ndocker exec prs-onprem-frontend env | grep VITE\n\n# Update API URL if needed\n# Edit .env file: VITE_APP_API_URL=https://your-domain.com/api\n\n# Restart frontend\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml restart frontend\n</code></pre></p>"},{"location":"deployment/troubleshooting/#backend-issues","title":"Backend Issues","text":""},{"location":"deployment/troubleshooting/#symptoms_7","title":"Symptoms","text":"<ul> <li>API endpoints return errors</li> <li>Database connection failures</li> <li>Authentication issues</li> </ul>"},{"location":"deployment/troubleshooting/#diagnosis_7","title":"Diagnosis","text":"<pre><code># Check backend logs\ndocker logs prs-onprem-backend --tail 100\n\n# Test API endpoints\ncurl -s https://your-domain.com/api/health\ncurl -s https://your-domain.com/api/auth/status\n\n# Check backend container\ndocker exec prs-onprem-backend npm run status\n</code></pre>"},{"location":"deployment/troubleshooting/#solutions_7","title":"Solutions","text":"<p>Application Errors: <pre><code># Restart backend\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml restart backend\n\n# Check for memory leaks\ndocker stats prs-onprem-backend\n\n# Update dependencies if needed\ndocker exec prs-onprem-backend npm update\n</code></pre></p> <p>Database Migration Issues: <pre><code># Check migration status\ndocker exec prs-onprem-backend npm run migrate:status\n\n# Run pending migrations\ndocker exec prs-onprem-backend npm run migrate\n\n# Reset migrations if needed (DANGEROUS)\ndocker exec prs-onprem-backend npm run migrate:reset\ndocker exec prs-onprem-backend npm run migrate\n</code></pre></p>"},{"location":"deployment/troubleshooting/#emergency-procedures","title":"Emergency Procedures","text":""},{"location":"deployment/troubleshooting/#complete-system-recovery","title":"Complete System Recovery","text":"<pre><code>#!/bin/bash\n# Emergency system recovery procedure\n\necho \"Starting emergency recovery...\"\n\n# Stop all services\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml down\n\n# Clean Docker system\ndocker system prune -f\n\n# Check and fix storage permissions\nsudo chown -R 999:999 /mnt/ssd/postgresql-hot /mnt/hdd/postgresql-cold\nsudo chown -R 999:999 /mnt/ssd/redis-data\nsudo chown -R 472:472 /mnt/ssd/grafana-data\n\n# Restart Docker daemon\nsudo systemctl restart docker\n\n# Start services one by one\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml up -d postgres\nsleep 30\n\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml up -d redis\nsleep 10\n\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml up -d backend\nsleep 20\n\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml up -d frontend nginx\n\n# Run health checks\n./scripts/system-health-check.sh\n\necho \"Emergency recovery completed\"\n</code></pre>"},{"location":"deployment/troubleshooting/#data-recovery","title":"Data Recovery","text":"<pre><code>#!/bin/bash\n# Emergency data recovery from backup\n\nBACKUP_DATE=\"$1\"\nif [ -z \"$BACKUP_DATE\" ]; then\n    echo \"Usage: $0 &lt;YYYYMMDD&gt;\"\n    exit 1\nfi\n\necho \"Starting data recovery from $BACKUP_DATE...\"\n\n# Stop application services\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml stop backend frontend worker\n\n# Restore database\nBACKUP_FILE=\"/mnt/hdd/postgres-backups/daily/prs_full_backup_${BACKUP_DATE}_*.sql\"\nif [ -f $BACKUP_FILE ]; then\n    docker exec prs-onprem-postgres-timescale pg_restore -U prs_admin -d prs_production --clean --if-exists \"$BACKUP_FILE\"\nelse\n    echo \"Backup file not found: $BACKUP_FILE\"\n    exit 1\nfi\n\n# Restore Redis data\nREDIS_BACKUP=\"/mnt/hdd/redis-backups/redis_backup_${BACKUP_DATE}_*.rdb.gz\"\nif [ -f $REDIS_BACKUP ]; then\n    docker-compose -f 02-docker-configuration/docker-compose.onprem.yml stop redis\n    gunzip -c \"$REDIS_BACKUP\" &gt; /mnt/ssd/redis-data/dump.rdb\n    docker-compose -f 02-docker-configuration/docker-compose.onprem.yml start redis\nfi\n\n# Restore file uploads\nFILE_BACKUP=\"/mnt/hdd/file-backups/$BACKUP_DATE\"\nif [ -d \"$FILE_BACKUP\" ]; then\n    rsync -av \"$FILE_BACKUP/\" /mnt/ssd/uploads/\nfi\n\n# Start services\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml start backend frontend worker\n\necho \"Data recovery completed\"\n</code></pre> <p>Troubleshooting Guide</p> <p>This comprehensive troubleshooting guide covers the most common issues and their solutions for the PRS on-premises deployment.</p> <p>Prevention</p> <p>Regular monitoring, maintenance, and backups are the best way to prevent issues. Follow the daily operations checklist to maintain system health.</p> <p>Emergency Procedures</p> <p>Emergency procedures should only be used when normal troubleshooting steps fail. Always ensure you have recent backups before performing emergency recovery.</p>"},{"location":"getting-started/overview/","title":"Getting Started Overview","text":""},{"location":"getting-started/overview/#welcome-to-prs-on-premises","title":"Welcome to PRS On-Premises","text":"<p>This guide will help you deploy and configure the PRS (Procurement and Requisition System) in your on-premises environment for maximum performance, security, and reliability.</p>"},{"location":"getting-started/overview/#what-youll-achieve","title":"What You'll Achieve","text":"<p>By following this documentation, you'll deploy a production-ready system with:</p>"},{"location":"getting-started/overview/#performance-improvements","title":"Performance Improvements","text":"<ul> <li>233% more concurrent users (100+ vs 30 on cloud)</li> <li>60-75% faster response times (50-200ms vs 200-500ms)</li> <li>400% better database performance (500+ vs 100 queries/sec)</li> <li>12,000% more storage capacity (2.4TB vs 20GB)</li> </ul>"},{"location":"getting-started/overview/#enterprise-features","title":"Enterprise Features","text":"<ul> <li>Dual Storage Architecture with automatic SSD/HDD tiering</li> <li>Zero-Deletion Data Policy with TimescaleDB compression</li> <li>Enterprise Security with SSL/TLS and hardening</li> <li>Automated Operations with monitoring and backup</li> <li>High Availability with redundant storage (RAID)</li> </ul>"},{"location":"getting-started/overview/#deployment-path","title":"Deployment Path","text":""},{"location":"getting-started/overview/#phase-1-infrastructure-setup-2-3-hours","title":"Phase 1: Infrastructure Setup (2-3 hours)","text":"<ol> <li>Hardware Requirements - Verify system specifications</li> <li>Prerequisites - Install required software</li> <li>Environment Setup - Configure system environment</li> </ol>"},{"location":"getting-started/overview/#phase-2-application-deployment-1-2-hours","title":"Phase 2: Application Deployment (1-2 hours)","text":"<ol> <li>Docker Configuration - Setup container environment</li> <li>Database Setup - Configure TimescaleDB</li> <li>SSL Configuration - Secure with HTTPS</li> </ol>"},{"location":"getting-started/overview/#phase-3-operations-setup-1-hour","title":"Phase 3: Operations Setup (1 hour)","text":"<ol> <li>Monitoring Setup - Configure dashboards and alerts</li> <li>Backup Configuration - Setup automated backups</li> <li>Testing &amp; Validation - Verify deployment</li> </ol>"},{"location":"getting-started/overview/#quick-start-options","title":"Quick Start Options","text":""},{"location":"getting-started/overview/#option-1-automated-deployment-recommended","title":"Option 1: Automated Deployment (Recommended)","text":"<pre><code># Clone deployment repository\ngit clone https://github.com/your-org/prs-deployment.git\ncd prs-deployment\n\n# Run automated setup\n./scripts/deploy-onprem.sh prod\n</code></pre>"},{"location":"getting-started/overview/#option-2-manual-step-by-step","title":"Option 2: Manual Step-by-Step","text":"<p>Follow the detailed guides in order: 1. Prerequisites 2. Environment Setup 3. Docker Configuration 4. Database Setup</p>"},{"location":"getting-started/overview/#option-3-quick-start-30-minutes","title":"Option 3: Quick Start (30 minutes)","text":"<p>For experienced administrators: 1. Quick Start Guide</p>"},{"location":"getting-started/overview/#system-architecture-overview","title":"System Architecture Overview","text":"<pre><code>graph TB\n    subgraph \"User Access Layer\"\n        USERS[Users&lt;br/&gt;100+ Concurrent] --&gt; LB[Load Balancer&lt;br/&gt;Nginx]\n    end\n\n    subgraph \"Application Layer\"\n        LB --&gt; FRONTEND[Frontend&lt;br/&gt;React/Vite]\n        LB --&gt; BACKEND[Backend API&lt;br/&gt;Node.js]\n        BACKEND --&gt; WORKER[Background Workers&lt;br/&gt;Redis Queue]\n    end\n\n    subgraph \"Data Layer\"\n        BACKEND --&gt; DB[TimescaleDB&lt;br/&gt;PostgreSQL 15]\n        BACKEND --&gt; CACHE[Redis Cache&lt;br/&gt;Session Store]\n        WORKER --&gt; DB\n        WORKER --&gt; CACHE\n    end\n\n    subgraph \"Storage Tiers\"\n        DB --&gt; SSD[SSD Storage&lt;br/&gt;470GB RAID1&lt;br/&gt;Hot Data 0-30 days]\n        DB --&gt; HDD[HDD Storage&lt;br/&gt;2.4TB RAID5&lt;br/&gt;Cold Data 30+ days]\n    end\n\n    subgraph \"Operations\"\n        MONITOR[Grafana&lt;br/&gt;Monitoring] --&gt; DB\n        MONITOR --&gt; CACHE\n        BACKUP[Automated Backup&lt;br/&gt;Daily/Incremental] --&gt; HDD\n    end\n\n    style SSD fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style HDD fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px\n    style MONITOR fill:#e8f5e8,stroke:#4caf50,stroke-width:2px\n    style BACKUP fill:#fff3e0,stroke:#ff9800,stroke-width:2px</code></pre>"},{"location":"getting-started/overview/#key-concepts","title":"Key Concepts","text":""},{"location":"getting-started/overview/#dual-storage-architecture","title":"Dual Storage Architecture","text":"<ul> <li>SSD Tier: Hot data (0-30 days) for fast access (&lt;50ms)</li> <li>HDD Tier: Cold data (30+ days) for capacity (&lt;2s)</li> <li>Automatic Movement: TimescaleDB moves data based on age</li> <li>Transparent Access: Applications query normally across tiers</li> </ul>"},{"location":"getting-started/overview/#zero-deletion-policy","title":"Zero-Deletion Policy","text":"<ul> <li>No Data Loss: System never deletes data</li> <li>Compression: Reduces storage by 60-80%</li> <li>Compliance Ready: Meets regulatory requirements</li> <li>Unlimited Growth: Scales with your data needs</li> </ul>"},{"location":"getting-started/overview/#enterprise-security","title":"Enterprise Security","text":"<ul> <li>SSL/TLS Encryption: All communications secured</li> <li>Network Isolation: Multi-tier network security</li> <li>Access Controls: Role-based permissions</li> <li>Audit Logging: Complete activity tracking</li> </ul>"},{"location":"getting-started/overview/#prerequisites-checklist","title":"Prerequisites Checklist","text":"<p>Before starting, ensure you have:</p>"},{"location":"getting-started/overview/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>[ ] Server: 16GB RAM, 8+ CPU cores</li> <li>[ ] SSD Storage: 470GB RAID1 array</li> <li>[ ] HDD Storage: 2.4TB RAID5 array</li> <li>[ ] Network: 1 Gbps interface</li> <li>[ ] UPS: Uninterruptible power supply</li> </ul>"},{"location":"getting-started/overview/#software-requirements","title":"Software Requirements","text":"<ul> <li>[ ] OS: Ubuntu 20.04+ or CentOS 8+</li> <li>[ ] Docker: Version 20.10+</li> <li>[ ] Docker Compose: Version 2.0+</li> <li>[ ] Git: Version 2.25+</li> <li>[ ] SSL Certificates: Valid domain certificates</li> </ul>"},{"location":"getting-started/overview/#network-requirements","title":"Network Requirements","text":"<ul> <li>[ ] Domain Name: Registered and configured</li> <li>[ ] DNS: Points to server IP</li> <li>[ ] Firewall: Configured for internal access</li> <li>[ ] Admin Access: SSH access configured</li> </ul>"},{"location":"getting-started/overview/#access-requirements","title":"Access Requirements","text":"<ul> <li>[ ] Admin Rights: Sudo access on server</li> <li>[ ] Repository Access: Git repository credentials</li> <li>[ ] SSL Certificates: Certificate files available</li> <li>[ ] API Credentials: External API access configured</li> </ul>"},{"location":"getting-started/overview/#support-resources","title":"Support Resources","text":""},{"location":"getting-started/overview/#documentation","title":"Documentation","text":"<ul> <li>FAQ - Common questions and answers</li> <li>Troubleshooting - Problem resolution</li> <li>Command Reference - Quick command guide</li> <li>Support - Getting help</li> </ul>"},{"location":"getting-started/overview/#tools-and-scripts","title":"Tools and Scripts","text":"<ul> <li>Health Check: <code>./scripts/system-health-check.sh</code></li> <li>Backup: <code>./scripts/backup-maintenance.sh</code></li> <li>Monitoring: <code>./scripts/performance-monitor.sh</code></li> <li>Security: <code>./scripts/security-hardening-check.sh</code></li> </ul>"},{"location":"getting-started/overview/#monitoring-and-alerts","title":"Monitoring and Alerts","text":"<ul> <li>Grafana Dashboards: Real-time system metrics</li> <li>Prometheus Alerts: Automated issue detection</li> <li>Log Analysis: Centralized logging and analysis</li> <li>Performance Tracking: Response time and throughput</li> </ul>"},{"location":"getting-started/overview/#next-steps","title":"Next Steps","text":""},{"location":"getting-started/overview/#for-new-deployments","title":"For New Deployments","text":"<ol> <li>Start with Prerequisites to verify requirements</li> <li>Follow Quick Start for rapid deployment</li> <li>Review Operations Guide for maintenance</li> </ol>"},{"location":"getting-started/overview/#for-existing-systems","title":"For Existing Systems","text":"<ol> <li>Check Migration Guide for upgrade procedures</li> <li>Review Performance Optimization for tuning</li> <li>Update Security Configuration for hardening</li> </ol>"},{"location":"getting-started/overview/#for-administrators","title":"For Administrators","text":"<ol> <li>Study Daily Operations for routine tasks</li> <li>Setup Monitoring for proactive management</li> <li>Configure Backup Procedures for data protection</li> </ol> <p>Getting Help</p> <p>If you encounter issues during deployment, check the Troubleshooting Guide or FAQ for solutions.</p> <p>Ready to Start</p> <p>Once you've reviewed the prerequisites, proceed to the Quick Start Guide for rapid deployment or Prerequisites for detailed setup.</p>"},{"location":"getting-started/prerequisites/","title":"Prerequisites and Server Setup","text":"<p>Critical: Complete Before Quick Start</p> <p>These prerequisites must be completed before running the Quick Start Guide. The <code>deploy-onprem.sh</code> script will fail without proper server setup.</p>"},{"location":"getting-started/prerequisites/#what-the-deploy-script-handles-vs-what-you-must-do","title":"What the Deploy Script Handles vs. What You Must Do","text":""},{"location":"getting-started/prerequisites/#deploy-script-handles-automatically","title":"Deploy Script Handles Automatically:","text":"<ul> <li>Docker installation and configuration</li> <li>Package installation (curl, wget, git, htop, etc.)</li> <li>Storage directory creation and permissions</li> <li>Firewall configuration (UFW rules)</li> <li>SSL certificate generation (self-signed)</li> <li>Service deployment and initialization</li> </ul>"},{"location":"getting-started/prerequisites/#you-must-setup-first","title":"You Must Setup First:","text":"<ul> <li>Server hardware and OS installation</li> <li>Storage mount points (<code>/mnt/ssd</code> and <code>/mnt/hdd</code>)</li> <li>Network configuration (static IP, DNS)</li> <li>Domain DNS records (pointing to server)</li> <li>Non-root user account with sudo access</li> </ul>"},{"location":"getting-started/prerequisites/#hardware-requirements","title":"Hardware Requirements","text":"Component Minimum Recommended Purpose CPU 4 cores 8+ cores Application processing RAM 16GB 32GB Database and caching SSD Storage 100GB 200GB+ Hot data (database, cache) HDD Storage 500GB 1TB+ Backups and archives Network 1Gbps 1Gbps+ Office network connectivity"},{"location":"getting-started/prerequisites/#operating-system","title":"Operating System","text":"<p>Required: Ubuntu 24.04 LTS (deploy script optimized for this) Supported: Ubuntu 22.04 LTS (with warnings) Installation: Clean server installation (not desktop)</p>"},{"location":"getting-started/prerequisites/#critical-setup-steps","title":"Critical Setup Steps","text":""},{"location":"getting-started/prerequisites/#1-storage-mount-points-required","title":"1. Storage Mount Points (REQUIRED)","text":"<p>The deploy script checks for these mount points and will fail if they don't exist:</p> <pre><code># Create mount points\nsudo mkdir -p /mnt/ssd /mnt/hdd\n\n# Example: Mount SSD for hot data\nsudo mount /dev/nvme0n1p1 /mnt/ssd\n\n# Example: Mount HDD for backups\nsudo mount /dev/sdb1 /mnt/hdd\n\n# Make mounts permanent\necho \"/dev/nvme0n1p1 /mnt/ssd ext4 defaults 0 2\" | sudo tee -a /etc/fstab\necho \"/dev/sdb1 /mnt/hdd ext4 defaults 0 2\" | sudo tee -a /etc/fstab\n\n# Verify mounts\ndf -h /mnt/ssd /mnt/hdd\n</code></pre> <p>Deploy Script Requirement</p> <p>The <code>check_prerequisites()</code> function will exit with error if <code>/mnt/ssd</code> or <code>/mnt/hdd</code> don't exist.</p>"},{"location":"getting-started/prerequisites/#2-user-account-setup-required","title":"2. User Account Setup (REQUIRED)","text":"<pre><code># Create deployment user (if not using existing user)\nsudo adduser prsadmin\n\n# Add user to sudo group\nsudo usermod -aG sudo prsadmin\n\n# Switch to deployment user\nsu - prsadmin\n</code></pre> <p>Do Not Run as Root</p> <p>The deploy script checks <code>if [[ $EUID -eq 0 ]]</code> and will exit with error if run as root.</p>"},{"location":"getting-started/prerequisites/#3-network-configuration","title":"3. Network Configuration","text":"<pre><code># Set static IP for office network\nsudo nano /etc/netplan/00-installer-config.yaml\n</code></pre> <p>Example configuration: <pre><code>network:\n  version: 2\n  ethernets:\n    ens18:  # Adjust interface name\n      dhcp4: false\n      addresses:\n        - 192.168.0.100/20  # Your server IP within 192.168.0.0/20 range\n      gateway4: 192.168.0.1\n      nameservers:\n        addresses:\n          - 8.8.8.8\n          - 8.8.4.4\n</code></pre></p> <p>Apply configuration: <pre><code>sudo netplan apply\n</code></pre></p>"},{"location":"getting-started/prerequisites/#4-domain-dns-setup","title":"4. Domain DNS Setup","text":""},{"location":"getting-started/prerequisites/#for-godaddy-domain-recommended","title":"For GoDaddy Domain (Recommended):","text":"<ol> <li>Point domain to office public IP in GoDaddy DNS</li> <li>Create A record: <code>prs.citylandcondo.com</code> \u2192 <code>[Office Public IP]</code> (NOT 192.168.0.100)</li> <li>Configure port forwarding: Port 80 \u2192 192.168.0.100:80 (temporary, for SSL cert generation)</li> <li>Optional internal DNS: <code>prs.citylandcondo.com</code> \u2192 <code>192.168.0.100</code> (for office network performance)</li> <li>SSL automation available post-deployment with IT coordination</li> </ol>"},{"location":"getting-started/prerequisites/#for-local-domain","title":"For Local Domain:","text":"<ol> <li>Use local DNS or hosts file</li> <li>Example: <code>prs.office.local</code> \u2192 <code>192.168.0.100</code></li> <li>Self-signed certificates will be used</li> </ol>"},{"location":"getting-started/prerequisites/#prerequisites-verification","title":"Prerequisites Verification","text":"<p>Run this verification script before Quick Start:</p> <pre><code>#!/bin/bash\n# Save as check-prerequisites.sh\n\necho \"Checking PRS Deployment Prerequisites...\"\necho \"\"\n\n# Check OS\necho \"Operating System:\"\nlsb_release -d\n\n# Check RAM (deploy script requires 16GB+)\necho \"\"\necho \"Memory:\"\nTOTAL_RAM=$(free -g | awk '/^Mem:/{print $2}')\nif [ \"$TOTAL_RAM\" -lt 15 ]; then\n    echo \"FAIL: Insufficient RAM: ${TOTAL_RAM}GB (16GB required)\"\nelse\n    echo \"PASS: RAM: ${TOTAL_RAM}GB\"\nfi\n\n# Check storage mounts (CRITICAL - deploy script checks these)\necho \"\"\necho \"Storage Mounts:\"\nif [ -d \"/mnt/ssd\" ]; then\n    echo \"PASS: /mnt/ssd exists\"\n    df -h /mnt/ssd 2&gt;/dev/null || echo \"   (not mounted)\"\nelse\n    echo \"FAIL: /mnt/ssd missing - DEPLOY WILL FAIL\"\nfi\n\nif [ -d \"/mnt/hdd\" ]; then\n    echo \"PASS: /mnt/hdd exists\"\n    df -h /mnt/hdd 2&gt;/dev/null || echo \"   (not mounted)\"\nelse\n    echo \"FAIL: /mnt/hdd missing - DEPLOY WILL FAIL\"\nfi\n\n# Check user (deploy script checks this)\necho \"\"\necho \"User Account:\"\nif [ \"$EUID\" -eq 0 ]; then\n    echo \"FAIL: Running as root - DEPLOY WILL FAIL\"\nelse\n    echo \"PASS: Running as non-root user: $USER\"\nfi\n\n# Check network\necho \"\"\necho \"Network:\"\nip addr show | grep \"inet \" | grep -v \"127.0.0.1\"\n\necho \"\"\necho \"Prerequisites Check Complete!\"\necho \"\"\nif [ -d \"/mnt/ssd\" ] &amp;&amp; [ -d \"/mnt/hdd\" ] &amp;&amp; [ \"$EUID\" -ne 0 ] &amp;&amp; [ \"$TOTAL_RAM\" -ge 15 ]; then\n    echo \"PASS: Ready for Quick Start deployment!\"\nelse\n    echo \"FAIL: Fix the issues above before proceeding with deployment.\"\nfi\n</code></pre> <p>Save and run: <pre><code>chmod +x check-prerequisites.sh\n./check-prerequisites.sh\n</code></pre></p>"},{"location":"getting-started/prerequisites/#essential-system-preparation","title":"Essential System Preparation","text":""},{"location":"getting-started/prerequisites/#1-system-updates-required","title":"1. System Updates (REQUIRED)","text":"<pre><code># Update package lists and upgrade system\nsudo apt update &amp;&amp; sudo apt upgrade -y\n\n# Install essential tools\nsudo apt install -y curl wget git nano htop\n\n# Reboot if kernel was updated\nsudo reboot\n</code></pre>"},{"location":"getting-started/prerequisites/#2-github-cli-installation-required","title":"2. GitHub CLI Installation (REQUIRED)","text":"<pre><code># Install GitHub CLI for repository access\ncurl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg\nsudo chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | sudo tee /etc/apt/sources.list.d/github-cli.list &gt; /dev/null\nsudo apt update\nsudo apt install gh\n\n# Authenticate with GitHub\ngh auth login\n</code></pre> <p>GitHub Authentication Required</p> <p>The deploy script needs access to private repositories. You must complete <code>gh auth login</code> before deployment.</p>"},{"location":"getting-started/prerequisites/#3-clone-prs-deployment-repository-required","title":"3. Clone PRS Deployment Repository (REQUIRED)","text":"<pre><code># Create base directory\nsudo mkdir -p /opt/prs\nsudo chown $USER:$USER /opt/prs\n\n# Clone the deployment repository\ncd /opt/prs\ngit clone https://github.com/stratpoint-engineering/prs-deployment.git\n\n# Navigate to scripts directory\ncd prs-deployment/scripts\n\n# Make scripts executable\nchmod +x *.sh\n</code></pre> <p>Repository Structure</p> <p>The deploy script expects <code>/opt/prs</code> as the base directory and will create additional application repositories there during deployment. The <code>prs-deployment</code> repository contains all the deployment scripts and configuration.</p>"},{"location":"getting-started/prerequisites/#ready-for-quick-start","title":"Ready for Quick Start","text":"<p>Once you've completed:</p> <ol> <li>System updates and GitHub CLI installation</li> <li>Storage mount points setup (<code>/mnt/ssd</code> and <code>/mnt/hdd</code>)</li> <li>Network configuration (static IP in 192.168.0.0/20 range)</li> <li>Domain DNS pointing to your server</li> <li>Non-root user with sudo access</li> <li>Prerequisites verification (./check-prerequisites.sh)</li> </ol> <p>You're ready for the Quick Start Guide!</p> <p>What the Deploy Script Handles</p> <p>After prerequisites are met, the deploy script automatically handles: - Docker installation and configuration - Package installation (curl, wget, git, htop, etc.) - Storage directory creation and permissions - Firewall configuration (UFW rules for 192.168.0.0/20) - SSL certificate generation (self-signed initially) - Service deployment and initialization</p> <p>Time Investment</p> <p>Proper prerequisite setup takes 30-60 minutes but prevents deployment failures and saves hours of troubleshooting later.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start Guide - PRS Deployment","text":""},{"location":"getting-started/quick-start/#overview","title":"Overview","text":"<p>This guide will get you from zero to a fully operational PRS system in 2-3 hours using the proven <code>deploy-onprem.sh</code> script with helpful configuration tools.</p>"},{"location":"getting-started/quick-start/#what-youll-get","title":"What You'll Get","text":"<ul> <li>Complete PRS application stack with all services running</li> <li>SSL/TLS security with GoDaddy or Let's Encrypt support</li> <li>Enterprise backup system with optional NAS integration</li> <li>Comprehensive monitoring with Grafana dashboards</li> <li>Automated maintenance and health checks</li> <li>Office network security configuration</li> </ul>"},{"location":"getting-started/quick-start/#simple-3-step-process","title":"Simple 3-Step Process","text":"Step Duration Description 1. Configure 10-15 min Use helper script to set up configuration 2. Deploy 1-2 hours Run deploy-onprem.sh for complete deployment 3. Automate 15-30 min Set up backup and monitoring automation <p>Total Time: 2-3 hours</p> <p>Prerequisites Required</p> <p>STOP! You must complete Prerequisites first. The deployment will fail without proper server setup (storage mounts, user account, etc.).</p> <p>Proven Approach</p> <p>This guide uses the existing, battle-tested <code>deploy-onprem.sh</code> script that handles all the complex deployment logic. We simply add configuration helpers and post-deployment automation.</p> <p>Office Network Ready</p> <p>Optimized for office environments with GoDaddy SSL support and network-restricted monitoring access.</p>"},{"location":"getting-started/quick-start/#step-1-configuration-10-15-minutes","title":"Step 1: Configuration (10-15 minutes)","text":""},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<p>Complete Prerequisites First</p> <p>\ud83d\udcd6 Read Prerequisites Guide - Essential server setup required before proceeding.</p> <p>Critical requirements verified by deploy script: - Ubuntu 24.04 LTS server (22.04 supported) - System updated (<code>sudo apt update &amp;&amp; sudo apt upgrade -y</code>) - GitHub CLI installed and authenticated (<code>gh auth login</code>) - 16GB+ RAM (checked by script) - Storage mounts <code>/mnt/ssd</code> and <code>/mnt/hdd</code> (required) - Non-root user with sudo access (script fails if root) - Domain name configured (e.g., prs.citylandcondo.com) - Network connectivity and static IP in 192.168.0.0/20 range</p> <p>For GoDaddy SSL (no browser warnings): - Static public IP address from ISP (absolutely required) - Router/firewall with port forwarding capability - IT admin access to manage DNS and port forwarding</p> <p>Quick verification: <pre><code># Run this to check if you're ready\n./check-prerequisites.sh\n</code></pre></p>"},{"location":"getting-started/quick-start/#quick-configuration","title":"Quick Configuration","text":"<pre><code># Navigate to the PRS deployment directory (if not already there)\ncd /opt/prs/prs-deployment/scripts\n\n# Run the configuration helper\n./quick-setup-helper.sh\n</code></pre> <p>The helper will prompt you for: - Domain name (e.g., prs.citylandcondo.com) - Admin email for notifications - SSL method (GoDaddy/Let's Encrypt/Self-signed) - Database passwords (auto-generated) - NAS settings (optional)</p> <p>GoDaddy SSL</p> <p>If your domain is <code>*.citylandcondo.com</code>, the system will automatically use the existing GoDaddy SSL automation.</p>"},{"location":"getting-started/quick-start/#repository-configuration","title":"Repository Configuration","text":"<p>Configure backend and frontend repository URLs:</p> <pre><code># Edit the environment file to configure repositories\nnano /opt/prs/prs-deployment/02-docker-configuration/.env\n</code></pre> <p>Update these repository settings: <pre><code># Repository Configuration\nREPO_BASE_DIR=/opt/prs\nBACKEND_REPO_NAME=prs-backend-a\nFRONTEND_REPO_NAME=prs-frontend-a\n\n# Repository URLs (update with your actual repositories)\nBACKEND_REPO_URL=https://github.com/your-org/prs-backend-a.git\nFRONTEND_REPO_URL=https://github.com/your-org/prs-frontend-a.git\n\n# Git branches to use\nBACKEND_BRANCH=main\nFRONTEND_BRANCH=main\n</code></pre></p> <p>Repository URLs Required</p> <p>You must update the repository URLs to point to your actual backend and frontend repositories. The default URLs are examples and may not be accessible.</p> <p>Repository Structure</p> <p>The deploy script expects repositories to be located at: - Backend: <code>/opt/prs/prs-backend-a/</code> - Frontend: <code>/opt/prs/prs-frontend-a/</code></p> <p>GitHub Authentication</p> <p>Ensure you have GitHub CLI authenticated (<code>gh auth login</code>) or SSH keys configured for repository access.</p>"},{"location":"getting-started/quick-start/#step-2-deployment-1-2-hours","title":"Step 2: Deployment (1-2 hours)","text":""},{"location":"getting-started/quick-start/#run-the-deployment","title":"Run the Deployment","text":"<p>The proven <code>deploy-onprem.sh</code> script handles the complete deployment:</p> <pre><code># Run the complete deployment (1-2 hours)\nsudo ./deploy-onprem.sh deploy\n</code></pre> <p>This idempotent command will: - Install all dependencies (Docker, packages, etc.) - Configure storage (SSD/HDD setup with proper permissions) - Set up SSL certificates (self-signed initially) - Configure firewall for office network access (192.168.0.0/20) - Clone and build application repositories - Deploy all services (PostgreSQL+TimescaleDB, Redis, Nginx, etc.) - Initialize database with TimescaleDB tiered storage and create admin user - Configure automated data movement (SSD \u2192 HDD based on age) - Start monitoring services (Grafana, Prometheus, Node Exporter)</p> <p>Automated TimescaleDB Tiered Storage</p> <p>The deployment now automatically configures: - SSD tablespace (<code>/mnt/ssd/postgresql-hot</code>) for new data - HDD tablespace (<code>/mnt/hdd/postgresql-cold</code>) for old data - 48 hypertables with intelligent compression - Data movement policies - automatically moves data SSD \u2192 HDD after 14-60 days - Zero deletion policy - all data preserved permanently</p> <p>Idempotent Design</p> <p>The script can be run multiple times safely. It will skip completed steps and only perform necessary changes.</p>"},{"location":"getting-started/quick-start/#monitor-progress","title":"Monitor Progress","text":"<pre><code># Check what's been completed\n./deploy-onprem.sh check-state\n\n# Watch service status\nwatch docker ps\n\n# View deployment logs\ntail -f /var/log/prs-deploy.log\n\n# Check specific service logs\ndocker logs prs-onprem-backend --tail 50\ndocker logs prs-onprem-postgres-timescale --tail 50\n</code></pre>"},{"location":"getting-started/quick-start/#verify-deployment","title":"Verify Deployment","text":"<p>After deployment completes:</p> <pre><code># Check comprehensive system status\n./deploy-onprem.sh status\n\n# Run health check\n./deploy-onprem.sh health\n\n# Test application access\ncurl -k https://your-domain.com/api/health\n\n# Connect to database (optional)\n./deploy-onprem.sh db-connect\n</code></pre>"},{"location":"getting-started/quick-start/#godaddy-ssl-setup-required-for-citylandcondocom","title":"GoDaddy SSL Setup (Required for *.citylandcondo.com)","text":"<p>Office Network + GoDaddy SSL is Possible</p> <p>YES, you can use GoDaddy SSL with office-network-only setup. This requires DNS configuration and temporary port forwarding during certificate generation.</p> <p>Static Public IP Required</p> <p>A static public IP address is absolutely required for GoDaddy SSL to work. Without it: - DNS cannot point to your office - Let's Encrypt validation will fail - You'll be forced to use self-signed certificates (browser warnings)</p> <p>Before proceeding, confirm you have: - Static public IP from your ISP - Router with port forwarding capability - IT admin access to manage DNS and firewall</p> <p>If your domain is <code>*.citylandcondo.com</code>, set up proper SSL certificates:</p>"},{"location":"getting-started/quick-start/#prerequisites-it-coordination-required","title":"Prerequisites (IT Coordination Required):","text":"<ol> <li>GoDaddy DNS A Record: <code>prs.citylandcondo.com</code> \u2192 <code>[Office Public IP]</code></li> <li>Temporary Port Forwarding: Port 80 \u2192 192.168.0.100:80 (during cert generation only)</li> <li>Optional Internal DNS: <code>prs.citylandcondo.com</code> \u2192 <code>192.168.0.100</code> (for better performance)</li> </ol>"},{"location":"getting-started/quick-start/#ssl-certificate-generation","title":"SSL Certificate Generation:","text":"<pre><code># Run GoDaddy SSL automation (requires port 80 forwarding)\n./ssl-automation-citylandcondo.sh\n\n# Restart services to use new certificates\n./deploy-onprem.sh restart\n</code></pre> <p>IT Coordination Required</p> <p>See IT Network Admin Coordination Guide for detailed DNS and port forwarding instructions. The IT admin needs to temporarily enable port 80 forwarding during certificate generation (~5-10 minutes every 90 days).</p> <p>Renewal Process</p> <p>Certificates auto-renew every 90 days. You'll receive email alerts to coordinate temporary port forwarding with IT admin.</p> <p>Complete Implementation Guide</p> <p>For detailed custom domain implementation, see Custom Domain Implementation Guide.</p>"},{"location":"getting-started/quick-start/#alternative-self-signed-certificates-no-public-ip-required","title":"Alternative: Self-Signed Certificates (No Public IP Required)","text":"<p>If you do not have a static public IP, you must use self-signed certificates:</p> <pre><code># The deploy script automatically generates self-signed certificates\n# No additional configuration needed\n\n# Access via IP address to avoid domain warnings\n# https://192.168.0.100\n</code></pre> <p>Browser Warnings with Self-Signed</p> <p>Self-signed certificates will show browser security warnings on first access. Users must click \"Advanced\" \u2192 \"Proceed to site\" to continue. This is the trade-off for not having a public IP address.</p>"},{"location":"getting-started/quick-start/#step-5-production-optimization-critical","title":"Step 5: Production Optimization (CRITICAL)","text":"<p>Production Optimization Required</p> <p>This step is MANDATORY for production deployment. Skipping optimization will result in poor performance, security vulnerabilities, and system instability under load.</p>"},{"location":"getting-started/quick-start/#51-server-level-optimization","title":"5.1 Server-Level Optimization","text":"<p>Apply system-level optimizations for 100+ concurrent users:</p> <pre><code># Manual server optimization (no dedicated script yet)\n# Set CPU governor to performance\necho performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\n\n# Optimize kernel parameters\nsudo sysctl -w net.core.somaxconn=65535\nsudo sysctl -w net.ipv4.tcp_max_syn_backlog=65535\nsudo sysctl -w vm.swappiness=10\n\n# Increase file descriptor limits\necho \"* soft nofile 65536\" | sudo tee -a /etc/security/limits.conf\necho \"* hard nofile 65536\" | sudo tee -a /etc/security/limits.conf\n\n# Verify settings\nsysctl net.core.somaxconn net.ipv4.tcp_max_syn_backlog vm.swappiness\n</code></pre> <p>What this optimizes (see Performance Optimization): - CPU governor set to performance mode - Network TCP buffers tuning for high concurrency - Kernel parameters for production workload - File descriptor limits increased to 65536</p> <p>Manual Optimization</p> <p>Currently requires manual configuration. A dedicated <code>optimize-server-performance.sh</code> script can be created for automation.</p>"},{"location":"getting-started/quick-start/#52-docker-and-container-optimization","title":"5.2 Docker and Container Optimization","text":"<p>Optimize Docker daemon and container configuration:</p> <pre><code># Manual Docker optimization (no dedicated script yet)\n# Configure Docker daemon\nsudo tee /etc/docker/daemon.json &gt; /dev/null &lt;&lt;EOF\n{\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n  },\n  \"storage-driver\": \"overlay2\",\n  \"default-ulimits\": {\n    \"nofile\": {\n      \"Name\": \"nofile\",\n      \"Hard\": 65536,\n      \"Soft\": 65536\n    }\n  }\n}\nEOF\n\n# Restart Docker with new configuration\nsudo systemctl restart docker\n\n# Verify Docker optimization\ndocker info | grep -E \"Storage Driver|Logging Driver\"\n</code></pre> <p>What this optimizes (see Docker Configuration): - Logging configuration with rotation and compression - Storage driver optimization - File descriptor limits for containers - Resource allocation optimization</p> <p>Manual Optimization</p> <p>Currently requires manual configuration. The deploy script handles basic Docker setup.</p>"},{"location":"getting-started/quick-start/#53-database-optimization-automated","title":"5.3 Database Optimization (AUTOMATED)","text":"<p>Optimize PostgreSQL and TimescaleDB for production:</p> <pre><code># Apply comprehensive TimescaleDB optimization (automated script)\n./timescaledb-post-setup-optimization.sh\n\n# Run automatic TimescaleDB optimizer (Day 1 safe)\n./timescaledb-auto-optimizer.sh\n</code></pre> <p>What this optimizes automatically: - TimescaleDB compression policies for all hypertables (48 tables) - PostgreSQL memory settings optimized for 16GB RAM - Connection limits and performance tuning - Chunk compression - compresses all uncompressed chunks - Retention policies for data lifecycle management - Monitoring views for performance tracking - Background job optimization for compression and retention</p> <p>Fully Automated Optimization</p> <p>Both scripts are safe to run from Day 1 even with no data. They automatically: - Set up compression policies for future data - Compress existing data immediately - Optimize PostgreSQL settings for production - Create monitoring views for ongoing health checks</p> <p>Zero Deletion Policy Compliant</p> <p>The optimizer never removes tables - it only optimizes existing hypertables for better performance and compression.</p> <p>Schedule weekly optimization: <pre><code># Add to crontab for weekly automatic optimization\necho \"0 2 * * 0 /opt/prs/prs-deployment/scripts/timescaledb-auto-optimizer.sh\" | crontab -\n</code></pre></p>"},{"location":"getting-started/quick-start/#54-application-and-api-optimization","title":"5.4 Application and API Optimization","text":"<p>Optimize application performance and security:</p> <pre><code># Manual application optimization (no dedicated script yet)\n# Application settings are configured via environment variables in .env file\n# Edit the .env file for optimization:\n\n# API rate limiting (configure in .env)\necho \"RATE_LIMIT_WINDOW_MS=900000\" &gt;&gt; /opt/prs/prs-deployment/02-docker-configuration/.env\necho \"RATE_LIMIT_MAX_REQUESTS=100\" &gt;&gt; /opt/prs/prs-deployment/02-docker-configuration/.env\n\n# File upload limits\necho \"MAX_FILE_SIZE=50MB\" &gt;&gt; /opt/prs/prs-deployment/02-docker-configuration/.env\n\n# Restart application to apply changes\n./deploy-onprem.sh restart\n</code></pre> <p>What this optimizes (see Application Configuration): - API rate limiting via environment variables - File upload limits and security - Session management through application configuration - Memory usage optimization for Node.js</p> <p>Environment-Based Configuration</p> <p>Application optimization is primarily done through environment variables in the <code>.env</code> file rather than dedicated scripts.</p>"},{"location":"getting-started/quick-start/#55-security-hardening","title":"5.5 Security Hardening","text":"<p>Apply comprehensive security hardening:</p> <pre><code># Run security hardening check (existing script)\n./security-hardening-check.sh\n\n# Manual SSL security headers (no dedicated script yet)\n# Configure Nginx security headers in the application\n# This is typically done through the application's Nginx configuration\n# which is managed by the deploy script\n\n# Verify security settings\n./security-hardening-check.sh --verify\n</code></pre> <p>What this hardens (see Security Configuration): - Network security with UFW firewall rules (handled by deploy script) - System security checks and recommendations - Container security with isolation and limits - Database security with encrypted connections - Authentication security through application configuration</p> <p>Automated Security</p> <p>The <code>security-hardening-check.sh</code> script provides security verification and recommendations. Most security hardening is handled by the deploy script.</p>"},{"location":"getting-started/quick-start/#56-monitoring-and-alerting-setup","title":"5.6 Monitoring and Alerting Setup","text":"<p>Configure comprehensive monitoring:</p> <pre><code># Set up monitoring automation (existing script)\n./setup-monitoring-automation.sh\n\n# Generate monitoring reports (existing script)\n./generate-monitoring-report.sh\n\n# Check system health (existing script)\n./system-health-check.sh\n</code></pre> <p>What this sets up (see Monitoring Configuration): - Grafana dashboards for all services (automated setup) - Prometheus metrics collection and retention - System health monitoring automation - Performance monitoring reports - Application health checks automation</p> <p>Automated Monitoring</p> <p>The <code>setup-monitoring-automation.sh</code> script handles most monitoring configuration automatically.</p>"},{"location":"getting-started/quick-start/#57-backup-and-recovery-setup","title":"5.7 Backup and Recovery Setup","text":"<p>Configure enterprise backup system:</p> <pre><code># Set up backup automation (existing script)\n./setup-backup-automation.sh\n\n# Verify backups (existing script)\n./verify-backups.sh\n\n# Test NAS connection (existing script)\n./test-nas-connection.sh\n\n# Manual backup test\n./backup-full.sh\n</code></pre> <p>What this sets up (see Backup Operations): - Daily database backups with compression (automated) - Application data backups with versioning - NAS integration for offsite storage (if configured) - Backup verification and integrity checks (automated) - Recovery procedures through existing scripts</p> <p>Automated Backup System</p> <p>The backup system is fully automated with existing scripts for setup, verification, and testing.</p>"},{"location":"getting-started/quick-start/#58-performance-testing-and-validation","title":"5.8 Performance Testing and Validation","text":"<p>Validate production readiness:</p> <pre><code># Run performance tests (existing script)\n./performance-test.sh\n\n# Generate monitoring report (existing script)\n./generate-monitoring-report.sh\n\n# Check system performance (existing script)\n./system-performance-monitor.sh\n\n# Database performance monitoring (existing script)\n./database-performance-monitor.sh\n</code></pre> <p>What this validates: - System performance testing and monitoring - Database performance under load - Application health monitoring - Resource utilization tracking - Performance metrics collection and reporting</p> <p>Automated Testing</p> <p>Performance testing and monitoring are handled by existing scripts that provide comprehensive system validation.</p> <p>Performance Impact</p> <p>These optimizations provide: - 40-60% improvement in network throughput - 25-35% improvement in database performance - 30-50% improvement in file I/O operations - 3x improvement in concurrent connection capacity - Sub-200ms response times for 100+ concurrent users</p> <p>Restart Required</p> <p>Some optimizations require service restarts. Plan for a brief maintenance window after optimization.</p>"},{"location":"getting-started/quick-start/#step-6-timescaledb-optimization-5-10-minutes","title":"Step 6: TimescaleDB Optimization (5-10 minutes)","text":"<p>Day 1 Safe - Run Immediately</p> <p>This optimization is safe to run from Day 1 even with no data. It prepares your database for optimal performance and can be run multiple times safely.</p>"},{"location":"getting-started/quick-start/#immediate-database-optimization","title":"Immediate Database Optimization","text":"<p>After deployment completes, immediately optimize your TimescaleDB setup:</p> <pre><code># Navigate to scripts directory\ncd /opt/prs/prs-deployment/scripts\n\n# Run comprehensive TimescaleDB optimization\n./timescaledb-auto-optimizer.sh\n</code></pre> <p>What this does automatically: - \u2705 Compresses all uncompressed chunks (immediate performance boost) - \u2705 Sets up compression policies for all 48 hypertables - \u2705 Optimizes PostgreSQL settings for 16GB RAM - \u2705 Creates monitoring views for health tracking - \u2705 Configures background jobs for automatic maintenance - \u2705 Generates optimization report showing results</p>"},{"location":"getting-started/quick-start/#schedule-weekly-optimization","title":"Schedule Weekly Optimization","text":"<pre><code># Add weekly TimescaleDB optimization to crontab\n(crontab -l 2&gt;/dev/null; echo \"0 2 * * 0 /opt/prs/prs-deployment/scripts/timescaledb-auto-optimizer.sh &gt;&gt; /var/log/timescaledb-optimizer.log 2&gt;&amp;1\") | crontab -\n</code></pre> <p>Zero Deletion Policy</p> <p>The optimizer respects your zero deletion policy - it only optimizes existing tables, never removes them. Perfect for production environments.</p> <p>Monitor Results</p> <p>Check optimization results: <code>tail -f /var/log/timescaledb-optimizer.log</code></p>"},{"location":"getting-started/quick-start/#step-7-basic-automation-setup-15-30-minutes","title":"Step 7: Basic Automation Setup (15-30 minutes)","text":""},{"location":"getting-started/quick-start/#set-up-basic-backup-automation","title":"Set Up Basic Backup Automation","text":"<pre><code># Configure basic automated backups\n./setup-backup-automation.sh\n</code></pre> <p>Basic backup automation sets up: - Daily database backups at 2:00 AM - Daily application backups at 3:00 AM - Daily backup verification at 4:00 AM - Weekly maintenance on Sundays at 1:00 AM - NAS connectivity monitoring (if configured)</p>"},{"location":"getting-started/quick-start/#set-up-basic-monitoring-automation","title":"Set Up Basic Monitoring Automation","text":"<pre><code># Configure basic automated monitoring\n./setup-monitoring-automation.sh\n</code></pre> <p>Basic monitoring automation sets up: - System performance monitoring every 5 minutes - Application health checks every 10 minutes - Database performance monitoring every 15 minutes - Daily monitoring reports at 8:00 AM - Basic security hardening (fail2ban, automatic updates)</p> <p>Basic vs. Production Setup</p> <p>Step 3 sets up basic automation to get the system running. Step 4 (Production Optimization) is where comprehensive optimization and advanced monitoring are configured.</p>"},{"location":"getting-started/quick-start/#production-deployment-complete","title":"Production Deployment Complete!","text":""},{"location":"getting-started/quick-start/#access-your-production-system","title":"Access Your Production System","text":"Service URL Access Purpose Main Application <code>https://your-domain.com</code> Office Network Only Primary PRS application Grafana Monitoring <code>http://server-ip:3000</code> Office Network Only Performance dashboards Adminer (Database) <code>http://server-ip:8080</code> Office Network Only Database administration Portainer (Containers) <code>http://server-ip:9000</code> Office Network Only Container management Prometheus Metrics <code>http://server-ip:9090</code> Office Network Only Raw metrics data <p>Office Network Only</p> <p>ALL services are only accessible within the office network (192.168.0.0/20). This is an internal deployment, not a public-facing system.</p> <p>Service Access</p> <p>The deploy script shows exact URLs after completion. Use <code>./deploy-onprem.sh status</code> to see current access information.</p>"},{"location":"getting-started/quick-start/#production-system-status","title":"Production System Status","text":"<p>Your enterprise-grade PRS system now includes:</p>"},{"location":"getting-started/quick-start/#core-infrastructure","title":"Core Infrastructure:","text":"<ul> <li>High-performance application stack with optimized resource allocation</li> <li>Enterprise-grade security with SSL/TLS encryption and hardening</li> <li>Dual storage architecture (SSD for hot data, HDD for cold storage)</li> <li>Office network security configuration (192.168.0.0/20)</li> </ul>"},{"location":"getting-started/quick-start/#database-and-performance","title":"Database and Performance:","text":"<ul> <li>TimescaleDB optimization with 38 hypertables for time-series data</li> <li>PostgreSQL tuning for 100+ concurrent users</li> <li>Database compression and retention policies</li> <li>Connection pooling and query optimization</li> </ul>"},{"location":"getting-started/quick-start/#monitoring-and-operations","title":"Monitoring and Operations:","text":"<ul> <li>Comprehensive monitoring with Grafana dashboards and Prometheus metrics</li> <li>Automated alerting for critical thresholds</li> <li>Performance tracking and capacity planning</li> <li>Health checks and automated recovery</li> </ul>"},{"location":"getting-started/quick-start/#backup-and-recovery","title":"Backup and Recovery:","text":"<ul> <li>Automated backup system with optional NAS integration</li> <li>Daily verification and integrity checks</li> <li>Retention policies for compliance</li> <li>Disaster recovery procedures</li> </ul>"},{"location":"getting-started/quick-start/#security-and-compliance","title":"Security and Compliance:","text":"<ul> <li>Network security with firewall rules and intrusion detection</li> <li>Container security with isolation and resource limits</li> <li>Audit logging and security monitoring</li> <li>SSL/TLS security headers and encryption</li> <li>Automated maintenance procedures and health checks</li> <li>Office network security configuration (192.168.0.0/20)</li> </ul>"},{"location":"getting-started/quick-start/#daily-operations","title":"Daily Operations","text":"<pre><code># Check system health\n./system-health-check.sh\n\n# View service status\ndocker ps\n\n# Check backup status\nls -la /mnt/hdd/postgres-backups/daily/\n\n# View monitoring logs\ntail -f /var/log/prs-monitoring.log\n\n# Manual backup\n./backup-full.sh\n</code></pre>"},{"location":"getting-started/quick-start/#maintenance-schedule","title":"Maintenance Schedule","text":"Task Frequency Automated Script Database Backup Daily 2:00 AM \u2705 <code>backup-full.sh</code> Application Backup Daily 3:00 AM \u2705 <code>backup-application-data.sh</code> Backup Verification Daily 4:00 AM \u2705 <code>verify-backups.sh</code> TimescaleDB Optimization Weekly Sunday 2:00 AM \u2705 <code>timescaledb-auto-optimizer.sh</code> System Monitoring Every 5-15 min \u2705 <code>system-performance-monitor.sh</code> Weekly Maintenance Sunday 1:00 AM \u2705 <code>weekly-maintenance-automation.sh</code> Security Updates Weekly \u2705 <code>unattended-upgrades</code> <p>TimescaleDB Auto-Optimization</p> <p>NEW: Weekly TimescaleDB optimization automatically: - Compresses uncompressed chunks (improves performance) - Analyzes compression effectiveness - Optimizes chunk intervals - Monitors background job health - Generates optimization reports</p>"},{"location":"getting-started/quick-start/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quick-start/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/quick-start/#configuration-helper-issues","title":"Configuration Helper Issues","text":"<p>Problem: Permission denied <pre><code># Make sure script is executable\nchmod +x ./quick-setup-helper.sh\nsudo ./quick-setup-helper.sh\n</code></pre></p> <p>Problem: Domain not accessible - Check DNS configuration - Verify firewall allows ports 80/443 - For office networks, ensure domain points to server IP</p>"},{"location":"getting-started/quick-start/#deployment-issues","title":"Deployment Issues","text":"<p>Problem: deploy-onprem.sh fails <pre><code># Check what's been completed\n./deploy-onprem.sh check-state\n\n# View deployment status\n./deploy-onprem.sh status\n\n# Check Docker status\nsudo systemctl status docker\n\n# Reset deployment state if needed\n./deploy-onprem.sh reset-state\n</code></pre></p> <p>Problem: Services won't start <pre><code># Check comprehensive status\n./deploy-onprem.sh status\n\n# View service logs\ndocker logs prs-onprem-backend --tail 100\ndocker logs prs-onprem-postgres-timescale --tail 100\n\n# Restart specific services\n./deploy-onprem.sh restart\n\n# Stop and start services\n./deploy-onprem.sh stop\n./deploy-onprem.sh start\n</code></pre></p>"},{"location":"getting-started/quick-start/#ssl-certificate-issues","title":"SSL Certificate Issues","text":"<p>Problem: GoDaddy SSL automation fails <pre><code># Check if domain is correct\necho $DOMAIN\n\n# Manually run SSL script\n./ssl-automation-citylandcondo.sh\n\n# Fall back to self-signed\n# Edit .env file and set SSL_METHOD=3\n</code></pre></p> <p>Problem: Let's Encrypt fails <pre><code># Check domain DNS\nnslookup your-domain.com\n\n# Ensure ports 80/443 are open\nsudo ufw status\n\n# Try manual certificate generation\nsudo certbot certonly --standalone -d your-domain.com\n</code></pre></p>"},{"location":"getting-started/quick-start/#nas-backup-issues","title":"NAS Backup Issues","text":"<p>Problem: NAS connectivity fails <pre><code># Test NAS connection\n./test-nas-connection.sh\n\n# Check network connectivity\nping your-nas-ip\n\n# Verify credentials in nas-config.sh\ncat nas-config.sh\n</code></pre></p>"},{"location":"getting-started/quick-start/#performance-issues","title":"Performance Issues","text":"<p>Problem: System running slowly <pre><code># Check system resources\nhtop\ndf -h\n\n# Check Docker resource usage\ndocker stats\n\n# Optimize database\ndocker exec prs-onprem-postgres-timescale psql -U prs_user -d prs_production -c \"VACUUM ANALYZE;\"\n</code></pre></p>"},{"location":"getting-started/quick-start/#getting-help","title":"Getting Help","text":""},{"location":"getting-started/quick-start/#log-locations","title":"Log Locations","text":"Component Log Location Deployment <code>/var/log/prs-deploy.log</code> Application <code>docker logs prs-onprem-backend</code> Database <code>docker logs prs-onprem-postgres-timescale</code> Backup <code>/var/log/prs-backup.log</code> Monitoring <code>/var/log/prs-monitoring.log</code>"},{"location":"getting-started/quick-start/#useful-commands","title":"Useful Commands","text":"<pre><code># Comprehensive system status\n./deploy-onprem.sh status\n\n# Check deployment state\n./deploy-onprem.sh check-state\n\n# Run health checks\n./deploy-onprem.sh health\n\n# Connect to database\n./deploy-onprem.sh db-connect\n\n# View all available commands\n./deploy-onprem.sh help\n\n# Check service logs\ndocker logs prs-onprem-backend --tail 50\n\n# Manual backup test\n./backup-full.sh\n\n# System resource usage\nhtop &amp;&amp; df -h\n</code></pre>"},{"location":"getting-started/quick-start/#reset-and-restart","title":"Reset and Restart","text":"<p>Service restart: <pre><code># Restart all services\n./deploy-onprem.sh restart\n\n# Stop and start services\n./deploy-onprem.sh stop\n./deploy-onprem.sh start\n</code></pre></p> <p>Complete reset (if needed): <pre><code># Reset deployment state\n./deploy-onprem.sh reset-state\n\n# Clean Docker resources\ndocker system prune -f\n\n# Re-run deployment\n./deploy-onprem.sh deploy\n</code></pre></p> <p>Quick Start Complete</p> <p>\ud83c\udf89 Your PRS system is now fully deployed and operational!</p> <p>What you accomplished: - \u2705 Complete enterprise PRS application stack - \u2705 SSL/TLS security with automated certificate management - \u2705 Enterprise backup system with optional NAS integration - \u2705 Comprehensive monitoring with Grafana dashboards - \u2705 Automated maintenance and health checks - \u2705 Office network security configuration</p> <p>Total deployment time: 2-3 hours</p> <p>Your system is production-ready and optimized for office network use!</p> <p>Next Steps</p> <ol> <li>Access Grafana at <code>:3000</code> to configure monitoring dashboards</li> <li>Test backup system by running <code>./backup-full.sh</code></li> <li>Create user accounts in the application</li> <li>Configure email notifications for alerts</li> <li>Review security settings and adjust firewall rules as needed</li> </ol> <p>Support Resources</p> <ul> <li>Documentation: <code>/opt/prs-deployment/docs/</code></li> <li>Scripts: <code>/opt/prs-deployment/scripts/</code></li> <li>Configuration: <code>/opt/prs-deployment/02-docker-configuration/</code></li> <li>Logs: <code>/var/log/prs-*.log</code></li> </ul>"},{"location":"hardware/network/","title":"Network Setup","text":""},{"location":"hardware/network/#network-architecture","title":"Network Architecture","text":"<p>The PRS on-premises deployment uses a multi-tier network architecture designed for security, performance, and scalability.</p>"},{"location":"hardware/network/#network-topology","title":"Network Topology","text":"<pre><code>graph TB\n    subgraph \"External Network\"\n        INT[Internet] --&gt; FW[Hardware Firewall]\n        VPN[VPN Access] --&gt; FW\n        REMOTE[Remote Users] --&gt; VPN\n    end\n\n    subgraph \"DMZ Network\"\n        FW --&gt; LB[Load Balancer&lt;br/&gt;192.168.0.50]\n        LB --&gt; PROXY[Reverse Proxy&lt;br/&gt;192.168.0.100]\n    end\n\n    subgraph \"Internal Network (192.168.0.0/20)\"\n        PROXY --&gt; CLI[Client Workstations&lt;br/&gt;192.168.0.101-200]\n        PROXY --&gt; ADM[Admin Workstations&lt;br/&gt;192.168.0.201-220]\n        PROXY --&gt; SRV[PRS Server&lt;br/&gt;192.168.0.100]\n    end\n\n    subgraph \"Service Network (172.20.0.0/24)\"\n        SRV --&gt; NGINX[Nginx: 172.20.0.5]\n        SRV --&gt; BACKEND[Backend: 172.20.0.20]\n        SRV --&gt; FRONTEND[Frontend: 172.20.0.15]\n        SRV --&gt; DB[Database: 172.20.0.30]\n        SRV --&gt; REDIS[Redis: 172.20.0.35]\n        SRV --&gt; WORKER[Worker: 172.20.0.40]\n    end\n\n    subgraph \"Management Network (172.20.0.0/24)\"\n        SRV --&gt; ADMINER[Adminer: 172.20.0.10]\n        SRV --&gt; GRAFANA[Grafana: 172.20.0.25]\n        SRV --&gt; PROMETHEUS[Prometheus: 172.20.0.45]\n        SRV --&gt; PORTAINER[Portainer: 172.20.0.55]\n    end\n\n    style FW fill:#ffcdd2\n    style SRV fill:#e8f5e8,stroke:#4caf50,stroke-width:2px\n    style DB fill:#e3f2fd,stroke:#1976d2,stroke-width:2px</code></pre>"},{"location":"hardware/network/#network-configuration","title":"Network Configuration","text":""},{"location":"hardware/network/#segments","title":"Segments","text":"Network CIDR Purpose VLAN Access External Public IPs Internet access - Public DMZ 192.168.0.0/24 Public services 10 Restricted Internal 192.168.0.0/20 Client access 20 Internal Service 172.20.0.0/24 Container network - Isolated Management 172.21.0.0/24 Admin interfaces 30 Admin only"},{"location":"hardware/network/#address-allocation","title":"Address Allocation","text":""},{"location":"hardware/network/#infrastructure","title":"Infrastructure","text":"Service IP Address Port Access Level PRS Server 192.168.0.100 Multiple Internal Load Balancer 192.168.0.50 80, 443 Public DNS Server 192.168.0.10 53 Internal NTP Server 192.168.0.11 123 Internal"},{"location":"hardware/network/#services","title":"Services","text":"Service Container IP Host Port Purpose Nginx 172.20.0.5 80, 443 Reverse proxy Frontend 172.20.0.15 3000 React application Backend 172.20.0.20 4000 API server Database 172.20.0.30 5432 PostgreSQL Redis 172.20.0.35 6379 Cache server Worker 172.20.0.40 - Background jobs"},{"location":"hardware/network/#services_1","title":"Services","text":"Service Container IP Host Port Purpose Adminer 172.20.0.10 8080 Database admin Grafana 172.20.0.25 3001 Monitoring Prometheus 172.20.0.45 9090 Metrics Portainer 172.20.0.55 9000 Container admin"},{"location":"hardware/network/#network-interface-configuration","title":"Network Interface Configuration","text":""},{"location":"hardware/network/#interface-configuration","title":"Interface Configuration","text":"<pre><code># /etc/netplan/01-network-config.yaml\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp0s3:\n      addresses:\n        - 192.168.0.100/20\n      gateway4: 192.168.0.1\n      nameservers:\n        addresses:\n          - 192.168.0.10\n          - 8.8.8.8\n        search:\n          - company.local\n      dhcp4: false\n      dhcp6: false\n</code></pre>"},{"location":"hardware/network/#network-configuration_1","title":"Network Configuration","text":"<pre><code># Apply netplan configuration\nsudo netplan apply\n\n# Verify configuration\nip addr show\nip route show\n</code></pre>"},{"location":"hardware/network/#performance-tuning","title":"Performance Tuning","text":"<pre><code># TCP buffer optimization\necho 'net.core.rmem_max = 16777216' | sudo tee -a /etc/sysctl.conf\necho 'net.core.wmem_max = 16777216' | sudo tee -a /etc/sysctl.conf\necho 'net.ipv4.tcp_rmem = 4096 87380 16777216' | sudo tee -a /etc/sysctl.conf\necho 'net.ipv4.tcp_wmem = 4096 65536 16777216' | sudo tee -a /etc/sysctl.conf\n\n# Connection tracking optimization\necho 'net.netfilter.nf_conntrack_max = 262144' | sudo tee -a /etc/sysctl.conf\necho 'net.netfilter.nf_conntrack_tcp_timeout_established = 1200' | sudo tee -a /etc/sysctl.conf\n\n# Apply changes\nsudo sysctl -p\n</code></pre>"},{"location":"hardware/network/#firewall-configuration","title":"Firewall Configuration","text":""},{"location":"hardware/network/#firewall-rules","title":"Firewall Rules","text":"<pre><code># Reset firewall\nsudo ufw --force reset\n\n# Default policies\nsudo ufw default deny incoming\nsudo ufw default allow outgoing\n\n# SSH access (admin only)\nsudo ufw allow from 192.168.0.201/24 to any port 22\n\n# HTTP/HTTPS access (internal network)\nsudo ufw allow from 192.168.0.0/20 to any port 80\nsudo ufw allow from 192.168.0.0/20 to any port 443\n\n# Management interfaces (admin only)\nsudo ufw allow from 192.168.0.201/24 to any port 8080  # Adminer\nsudo ufw allow from 192.168.0.201/24 to any port 3001  # Grafana\nsudo ufw allow from 192.168.0.201/24 to any port 9000  # Portainer\nsudo ufw allow from 192.168.0.201/24 to any port 9090  # Prometheus\n\n# Enable firewall\nsudo ufw --force enable\n\n# Check status\nsudo ufw status verbose\n</code></pre>"},{"location":"hardware/network/#firewall-rules_1","title":"Firewall Rules","text":"<pre><code># Rate limiting for HTTP\nsudo ufw limit 80/tcp\nsudo ufw limit 443/tcp\n\n# Block common attack patterns\nsudo ufw deny from 192.168.0.0/20 to any port 22 comment 'Block SSH from clients'\nsudo ufw deny from 172.20.0.0/24 to any port 22 comment 'Block SSH from containers'\n\n# Allow Docker network\nsudo ufw allow in on docker0\nsudo ufw allow in on br-*\n</code></pre>"},{"location":"hardware/network/#dns-configuration","title":"DNS Configuration","text":""},{"location":"hardware/network/#dns-setup","title":"DNS Setup","text":"<pre><code># /etc/systemd/resolved.conf\n[Resolve]\nDNS=192.168.0.10 8.8.8.8\nFallbackDNS=1.1.1.1 8.8.4.4\nDomains=company.local\nDNSSEC=yes\nDNSOverTLS=opportunistic\nCache=yes\n</code></pre>"},{"location":"hardware/network/#records","title":"Records","text":"<pre><code># Internal DNS records (bind9 format)\n$ORIGIN company.local.\n$TTL 86400\n\n@       IN  SOA     ns1.company.local. admin.company.local. (\n                    2024082201  ; Serial\n                    3600        ; Refresh\n                    1800        ; Retry\n                    604800      ; Expire\n                    86400       ; Minimum TTL\n)\n\n; Name servers\n@       IN  NS      ns1.company.local.\n\n; A records\nns1     IN  A       192.168.0.10\nprs     IN  A       192.168.0.100\nadmin   IN  A       192.168.0.100\ngrafana IN  A       192.168.0.100\n\n; CNAME records\nwww     IN  CNAME   prs\napi     IN  CNAME   prs\n</code></pre>"},{"location":"hardware/network/#load-balancing","title":"Load Balancing","text":""},{"location":"hardware/network/#load-balancer-configuration","title":"Load Balancer Configuration","text":"<pre><code># /etc/nginx/sites-available/prs-lb\nupstream prs_backend {\n    least_conn;\n    server 192.168.0.100:80 max_fails=3 fail_timeout=30s;\n    # Add more servers for horizontal scaling\n    # server 192.168.0.101:80 max_fails=3 fail_timeout=30s;\n}\n\nserver {\n    listen 80;\n    listen 443 ssl http2;\n    server_name prs.company.local;\n\n    # SSL configuration\n    ssl_certificate /etc/ssl/certs/prs.crt;\n    ssl_certificate_key /etc/ssl/private/prs.key;\n\n    # Security headers\n    add_header X-Frame-Options DENY;\n    add_header X-Content-Type-Options nosniff;\n    add_header X-XSS-Protection \"1; mode=block\";\n\n    location / {\n        proxy_pass http://prs_backend;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n\n        # Connection settings\n        proxy_connect_timeout 30s;\n        proxy_send_timeout 30s;\n        proxy_read_timeout 30s;\n    }\n\n    # Health check endpoint\n    location /health {\n        access_log off;\n        return 200 \"healthy\\n\";\n        add_header Content-Type text/plain;\n    }\n}\n</code></pre>"},{"location":"hardware/network/#network-monitoring","title":"Network Monitoring","text":""},{"location":"hardware/network/#monitoring","title":"Monitoring","text":"<pre><code># Install monitoring tools\nsudo apt install iftop nethogs nload\n\n# Monitor interface bandwidth\nsudo iftop -i enp0s3\n\n# Monitor per-process network usage\nsudo nethogs enp0s3\n\n# Real-time network load\nnload enp0s3\n</code></pre>"},{"location":"hardware/network/#monitoring_1","title":"Monitoring","text":"<pre><code># Monitor active connections\nss -tuln\n\n# Monitor connection states\nss -s\n\n# Monitor Docker network\ndocker network ls\ndocker network inspect prs_onprem_network\n</code></pre>"},{"location":"hardware/network/#testing","title":"Testing","text":"<pre><code># Network throughput test\niperf3 -s  # On server\niperf3 -c 192.168.0.100 -t 60 -P 4  # On client\n\n# Latency test\nping -c 100 192.168.0.100\n\n# DNS resolution test\ndig @192.168.0.10 prs.company.local\n</code></pre>"},{"location":"hardware/network/#network-troubleshooting","title":"Network Troubleshooting","text":""},{"location":"hardware/network/#issues","title":"Issues","text":""},{"location":"hardware/network/#problems","title":"Problems","text":"<pre><code># Check interface status\nip link show\n\n# Check routing table\nip route show\n\n# Check DNS resolution\nnslookup prs.company.local\ndig prs.company.local\n\n# Test connectivity\nping 192.168.0.100\ntelnet 192.168.0.100 80\n</code></pre>"},{"location":"hardware/network/#issues_1","title":"Issues","text":"<pre><code># Check network statistics\ncat /proc/net/dev\n\n# Check for packet drops\nnetstat -i\n\n# Check network errors\ndmesg | grep -i network\n\n# Monitor network queues\ntc -s qdisc show dev enp0s3\n</code></pre>"},{"location":"hardware/network/#network-issues","title":"Network Issues","text":"<pre><code># Check Docker networks\ndocker network ls\n\n# Inspect network configuration\ndocker network inspect prs_onprem_network\n\n# Check container connectivity\ndocker exec prs-onprem-backend ping prs-onprem-postgres-timescale\n\n# Restart Docker networking\nsudo systemctl restart docker\n</code></pre>"},{"location":"hardware/network/#network-security","title":"Network Security","text":""},{"location":"hardware/network/#hardening","title":"Hardening","text":"<pre><code># Disable unused network services\nsudo systemctl disable avahi-daemon\nsudo systemctl disable cups-browsed\n\n# Enable SYN flood protection\necho 'net.ipv4.tcp_syncookies = 1' | sudo tee -a /etc/sysctl.conf\n\n# Disable IP forwarding (if not needed)\necho 'net.ipv4.ip_forward = 0' | sudo tee -a /etc/sysctl.conf\n\n# Disable IPv6 (if not used)\necho 'net.ipv6.conf.all.disable_ipv6 = 1' | sudo tee -a /etc/sysctl.conf\n</code></pre>"},{"location":"hardware/network/#detection","title":"Detection","text":"<pre><code># Install and configure fail2ban\nsudo apt install fail2ban\n\n# Configure fail2ban for SSH\nsudo tee /etc/fail2ban/jail.local &lt;&lt; EOF\n[DEFAULT]\nbantime = 3600\nfindtime = 600\nmaxretry = 3\n\n[sshd]\nenabled = true\nport = ssh\nfilter = sshd\nlogpath = /var/log/auth.log\nmaxretry = 3\nbantime = 3600\nEOF\n\n# Start fail2ban\nsudo systemctl enable fail2ban\nsudo systemctl start fail2ban\n</code></pre>"},{"location":"hardware/network/#network-capacity-planning","title":"Network Capacity Planning","text":""},{"location":"hardware/network/#utilization","title":"Utilization","text":"Metric Current Capacity Utilization Bandwidth 200 Mbps 1 Gbps 20% Connections 500 65536 &lt;1% DNS Queries 1000/min 10000/min 10% Firewall Rules 50 1000 5%"},{"location":"hardware/network/#considerations","title":"Considerations","text":""},{"location":"hardware/network/#scaling","title":"Scaling","text":"<pre><code># Add additional PRS servers\n# Update load balancer configuration\nupstream prs_backend {\n    least_conn;\n    server 192.168.0.100:80 max_fails=3 fail_timeout=30s;\n    server 192.168.0.101:80 max_fails=3 fail_timeout=30s;\n    server 192.168.0.102:80 max_fails=3 fail_timeout=30s;\n}\n</code></pre>"},{"location":"hardware/network/#upgrades","title":"Upgrades","text":"<ul> <li>10 Gbps Interface: For high-throughput requirements</li> <li>Link Aggregation: Bond multiple 1 Gbps interfaces</li> <li>VLAN Segmentation: Separate traffic types</li> <li>QoS Implementation: Prioritize critical traffic</li> </ul> <p>Network Performance</p> <p>The network is optimized for low latency and high throughput to support 100+ concurrent users with sub-200ms response times.</p> <p>Security First</p> <p>All network access is controlled through firewall rules and proper network segmentation to ensure security.</p>"},{"location":"hardware/optimization/","title":"Performance Optimization","text":""},{"location":"hardware/optimization/#overview","title":"Overview","text":"<p>This guide covers comprehensive performance optimization for the PRS on-premises deployment, targeting 100+ concurrent users with sub-200ms response times.</p>"},{"location":"hardware/optimization/#memory-optimization","title":"Memory Optimization","text":""},{"location":"hardware/optimization/#memory-allocation","title":"Memory Allocation","text":"<pre><code>pie title Memory Distribution (16GB Total)\n    \"PostgreSQL\" : 6\n    \"Backend API\" : 4\n    \"Redis Cache\" : 2\n    \"System/Docker\" : 3\n    \"Monitoring\" : 1</code></pre>"},{"location":"hardware/optimization/#configuration","title":"Configuration","text":""},{"location":"hardware/optimization/#memory-settings","title":"Memory Settings","text":"<pre><code>-- Memory configuration for 6GB allocation\nshared_buffers = 2GB                    -- 33% of allocated RAM\neffective_cache_size = 4GB              -- 67% of allocated RAM\nwork_mem = 32MB                         -- Per-query working memory\nmaintenance_work_mem = 512MB            -- Maintenance operations\nwal_buffers = 32MB                      -- WAL buffer size\n\n-- Connection and worker settings\nmax_connections = 150                   -- 5x increase from cloud\nmax_worker_processes = 32               -- Utilize all CPU cores\nmax_parallel_workers = 16               -- Parallel query execution\nmax_parallel_workers_per_gather = 4    -- Per-query parallelism\n</code></pre>"},{"location":"hardware/optimization/#api-memory-settings","title":"API Memory Settings","text":"<pre><code>// Node.js memory optimization\nprocess.env.NODE_OPTIONS = '--max-old-space-size=2048 --max-semi-space-size=128';\n\n// Connection pool optimization\nconst poolConfig = {\n  min: 5,                              // Minimum connections\n  max: 20,                             // Maximum connections (vs 3 on cloud)\n  acquire: 30000,                      // Connection timeout\n  idle: 10000,                         // Idle timeout\n  evict: 20000                         // Eviction timeout\n};\n</code></pre>"},{"location":"hardware/optimization/#memory-configuration","title":"Memory Configuration","text":"<pre><code># Redis memory settings (2GB allocation)\nmaxmemory 2gb\nmaxmemory-policy allkeys-lru\nmaxmemory-samples 5\n\n# Persistence settings\nsave 900 1\nsave 300 10\nsave 60 10000\nappendonly yes\nappendfsync everysec\n</code></pre>"},{"location":"hardware/optimization/#monitoring","title":"Monitoring","text":"<pre><code># Monitor memory usage\nfree -h\ncat /proc/meminfo\n\n# Monitor per-process memory\nps aux --sort=-%mem | head -20\n\n# Monitor Docker container memory\ndocker stats --no-stream\n\n# Check for memory leaks\nvalgrind --tool=memcheck --leak-check=full node app.js\n</code></pre>"},{"location":"hardware/optimization/#cpu-optimization","title":"CPU Optimization","text":""},{"location":"hardware/optimization/#configuration_1","title":"Configuration","text":"Setting Value Purpose CPU Cores 4+ physical Parallel processing CPU Governor performance Maximum performance CPU Affinity Balanced Distribute load NUMA Enabled Memory locality"},{"location":"hardware/optimization/#performance-tuning","title":"Performance Tuning","text":"<pre><code># Set CPU governor to performance\necho performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\n\n# Disable CPU frequency scaling\necho 1 | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo\n\n# Set CPU affinity for critical processes\ntaskset -cp 0,1 $(pgrep postgres)\ntaskset -cp 2,3 $(pgrep node)\n</code></pre>"},{"location":"hardware/optimization/#priority-optimization","title":"Priority Optimization","text":"<pre><code># Set process priorities\nsudo renice -10 $(pgrep postgres)      # Higher priority for database\nsudo renice -5 $(pgrep node)           # Higher priority for backend\nsudo renice 5 $(pgrep prometheus)      # Lower priority for monitoring\n</code></pre>"},{"location":"hardware/optimization/#storage-performance","title":"Storage Performance","text":""},{"location":"hardware/optimization/#optimization","title":"Optimization","text":"<pre><code># Enable TRIM for SSD longevity\nsudo fstrim -v /mnt/ssd\n\n# Set optimal I/O scheduler\necho noop | sudo tee /sys/block/sda/queue/scheduler\n\n# Optimize read-ahead\necho 256 | sudo tee /sys/block/sda/queue/read_ahead_kb\n\n# Set queue depth\necho 32 | sudo tee /sys/block/sda/queue/nr_requests\n</code></pre>"},{"location":"hardware/optimization/#optimization_1","title":"Optimization","text":"<pre><code># Set deadline scheduler for HDD\necho deadline | sudo tee /sys/block/md1/queue/scheduler\n\n# Optimize for sequential access\necho 8192 | sudo tee /sys/block/md1/queue/read_ahead_kb\n\n# Enable write-back caching (with UPS)\nhdparm -W1 /dev/md1\n</code></pre>"},{"location":"hardware/optimization/#optimization_2","title":"Optimization","text":"<pre><code># Mount options for performance\n# SSD mount options\n/dev/md0 /mnt/ssd ext4 defaults,noatime,discard,barrier=0 0 2\n\n# HDD mount options\n/dev/md1 /mnt/hdd ext4 defaults,noatime,data=writeback 0 2\n\n# Optimize ext4 parameters\ntune2fs -o journal_data_writeback /dev/md0\ntune2fs -O ^has_journal /dev/md0  # Remove journal for SSD (risky)\n</code></pre>"},{"location":"hardware/optimization/#network-performance","title":"Network Performance","text":""},{"location":"hardware/optimization/#tuning","title":"Tuning","text":"<pre><code># TCP buffer optimization\nnet.core.rmem_max = 16777216\nnet.core.wmem_max = 16777216\nnet.ipv4.tcp_rmem = 4096 87380 16777216\nnet.ipv4.tcp_wmem = 4096 65536 16777216\n\n# Connection optimization\nnet.core.somaxconn = 65536\nnet.core.netdev_max_backlog = 5000\nnet.ipv4.tcp_max_syn_backlog = 8192\n\n# TCP congestion control\nnet.ipv4.tcp_congestion_control = bbr\nnet.core.default_qdisc = fq\n</code></pre>"},{"location":"hardware/optimization/#performance","title":"Performance","text":"<pre><code># Nginx optimization\nworker_processes auto;\nworker_connections 4096;\nworker_rlimit_nofile 65536;\n\n# Enable sendfile and TCP optimizations\nsendfile on;\ntcp_nopush on;\ntcp_nodelay on;\n\n# Gzip compression\ngzip on;\ngzip_vary on;\ngzip_min_length 1024;\ngzip_types text/plain text/css application/json application/javascript;\n\n# Caching\nlocation ~* \\.(jpg|jpeg|png|gif|ico|css|js)$ {\n    expires 1y;\n    add_header Cache-Control \"public, immutable\";\n}\n</code></pre>"},{"location":"hardware/optimization/#database-performance","title":"Database Performance","text":""},{"location":"hardware/optimization/#optimization_3","title":"Optimization","text":"<pre><code>-- Storage optimization\nrandom_page_cost = 1.1                 -- SSD optimization\neffective_io_concurrency = 200         -- SSD concurrent I/O\nseq_page_cost = 1.0                    -- Sequential scan cost\n\n-- Checkpoint optimization\ncheckpoint_completion_target = 0.9     -- Smooth checkpoints\ncheckpoint_timeout = 15min             -- Checkpoint frequency\nmax_wal_size = 2GB                     -- WAL size limit\nmin_wal_size = 512MB                   -- Minimum WAL size\n\n-- Query optimization\ndefault_statistics_target = 100        -- Statistics detail\nconstraint_exclusion = partition       -- Partition pruning\n</code></pre>"},{"location":"hardware/optimization/#optimization_4","title":"Optimization","text":"<pre><code>-- TimescaleDB specific settings\ntimescaledb.max_background_workers = 16\nshared_preload_libraries = 'timescaledb'\n\n-- Compression optimization\nALTER TABLE notifications SET (\n    timescaledb.compress,\n    timescaledb.compress_segmentby = 'user_id',\n    timescaledb.compress_orderby = 'created_at DESC'\n);\n\n-- Chunk interval optimization\nSELECT set_chunk_time_interval('notifications', INTERVAL '1 day');\nSELECT set_chunk_time_interval('audit_logs', INTERVAL '1 day');\n</code></pre>"},{"location":"hardware/optimization/#optimization_5","title":"Optimization","text":"<pre><code>-- Time-based indexes\nCREATE INDEX CONCURRENTLY idx_notifications_time \nON notifications (created_at DESC);\n\n-- Composite indexes for common queries\nCREATE INDEX CONCURRENTLY idx_notifications_user_time \nON notifications (user_id, created_at DESC);\n\n-- Partial indexes for hot data\nCREATE INDEX CONCURRENTLY idx_notifications_recent \nON notifications (user_id, created_at DESC) \nWHERE created_at &gt;= NOW() - INTERVAL '30 days';\n</code></pre>"},{"location":"hardware/optimization/#application-performance","title":"Application Performance","text":""},{"location":"hardware/optimization/#api-optimization","title":"API Optimization","text":"<pre><code>// Connection pooling\nconst pool = new Pool({\n  host: process.env.POSTGRES_HOST,\n  database: process.env.POSTGRES_DB,\n  user: process.env.POSTGRES_USER,\n  password: process.env.POSTGRES_PASSWORD,\n  port: process.env.POSTGRES_PORT,\n  max: 20,                    // Maximum pool size\n  min: 5,                     // Minimum pool size\n  idleTimeoutMillis: 10000,   // Idle timeout\n  connectionTimeoutMillis: 30000, // Connection timeout\n});\n\n// Query optimization\nconst query = `\n  SELECT * FROM notifications \n  WHERE user_id = $1 AND created_at &gt;= $2 \n  ORDER BY created_at DESC \n  LIMIT $3\n`;\n\n// Caching strategy\nconst redis = new Redis({\n  host: process.env.REDIS_HOST,\n  port: process.env.REDIS_PORT,\n  password: process.env.REDIS_PASSWORD,\n  retryDelayOnFailover: 100,\n  maxRetriesPerRequest: 3,\n});\n</code></pre>"},{"location":"hardware/optimization/#optimization_6","title":"Optimization","text":"<pre><code>// Vite build optimization\nexport default defineConfig({\n  build: {\n    target: 'es2015',\n    minify: 'terser',\n    terserOptions: {\n      compress: {\n        drop_console: true,\n        drop_debugger: true,\n      },\n    },\n    rollupOptions: {\n      output: {\n        manualChunks: {\n          vendor: ['react', 'react-dom'],\n          utils: ['lodash', 'moment'],\n        },\n      },\n    },\n  },\n  server: {\n    hmr: {\n      overlay: false,\n    },\n  },\n});\n</code></pre>"},{"location":"hardware/optimization/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"hardware/optimization/#performance-indicators","title":"Performance Indicators","text":"Metric Target Warning Critical Response Time &lt;200ms &gt;500ms &gt;1000ms Throughput 1000 req/s &lt;500 req/s &lt;100 req/s CPU Usage &lt;60% &gt;70% &gt;85% Memory Usage &lt;75% &gt;80% &gt;90% Disk I/O &lt;80% &gt;85% &gt;95%"},{"location":"hardware/optimization/#testing","title":"Testing","text":"<pre><code># Load testing with Apache Bench\nab -n 10000 -c 100 https://your-domain.com/\n\n# API performance testing\nab -n 5000 -c 50 https://your-domain.com/api/health\n\n# Database performance testing\npgbench -i -s 100 prs_production\npgbench -c 10 -j 2 -t 1000 prs_production\n</code></pre>"},{"location":"hardware/optimization/#monitoring_1","title":"Monitoring","text":"<pre><code># System performance monitoring\nhtop\niotop\nnethogs\n\n# Database performance monitoring\nSELECT * FROM pg_stat_activity WHERE state = 'active';\nSELECT * FROM pg_stat_statements ORDER BY total_time DESC LIMIT 10;\n\n# Application performance monitoring\ndocker stats\ndocker logs prs-onprem-backend | grep \"response time\"\n</code></pre>"},{"location":"hardware/optimization/#performance-tuning-checklist","title":"Performance Tuning Checklist","text":""},{"location":"hardware/optimization/#level","title":"Level","text":"<ul> <li>[ ] CPU Governor: Set to performance mode</li> <li>[ ] Memory: Optimize allocation per service</li> <li>[ ] Storage: Configure optimal I/O schedulers</li> <li>[ ] Network: Tune TCP buffers and congestion control</li> <li>[ ] Kernel: Apply performance-oriented kernel parameters</li> </ul>"},{"location":"hardware/optimization/#level_1","title":"Level","text":"<ul> <li>[ ] Memory: Configure shared_buffers and work_mem</li> <li>[ ] Connections: Optimize max_connections and pooling</li> <li>[ ] Storage: Set random_page_cost for SSD</li> <li>[ ] Indexes: Create optimal indexes for queries</li> <li>[ ] Statistics: Update table statistics regularly</li> </ul>"},{"location":"hardware/optimization/#level_2","title":"Level","text":"<ul> <li>[ ] Connection Pooling: Configure optimal pool sizes</li> <li>[ ] Caching: Implement Redis caching strategy</li> <li>[ ] Queries: Optimize database queries</li> <li>[ ] Assets: Enable compression and caching</li> <li>[ ] Monitoring: Implement performance monitoring</li> </ul>"},{"location":"hardware/optimization/#level_3","title":"Level","text":"<ul> <li>[ ] Resource Limits: Set appropriate CPU/memory limits</li> <li>[ ] Health Checks: Configure proper health checks</li> <li>[ ] Logging: Optimize logging levels</li> <li>[ ] Networking: Use optimal network drivers</li> <li>[ ] Volumes: Use appropriate volume drivers</li> </ul>"},{"location":"hardware/optimization/#performance-troubleshooting","title":"Performance Troubleshooting","text":""},{"location":"hardware/optimization/#performance-issues","title":"Performance Issues","text":""},{"location":"hardware/optimization/#cpu-usage","title":"CPU Usage","text":"<pre><code># Identify CPU-intensive processes\ntop -o %CPU\nps aux --sort=-%cpu | head -20\n\n# Check for CPU throttling\ndmesg | grep -i \"cpu.*throttl\"\n\n# Monitor CPU frequency\nwatch -n 1 cat /proc/cpuinfo | grep MHz\n</code></pre>"},{"location":"hardware/optimization/#issues","title":"Issues","text":"<pre><code># Check for memory leaks\nps aux --sort=-%mem | head -20\ncat /proc/meminfo | grep -E \"(MemFree|MemAvailable|Buffers|Cached)\"\n\n# Check swap usage\nswapon -s\ncat /proc/swaps\n</code></pre>"},{"location":"hardware/optimization/#bottlenecks","title":"Bottlenecks","text":"<pre><code># Monitor I/O wait\niostat -x 1\n\n# Check for I/O bottlenecks\niotop -a\n\n# Monitor disk queue depth\ncat /sys/block/sda/queue/nr_requests\n</code></pre>"},{"location":"hardware/optimization/#issues_1","title":"Issues","text":"<pre><code># Monitor network utilization\niftop -i eth0\n\n# Check for packet drops\nnetstat -i\n\n# Monitor connection states\nss -s\n</code></pre> <p>Performance Target</p> <p>With proper optimization, the system should achieve &lt;200ms response times for 95% of requests with 100+ concurrent users.</p> <p>Continuous Optimization</p> <p>Performance optimization is an ongoing process. Regular monitoring and tuning ensure optimal system performance.</p>"},{"location":"hardware/requirements/","title":"Hardware Requirements","text":""},{"location":"hardware/requirements/#overview","title":"Overview","text":"<p>This guide specifies the complete hardware requirements for the PRS on-premises deployment, optimized for 100+ concurrent users with enterprise-grade performance and reliability.</p>"},{"location":"hardware/requirements/#minimum-system-requirements","title":"Minimum System Requirements","text":""},{"location":"hardware/requirements/#server-specifications","title":"Server Specifications","text":"Component Minimum Recommended Enterprise CPU 4 cores @ 2.4GHz 8 cores @ 3.0GHz 16 cores @ 3.2GHz RAM 16 GB DDR4 32 GB DDR4 64 GB DDR4 SSD Storage 470 GB RAID1 1 TB RAID1 2 TB RAID1 HDD Storage 2.4 TB RAID5 5 TB RAID5 10 TB RAID5 Network 1 Gbps 1 Gbps 10 Gbps Power 650W UPS 1000W UPS 1500W UPS"},{"location":"hardware/requirements/#performance-targets","title":"Performance Targets","text":"Metric Target With Recommended With Enterprise Concurrent Users 100+ 200+ 500+ Response Time &lt;200ms &lt;100ms &lt;50ms Database Queries/sec 500+ 1000+ 2000+ File Upload Speed 50 MB/s 100 MB/s 200 MB/s Backup Speed 100 MB/s 200 MB/s 500 MB/s"},{"location":"hardware/requirements/#detailed-hardware-specifications","title":"Detailed Hardware Specifications","text":""},{"location":"hardware/requirements/#cpu-requirements","title":"CPU Requirements","text":""},{"location":"hardware/requirements/#minimum-configuration","title":"Minimum Configuration","text":"<ul> <li>Cores: 4 physical cores (8 threads)</li> <li>Clock Speed: 2.4 GHz base frequency</li> <li>Architecture: x86_64 (Intel/AMD)</li> <li>Features: Virtualization support (VT-x/AMD-V)</li> </ul>"},{"location":"hardware/requirements/#recommended-configuration","title":"Recommended Configuration","text":"<ul> <li>Cores: 8 physical cores (16 threads)</li> <li>Clock Speed: 3.0 GHz base frequency</li> <li>Cache: 16 MB L3 cache minimum</li> <li>Examples: Intel Xeon E-2288G, AMD EPYC 7302P</li> </ul>"},{"location":"hardware/requirements/#enterprise-configuration","title":"Enterprise Configuration","text":"<ul> <li>Cores: 16+ physical cores (32+ threads)</li> <li>Clock Speed: 3.2+ GHz base frequency</li> <li>Cache: 32+ MB L3 cache</li> <li>Examples: Intel Xeon Gold 6248R, AMD EPYC 7543</li> </ul>"},{"location":"hardware/requirements/#memory-requirements","title":"Memory Requirements","text":""},{"location":"hardware/requirements/#memory-configuration","title":"Memory Configuration","text":"<pre><code>Total RAM: 16 GB (Minimum) / 32 GB (Recommended)\n\u251c\u2500\u2500 System OS: 2 GB\n\u251c\u2500\u2500 Docker Engine: 1 GB\n\u251c\u2500\u2500 PostgreSQL: 6 GB (allocated)\n\u2502   \u251c\u2500\u2500 Shared Buffers: 2 GB\n\u2502   \u251c\u2500\u2500 Effective Cache: 4 GB\n\u251c\u2500\u2500 Redis Cache: 2 GB\n\u251c\u2500\u2500 Application: 4 GB\n\u2502   \u251c\u2500\u2500 Backend API: 2 GB\n\u2502   \u251c\u2500\u2500 Frontend: 1 GB\n\u2502   \u251c\u2500\u2500 Workers: 1 GB\n\u2514\u2500\u2500 System Buffer: 1 GB\n</code></pre>"},{"location":"hardware/requirements/#memory-specifications","title":"Memory Specifications","text":"<ul> <li>Type: DDR4-2400 or higher</li> <li>Configuration: ECC memory recommended</li> <li>Channels: Dual-channel minimum</li> <li>Expandability: Support for future upgrades</li> </ul>"},{"location":"hardware/requirements/#storage-requirements","title":"Storage Requirements","text":""},{"location":"hardware/requirements/#ssd-storage-hot-data-tier","title":"SSD Storage (Hot Data Tier)","text":"<p>Purpose: High-performance storage for active data (0-30 days)</p> Configuration Capacity RAID Level Performance Minimum 470 GB RAID1 500 IOPS Recommended 1 TB RAID1 1000 IOPS Enterprise 2 TB RAID10 2000+ IOPS <p>SSD Specifications: - Interface: SATA III (6 Gbps) or NVMe - Type: Enterprise SSD (not consumer grade) - Endurance: 1+ DWPD (Drive Writes Per Day) - Examples: Samsung 883 DCT, Intel D3-S4610</p> <p>SSD Usage Breakdown: <pre><code>Total SSD: 470 GB\n\u251c\u2500\u2500 PostgreSQL Hot Data: 200 GB\n\u251c\u2500\u2500 Redis Data: 50 GB\n\u251c\u2500\u2500 Application Uploads: 100 GB\n\u251c\u2500\u2500 Application Logs: 50 GB\n\u251c\u2500\u2500 System Cache: 50 GB\n\u2514\u2500\u2500 Free Space (20%): 20 GB\n</code></pre></p>"},{"location":"hardware/requirements/#hdd-storage-cold-data-tier","title":"HDD Storage (Cold Data Tier)","text":"<p>Purpose: High-capacity storage for archived data (30+ days)</p> Configuration Capacity RAID Level Performance Minimum 2.4 TB RAID5 200 IOPS Recommended 5 TB RAID5 300 IOPS Enterprise 10+ TB RAID6 500+ IOPS <p>HDD Specifications: - Interface: SATA III (6 Gbps) - Speed: 7200 RPM minimum - Cache: 128 MB or higher - Type: Enterprise/NAS drives - Examples: WD Red Pro, Seagate IronWolf Pro</p> <p>HDD Usage Breakdown: <pre><code>Total HDD: 2.4 TB\n\u251c\u2500\u2500 PostgreSQL Cold Data: 1.5 TB\n\u251c\u2500\u2500 Database Backups: 500 GB\n\u251c\u2500\u2500 Application Backups: 200 GB\n\u251c\u2500\u2500 Log Archives: 100 GB\n\u2514\u2500\u2500 Free Space (20%): 100 GB\n</code></pre></p>"},{"location":"hardware/requirements/#network-requirements","title":"Network Requirements","text":""},{"location":"hardware/requirements/#network-interface","title":"Network Interface","text":"<ul> <li>Speed: 1 Gbps minimum (10 Gbps recommended)</li> <li>Type: Ethernet (RJ45 or SFP+)</li> <li>Redundancy: Dual NICs for high availability</li> <li>Features: Jumbo frame support (9000 MTU)</li> </ul>"},{"location":"hardware/requirements/#network-architecture","title":"Network Architecture","text":"<pre><code>graph TB\n    subgraph \"External Network\"\n        INTERNET[Internet] --&gt; FIREWALL[Firewall]\n    end\n\n    subgraph \"DMZ Network (192.168.0.0/24)\"\n        FIREWALL --&gt; SWITCH[Core Switch]\n        SWITCH --&gt; SERVER[PRS Server&lt;br/&gt;192.168.0.100]\n    end\n\n    subgraph \"Management Network (192.168.0.201/24)\"\n        SWITCH --&gt; ADMIN[Admin Workstation&lt;br/&gt;192.168.0.201]\n        SWITCH --&gt; BACKUP[Backup Server&lt;br/&gt;192.168.0.202]\n    end\n\n    subgraph \"Client Network (192.168.0.0/20)\"\n        SWITCH --&gt; CLIENTS[Client Workstations&lt;br/&gt;192.168.1.0/24]\n    end\n\n    style SERVER fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style ADMIN fill:#e8f5e8,stroke:#4caf50,stroke-width:2px,stroke:#4caf50,stroke-width:2px\n    style CLIENTS fill:#fff3e0,stroke:#ff9800,stroke-width:2px,stroke:#ff9800,stroke-width:2px\n    style FIREWALL fill:#ffebee,stroke:#f44336,stroke-width:2px,stroke:#f44336,stroke-width:2px\n    style SWITCH fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px,stroke:#9c27b0,stroke-width:2px</code></pre>"},{"location":"hardware/requirements/#bandwidth-requirements","title":"Bandwidth Requirements","text":"<ul> <li>Internal Traffic: 100 Mbps per 10 concurrent users</li> <li>Database Replication: 50 Mbps sustained</li> <li>Backup Traffic: 200 Mbps during backup windows</li> <li>Monitoring Traffic: 10 Mbps continuous</li> </ul>"},{"location":"hardware/requirements/#power-and-cooling","title":"Power and Cooling","text":""},{"location":"hardware/requirements/#power-requirements","title":"Power Requirements","text":"<p>Server Power Consumption: - Idle: 150-200W - Normal Load: 300-400W - Peak Load: 500-600W - UPS Capacity: 1000W minimum (30 minutes runtime)</p> <p>UPS Specifications: - Type: Online/Double-conversion UPS - Capacity: 1000VA minimum (1500VA recommended) - Runtime: 30 minutes at full load - Features: Network monitoring, automatic shutdown - Examples: APC Smart-UPS, Eaton 5P</p>"},{"location":"hardware/requirements/#cooling-requirements","title":"Cooling Requirements","text":"<ul> <li>Operating Temperature: 18-27\u00b0C (64-80\u00b0F)</li> <li>Humidity: 20-80% RH (non-condensing)</li> <li>Airflow: Front-to-back cooling</li> <li>Redundancy: Redundant cooling fans</li> </ul>"},{"location":"hardware/requirements/#raid-configuration","title":"RAID Configuration","text":""},{"location":"hardware/requirements/#ssd-raid-configuration-raid1","title":"SSD RAID Configuration (RAID1)","text":"<pre><code># Create RAID1 for SSD drives\nsudo mdadm --create /dev/md0 --level=1 --raid-devices=2 /dev/sda /dev/sdb\n\n# Format and mount\nsudo mkfs.ext4 /dev/md0\nsudo mkdir -p /mnt/ssd\nsudo mount /dev/md0 /mnt/ssd\n\n# Add to fstab\necho \"/dev/md0 /mnt/ssd ext4 defaults,noatime 0 2\" | sudo tee -a /etc/fstab\n</code></pre>"},{"location":"hardware/requirements/#hdd-raid-configuration-raid5","title":"HDD RAID Configuration (RAID5)","text":"<pre><code># Create RAID5 for HDD drives (minimum 3 drives)\nsudo mdadm --create /dev/md1 --level=5 --raid-devices=3 /dev/sdc /dev/sdd /dev/sde\n\n# Format and mount\nsudo mkfs.ext4 /dev/md1\nsudo mkdir -p /mnt/hdd\nsudo mount /dev/md1 /mnt/hdd\n\n# Add to fstab\necho \"/dev/md1 /mnt/hdd ext4 defaults,noatime 0 2\" | sudo tee -a /etc/fstab\n</code></pre>"},{"location":"hardware/requirements/#hardware-validation","title":"Hardware Validation","text":""},{"location":"hardware/requirements/#pre-installation-testing","title":"Pre-Installation Testing","text":""},{"location":"hardware/requirements/#cpu-performance-test","title":"CPU Performance Test","text":"<pre><code># Install stress testing tools\nsudo apt install stress-ng sysbench\n\n# CPU stress test (run for 10 minutes)\nstress-ng --cpu $(nproc) --timeout 600s --metrics-brief\n\n# CPU benchmark\nsysbench cpu --cpu-max-prime=20000 --threads=$(nproc) run\n</code></pre>"},{"location":"hardware/requirements/#memory-testing","title":"Memory Testing","text":"<pre><code># Memory stress test\nstress-ng --vm 4 --vm-bytes 75% --timeout 300s --metrics-brief\n\n# Memory benchmark\nsysbench memory --memory-total-size=10G --threads=4 run\n</code></pre>"},{"location":"hardware/requirements/#storage-performance-testing","title":"Storage Performance Testing","text":"<pre><code># Install fio for storage testing\nsudo apt install fio\n\n# SSD performance test\nsudo fio --name=ssd-test --filename=/mnt/ssd/test --size=10G --rw=randwrite --bs=4k --numjobs=4 --time_based --runtime=300\n\n# HDD performance test\nsudo fio --name=hdd-test --filename=/mnt/hdd/test --size=10G --rw=randwrite --bs=64k --numjobs=2 --time_based --runtime=300\n</code></pre>"},{"location":"hardware/requirements/#network-performance-testing","title":"Network Performance Testing","text":"<pre><code># Install network testing tools\nsudo apt install iperf3 netperf\n\n# Network throughput test (requires iperf3 server)\niperf3 -c target-server -t 60 -P 4\n\n# Network latency test\nping -c 100 target-server\n</code></pre>"},{"location":"hardware/requirements/#raid-health-monitoring","title":"RAID Health Monitoring","text":""},{"location":"hardware/requirements/#raid-status-check","title":"RAID Status Check","text":"<pre><code># Check RAID status\ncat /proc/mdstat\n\n# Detailed RAID information\nsudo mdadm --detail /dev/md0\nsudo mdadm --detail /dev/md1\n\n# Monitor RAID health\nsudo mdadm --monitor --scan --daemonise\n</code></pre>"},{"location":"hardware/requirements/#disk-health-monitoring","title":"Disk Health Monitoring","text":"<pre><code># Install smartmontools\nsudo apt install smartmontools\n\n# Check disk health\nsudo smartctl -a /dev/sda\nsudo smartctl -a /dev/sdb\nsudo smartctl -a /dev/sdc\n\n# Enable automatic monitoring\nsudo systemctl enable smartd\nsudo systemctl start smartd\n</code></pre>"},{"location":"hardware/requirements/#hardware-procurement-guide","title":"Hardware Procurement Guide","text":""},{"location":"hardware/requirements/#recommended-server-configurations","title":"Recommended Server Configurations","text":""},{"location":"hardware/requirements/#budget-configuration-3000","title":"Budget Configuration (~$3,000)","text":"<ul> <li>Server: Dell PowerEdge T340 or HP ProLiant ML110</li> <li>CPU: Intel Xeon E-2224 (4 cores, 3.4 GHz)</li> <li>RAM: 32 GB DDR4 ECC</li> <li>SSD: 2x 480 GB Enterprise SSD (RAID1)</li> <li>HDD: 3x 2 TB Enterprise HDD (RAID5)</li> <li>UPS: APC Smart-UPS 1000VA</li> </ul>"},{"location":"hardware/requirements/#recommended-configuration-5000","title":"Recommended Configuration (~$5,000)","text":"<ul> <li>Server: Dell PowerEdge T440 or HP ProLiant ML350</li> <li>CPU: Intel Xeon Silver 4214 (12 cores, 2.2 GHz)</li> <li>RAM: 64 GB DDR4 ECC</li> <li>SSD: 2x 960 GB Enterprise SSD (RAID1)</li> <li>HDD: 4x 4 TB Enterprise HDD (RAID5)</li> <li>UPS: APC Smart-UPS 1500VA</li> </ul>"},{"location":"hardware/requirements/#enterprise-configuration-8000","title":"Enterprise Configuration (~$8,000)","text":"<ul> <li>Server: Dell PowerEdge R440 or HP ProLiant DL380</li> <li>CPU: Intel Xeon Gold 6248R (24 cores, 3.0 GHz)</li> <li>RAM: 128 GB DDR4 ECC</li> <li>SSD: 4x 1.92 TB Enterprise SSD (RAID10)</li> <li>HDD: 6x 8 TB Enterprise HDD (RAID6)</li> <li>UPS: APC Smart-UPS 3000VA</li> </ul>"},{"location":"hardware/requirements/#vendor-recommendations","title":"Vendor Recommendations","text":""},{"location":"hardware/requirements/#server-vendors","title":"Server Vendors","text":"<ul> <li>Dell: PowerEdge T-series (tower) or R-series (rack)</li> <li>HP: ProLiant ML-series (tower) or DL-series (rack)</li> <li>Lenovo: ThinkSystem ST-series (tower) or SR-series (rack)</li> <li>Supermicro: SuperServer series (custom configurations)</li> </ul>"},{"location":"hardware/requirements/#storage-vendors","title":"Storage Vendors","text":"<ul> <li>SSD: Samsung, Intel, Micron (enterprise grade)</li> <li>HDD: Western Digital Red Pro, Seagate IronWolf Pro</li> <li>RAID Controllers: LSI MegaRAID, Adaptec, Dell PERC</li> </ul>"},{"location":"hardware/requirements/#ups-vendors","title":"UPS Vendors","text":"<ul> <li>APC: Smart-UPS series</li> <li>Eaton: 5P series</li> <li>CyberPower: Professional series</li> <li>Tripp Lite: SmartOnline series</li> </ul> <p>Hardware Ready</p> <p>Once your hardware meets these specifications, you're ready to proceed with the Prerequisites and Installation.</p> <p>Performance Scaling</p> <p>The system can scale beyond these specifications. For larger deployments, consider clustering multiple servers or upgrading to enterprise-grade hardware.</p> <p>Hardware Compatibility</p> <p>Always verify hardware compatibility with your chosen operating system and ensure all components are on the vendor's hardware compatibility list.</p>"},{"location":"hardware/storage/","title":"Storage Configuration","text":""},{"location":"hardware/storage/#overview","title":"Overview","text":"<p>The PRS system implements a sophisticated dual-storage architecture that automatically manages data lifecycle between high-performance SSD and high-capacity HDD storage tiers.</p>"},{"location":"hardware/storage/#dual-storage-architecture","title":"Dual Storage Architecture","text":""},{"location":"hardware/storage/#tiers","title":"Tiers","text":"<pre><code>graph TB\n    subgraph \"Application Data Flow\"\n        APP[PRS Application] --&gt; WRITE[New Data]\n        APP --&gt; READ[Data Queries]\n    end\n\n    subgraph \"SSD Tier (470GB RAID1)\"\n        WRITE --&gt; SSD_HOT[Hot Data&lt;br/&gt;0-30 days]\n        SSD_HOT --&gt; SSD_COMP[Compressed Data&lt;br/&gt;7-30 days]\n    end\n\n    subgraph \"HDD Tier (2.4TB RAID5)\"\n        SSD_COMP --&gt; HDD_COLD[Cold Data&lt;br/&gt;30+ days]\n        HDD_COLD --&gt; HDD_ARCH[Archived Data&lt;br/&gt;1+ years]\n    end\n\n    subgraph \"Data Movement\"\n        AUTO1[Auto Compress&lt;br/&gt;7 days] --&gt; SSD_COMP\n        AUTO2[Auto Move&lt;br/&gt;30 days] --&gt; HDD_COLD\n    end\n\n    READ --&gt; SSD_HOT\n    READ --&gt; SSD_COMP\n    READ --&gt; HDD_COLD\n    READ --&gt; HDD_ARCH\n\n    style SSD_HOT fill:#e3f2fd,stroke:#2196f3,stroke-width:2px\n    style SSD_COMP fill:#e8f5e8,stroke:#4caf50,stroke-width:2px\n    style HDD_COLD fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px\n    style HDD_ARCH fill:#fce4ec,stroke:#e91e63,stroke-width:2px\n    style APP fill:#fff3e0,stroke:#ff9800,stroke-width:2px</code></pre>"},{"location":"hardware/storage/#characteristics","title":"Characteristics","text":"Storage Tier Technology Capacity Access Time Use Case SSD Hot NVMe/SATA SSD 470GB &lt;50ms Active operations, recent data SSD Compressed NVMe/SATA SSD 60-80% savings &lt;100ms Recent queries, cached data HDD Cold SATA HDD RAID5 2.4TB+ &lt;2s Historical queries, reports HDD Archive SATA HDD RAID5 Unlimited &lt;5s Long-term storage, compliance"},{"location":"hardware/storage/#ssd-storage-configuration-470gb-raid1","title":"SSD Storage Configuration (470GB RAID1)","text":""},{"location":"hardware/storage/#strategy","title":"Strategy","text":"Component Allocation Purpose Performance PostgreSQL Hot Data 200 GB Recent database records (0-30 days) &lt;50ms queries Redis Persistence 50 GB Cache snapshots and AOF files &lt;10ms access Application Uploads 100 GB User files and attachments &lt;100ms retrieval System Logs 50 GB Active application and system logs Real-time logging Nginx Cache 20 GB Web server static file cache &lt;5ms serving Monitoring Data 30 GB Prometheus metrics and Grafana data &lt;50ms dashboards System Reserve 20 GB Emergency space and temporary files Buffer space"},{"location":"hardware/storage/#mount-configuration","title":"Mount Configuration","text":"<pre><code># Optimal SSD mount options for performance\n/dev/md0 /mnt/ssd ext4 defaults,noatime,discard,barrier=0 0 2\n\n# Mount options explained:\n# noatime    - Don't update access times (performance boost)\n# discard    - Enable TRIM support for SSD longevity\n# barrier=0  - Disable write barriers (safe with UPS)\n</code></pre>"},{"location":"hardware/storage/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Set I/O scheduler for SSD\necho noop | sudo tee /sys/block/sda/queue/scheduler\n\n# Optimize read-ahead for SSD\necho 256 | sudo tee /sys/block/sda/queue/read_ahead_kb\n\n# Set optimal queue depth\necho 32 | sudo tee /sys/block/sda/queue/nr_requests\n</code></pre>"},{"location":"hardware/storage/#hdd-storage-configuration-24tb-raid5","title":"HDD Storage Configuration (2.4TB RAID5)","text":""},{"location":"hardware/storage/#strategy_1","title":"Strategy","text":"Component Allocation Purpose Retention PostgreSQL Cold Data 1,000 GB Historical database records (30+ days) Permanent Backup Archives 1,000 GB Database and application backups 1+ years Log Archives 200 GB Archived application and system logs 2+ years NAS Sync Staging 100 GB Network storage synchronization Temporary Future Growth 100+ TB Unlimited expansion capacity As needed"},{"location":"hardware/storage/#raid5-configuration","title":"RAID5 Configuration","text":"<pre><code># Create RAID5 array (example with 4 drives)\nsudo mdadm --create /dev/md1 --level=5 --raid-devices=4 \\\n  --chunk=256 /dev/sdb /dev/sdc /dev/sdd /dev/sde\n\n# Optimize RAID5 for large files\necho 8192 | sudo tee /sys/block/md1/md/stripe_cache_size\n\n# Set read-ahead for sequential access\necho 8192 | sudo tee /sys/block/md1/queue/read_ahead_kb\n</code></pre>"},{"location":"hardware/storage/#performance-optimization_1","title":"Performance Optimization","text":"<pre><code># Set I/O scheduler for HDD\necho deadline | sudo tee /sys/block/md1/queue/scheduler\n\n# Optimize for large sequential writes\necho 2048 | sudo tee /sys/block/md1/queue/nr_requests\n\n# Enable write-back caching (with UPS)\nhdparm -W1 /dev/md1\n</code></pre>"},{"location":"hardware/storage/#automatic-data-lifecycle","title":"Automatic Data Lifecycle","text":""},{"location":"hardware/storage/#movement-timeline","title":"Movement Timeline","text":"<pre><code>timeline\n    title Automatic Data Lifecycle Management\n\n    Day 0     : New Data Created\n              : Stored on SSD\n              : Uncompressed\n              : &lt;50ms access\n\n    Day 7     : High-Volume Data\n              : Auto-compressed\n              : 60-80% space savings\n              : &lt;100ms access\n\n    Day 14    : History Data\n              : Auto-compressed\n              : Optimized storage\n              : &lt;100ms access\n\n    Day 30    : All Data\n              : Moved to HDD\n              : Compressed storage\n              : &lt;2s access\n\n    Long-term : Permanent Storage\n              : Zero deletion\n              : Compliance ready\n              : &lt;5s access</code></pre>"},{"location":"hardware/storage/#policies","title":"Policies","text":"<pre><code>-- Compression policies by data type\nSELECT add_compression_policy('notifications', INTERVAL '7 days');\nSELECT add_compression_policy('audit_logs', INTERVAL '7 days');\nSELECT add_compression_policy('requisitions', INTERVAL '30 days');\n\n-- Data movement policies\nSELECT add_move_chunk_policy('notifications', INTERVAL '30 days', 'hdd_cold');\nSELECT add_move_chunk_policy('audit_logs', INTERVAL '30 days', 'hdd_cold');\nSELECT add_move_chunk_policy('requisitions', INTERVAL '30 days', 'hdd_cold');\n</code></pre>"},{"location":"hardware/storage/#storage-monitoring","title":"Storage Monitoring","text":""},{"location":"hardware/storage/#monitoring","title":"Monitoring","text":"<pre><code># Check storage usage\ndf -h /mnt/ssd /mnt/hdd\n\n# Check RAID status\ncat /proc/mdstat\n\n# Check SSD health\nsudo smartctl -a /dev/sda\n\n# Check HDD health\nsudo smartctl -a /dev/sdb\n</code></pre>"},{"location":"hardware/storage/#monitoring_1","title":"Monitoring","text":"<pre><code># Monitor I/O performance\niostat -x 1\n\n# Monitor storage latency\niotop -a\n\n# Check filesystem performance\niozone -a -g 4G\n</code></pre>"},{"location":"hardware/storage/#thresholds","title":"Thresholds","text":"Storage Warning Critical Action SSD Usage 80% (376GB) 90% (423GB) Archive old data HDD Usage 70% (1.68TB) 85% (2.04TB) Expand storage SSD IOPS 80% capacity 95% capacity Optimize queries HDD Throughput 80% bandwidth 95% bandwidth Balance load"},{"location":"hardware/storage/#storage-maintenance","title":"Storage Maintenance","text":""},{"location":"hardware/storage/#tasks","title":"Tasks","text":"<pre><code># Check storage health\nsudo smartctl -H /dev/sda /dev/sdb /dev/sdc /dev/sdd\n\n# Monitor RAID status\ncat /proc/mdstat\n\n# Check filesystem errors\ndmesg | grep -i error\n</code></pre>"},{"location":"hardware/storage/#tasks_1","title":"Tasks","text":"<pre><code># Run filesystem check (when unmounted)\nsudo fsck -f /dev/md0\nsudo fsck -f /dev/md1\n\n# Defragment if needed (ext4)\nsudo e4defrag /mnt/ssd\nsudo e4defrag /mnt/hdd\n\n# Update SMART data\nsudo smartctl -t short /dev/sda\n</code></pre>"},{"location":"hardware/storage/#tasks_2","title":"Tasks","text":"<pre><code># Full SMART test\nsudo smartctl -t long /dev/sda /dev/sdb /dev/sdc /dev/sdd\n\n# RAID consistency check\necho check | sudo tee /sys/block/md1/md/sync_action\n\n# Storage performance benchmark\nsudo fio --name=monthly-test --filename=/mnt/ssd/test --size=10G \\\n  --rw=randwrite --bs=4k --numjobs=4 --time_based --runtime=300\n</code></pre>"},{"location":"hardware/storage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"hardware/storage/#issues","title":"Issues","text":""},{"location":"hardware/storage/#performance-degradation","title":"Performance Degradation","text":"<pre><code># Check SSD wear level\nsudo smartctl -A /dev/sda | grep Wear_Leveling_Count\n\n# Enable TRIM if disabled\nsudo fstrim -v /mnt/ssd\n\n# Check for bad blocks\nsudo badblocks -v /dev/sda\n</code></pre>"},{"location":"hardware/storage/#raid-issues","title":"RAID Issues","text":"<pre><code># Check RAID health\nsudo mdadm --detail /dev/md1\n\n# Rebuild failed drive\nsudo mdadm --manage /dev/md1 --add /dev/sdf\n\n# Monitor rebuild progress\nwatch cat /proc/mdstat\n</code></pre>"},{"location":"hardware/storage/#full-issues","title":"Full Issues","text":"<pre><code># Find large files\nsudo find /mnt/ssd -type f -size +100M -exec ls -lh {} \\;\n\n# Clean temporary files\nsudo find /mnt/ssd -name \"*.tmp\" -delete\n\n# Compress old logs\nsudo find /mnt/ssd/logs -name \"*.log\" -mtime +7 -exec gzip {} \\;\n</code></pre>"},{"location":"hardware/storage/#capacity-planning","title":"Capacity Planning","text":""},{"location":"hardware/storage/#projections","title":"Projections","text":"Time Period SSD Usage HDD Usage Action Required Current 60% (282GB) 15% (360GB) Monitor 6 Months 75% (352GB) 25% (600GB) Plan SSD expansion 1 Year 85% (399GB) 40% (960GB) Expand SSD 2 Years 95% (446GB) 60% (1.44TB) Add SSD tier"},{"location":"hardware/storage/#options","title":"Options","text":""},{"location":"hardware/storage/#expansion","title":"Expansion","text":"<pre><code># Option 1: Add SSD to existing RAID1\nsudo mdadm --grow /dev/md0 --raid-devices=4 --add /dev/sdf /dev/sdg\n\n# Option 2: Create new SSD tier\nsudo mdadm --create /dev/md2 --level=1 --raid-devices=2 /dev/sdf /dev/sdg\n</code></pre>"},{"location":"hardware/storage/#expansion_1","title":"Expansion","text":"<pre><code># Add drives to RAID5 array\nsudo mdadm --grow /dev/md1 --raid-devices=6 --add /dev/sdf /dev/sdg\n\n# Monitor reshape progress\nwatch cat /proc/mdstat\n</code></pre> <p>Automatic Management</p> <p>The storage system is designed to be largely self-managing. TimescaleDB automatically handles data movement between tiers based on age and access patterns.</p> <p>Backup Critical</p> <p>Always maintain current backups before performing any storage maintenance operations.</p>"},{"location":"installation/database/","title":"Database Setup","text":""},{"location":"installation/database/#overview","title":"Overview","text":"<p>This guide covers the complete setup of TimescaleDB with dual storage architecture for the PRS on-premises deployment.</p>"},{"location":"installation/database/#timescaledb-installation","title":"TimescaleDB Installation","text":""},{"location":"installation/database/#container-configuration","title":"Container Configuration","text":"<p>The database runs in a Docker container with optimized settings for production use:</p> <pre><code>postgres:\n  image: timescale/timescaledb:latest-pg15\n  container_name: prs-onprem-postgres-timescale\n  restart: unless-stopped\n  environment:\n    - POSTGRES_DB=${POSTGRES_DB}\n    - POSTGRES_USER=${POSTGRES_USER}\n    - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n    - TIMESCALEDB_TELEMETRY=off\n  command: &gt;\n    postgres\n    -c max_connections=150\n    -c shared_buffers=2GB\n    -c effective_cache_size=4GB\n    -c work_mem=32MB\n    -c maintenance_work_mem=512MB\n    -c random_page_cost=1.1\n    -c effective_io_concurrency=200\n    -c shared_preload_libraries=timescaledb\n  volumes:\n    - database_data:/var/lib/postgresql/data\n    - /mnt/ssd/postgresql-hot:/mnt/ssd/postgresql-hot\n    - /mnt/hdd/postgresql-cold:/mnt/hdd/postgresql-cold\n    - /mnt/hdd/postgres-backups:/var/lib/postgresql/backups\n</code></pre>"},{"location":"installation/database/#database-initialization","title":"Database Initialization","text":""},{"location":"installation/database/#connect-to-database","title":"Connect to Database","text":"<pre><code># Wait for PostgreSQL to be ready\ndocker exec prs-onprem-postgres-timescale pg_isready -U prs_admin\n\n# Connect to database\ndocker exec -it prs-onprem-postgres-timescale psql -U prs_admin -d prs_production\n</code></pre>"},{"location":"installation/database/#enable-timescaledb-extension","title":"Enable TimescaleDB Extension","text":"<pre><code>-- Enable TimescaleDB extension\nCREATE EXTENSION IF NOT EXISTS timescaledb;\n\n-- Verify installation\nSELECT * FROM timescaledb_information.license;\n\n-- Configure telemetry (disabled for on-premises)\nALTER SYSTEM SET timescaledb.telemetry = 'off';\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"installation/database/#dual-storage-configuration","title":"Dual Storage Configuration","text":""},{"location":"installation/database/#create-tablespaces","title":"Create Tablespaces","text":"<pre><code>-- Create tablespaces for tiered storage\nCREATE TABLESPACE ssd_hot LOCATION '/mnt/ssd/postgresql-hot';\nCREATE TABLESPACE hdd_cold LOCATION '/mnt/hdd/postgresql-cold';\n\n-- Set default tablespace for new chunks\nALTER DATABASE prs_production SET default_tablespace = ssd_hot;\n\n-- Verify tablespaces\nSELECT spcname, pg_tablespace_location(oid) FROM pg_tablespace;\n</code></pre>"},{"location":"installation/database/#configure-storage-tiers","title":"Configure Storage Tiers","text":"<pre><code>-- Configure TimescaleDB for dual storage\nALTER SYSTEM SET temp_tablespaces = 'ssd_hot';\nALTER SYSTEM SET default_tablespace = 'ssd_hot';\n\n-- Reload configuration\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"installation/database/#database-schema-setup","title":"Database Schema Setup","text":""},{"location":"installation/database/#run-migrations","title":"Run Migrations","text":"<pre><code># Access backend container\ndocker exec -it prs-onprem-backend bash\n\n# Install dependencies (if needed)\nnpm install\n\n# Run database migrations\nnpm run migrate\n\n# Verify migration status\nnpm run migrate:status\n</code></pre>"},{"location":"installation/database/#setup-hypertables","title":"Setup Hypertables","text":"<pre><code># Setup TimescaleDB hypertables and compression\nnpm run setup:timescaledb\n\n# Verify hypertables\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT * FROM timescaledb_information.hypertables;\n\"\n</code></pre>"},{"location":"installation/database/#performance-optimization","title":"Performance Optimization","text":""},{"location":"installation/database/#memory-configuration","title":"Memory Configuration","text":"<pre><code>-- Optimize memory settings for 16GB RAM system\nALTER SYSTEM SET shared_buffers = '2GB';\nALTER SYSTEM SET effective_cache_size = '4GB';\nALTER SYSTEM SET work_mem = '32MB';\nALTER SYSTEM SET maintenance_work_mem = '512MB';\n\n-- WAL settings\nALTER SYSTEM SET wal_buffers = '32MB';\nALTER SYSTEM SET checkpoint_completion_target = 0.9;\nALTER SYSTEM SET max_wal_size = '2GB';\nALTER SYSTEM SET min_wal_size = '512MB';\n\n-- Reload configuration\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"installation/database/#connection-settings","title":"Connection Settings","text":"<pre><code>-- Connection and worker settings\nALTER SYSTEM SET max_connections = 150;\nALTER SYSTEM SET max_worker_processes = 32;\nALTER SYSTEM SET max_parallel_workers = 16;\nALTER SYSTEM SET max_parallel_workers_per_gather = 4;\n\n-- TimescaleDB specific settings\nALTER SYSTEM SET timescaledb.max_background_workers = 16;\n\n-- Reload configuration\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"installation/database/#storage-optimization","title":"Storage Optimization","text":"<pre><code>-- SSD optimization\nALTER SYSTEM SET random_page_cost = 1.1;\nALTER SYSTEM SET effective_io_concurrency = 200;\nALTER SYSTEM SET seq_page_cost = 1.0;\n\n-- Checkpoint optimization\nALTER SYSTEM SET checkpoint_timeout = '15min';\nALTER SYSTEM SET checkpoint_completion_target = 0.9;\n\n-- Reload configuration\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"installation/database/#data-lifecycle-policies","title":"Data Lifecycle Policies","text":""},{"location":"installation/database/#compression-policies","title":"Compression Policies","text":"<pre><code>-- High-volume tables - compress after 7 days\nSELECT add_compression_policy('notifications', INTERVAL '7 days');\nSELECT add_compression_policy('audit_logs', INTERVAL '7 days');\nSELECT add_compression_policy('histories', INTERVAL '7 days');\nSELECT add_compression_policy('comments', INTERVAL '7 days');\n\n-- History tables - compress after 14 days\nSELECT add_compression_policy('requisition_canvass_histories', INTERVAL '14 days');\nSELECT add_compression_policy('requisition_item_histories', INTERVAL '14 days');\nSELECT add_compression_policy('requisition_order_histories', INTERVAL '14 days');\n\n-- Business tables - compress after 30 days\nSELECT add_compression_policy('requisitions', INTERVAL '30 days');\nSELECT add_compression_policy('purchase_orders', INTERVAL '30 days');\nSELECT add_compression_policy('delivery_receipts', INTERVAL '30 days');\n</code></pre>"},{"location":"installation/database/#data-movement-policies","title":"Data Movement Policies","text":"<pre><code>-- Move chunks older than 30 days to HDD storage\nSELECT add_move_chunk_policy('notifications', INTERVAL '30 days', 'hdd_cold');\nSELECT add_move_chunk_policy('audit_logs', INTERVAL '30 days', 'hdd_cold');\nSELECT add_move_chunk_policy('requisitions', INTERVAL '30 days', 'hdd_cold');\nSELECT add_move_chunk_policy('purchase_orders', INTERVAL '30 days', 'hdd_cold');\n\n-- Move history tables after 14 days (faster archival)\nSELECT add_move_chunk_policy('requisition_canvass_histories', INTERVAL '14 days', 'hdd_cold');\nSELECT add_move_chunk_policy('requisition_item_histories', INTERVAL '14 days', 'hdd_cold');\n</code></pre>"},{"location":"installation/database/#database-validation","title":"Database Validation","text":""},{"location":"installation/database/#verify-installation","title":"Verify Installation","text":"<pre><code>-- Check TimescaleDB version\nSELECT * FROM timescaledb_information.license;\n\n-- Check hypertables\nSELECT hypertable_name, num_chunks FROM timescaledb_information.hypertables;\n\n-- Check compression policies\nSELECT * FROM timescaledb_information.compression_settings;\n\n-- Check data movement policies\nSELECT * FROM timescaledb_information.data_node_move_policies;\n</code></pre>"},{"location":"installation/database/#performance-testing","title":"Performance Testing","text":"<pre><code>-- Test query performance\nEXPLAIN ANALYZE SELECT COUNT(*) FROM notifications \nWHERE created_at &gt;= NOW() - INTERVAL '30 days';\n\n-- Check chunk distribution\nSELECT \n    hypertable_name,\n    tablespace_name,\n    COUNT(*) as chunk_count,\n    pg_size_pretty(SUM(chunk_size)) as total_size\nFROM timescaledb_information.chunks\nGROUP BY hypertable_name, tablespace_name;\n</code></pre>"},{"location":"installation/database/#connection-testing","title":"Connection Testing","text":"<pre><code># Test database connectivity from backend\ndocker exec prs-onprem-backend npm run db:test\n\n# Test connection pool\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT count(*) as connections, state \nFROM pg_stat_activity \nGROUP BY state;\n\"\n</code></pre>"},{"location":"installation/database/#backup-configuration","title":"Backup Configuration","text":""},{"location":"installation/database/#wal-archiving-setup","title":"WAL Archiving Setup","text":"<pre><code>-- Enable WAL archiving for point-in-time recovery\nALTER SYSTEM SET wal_level = 'replica';\nALTER SYSTEM SET archive_mode = 'on';\nALTER SYSTEM SET archive_command = 'cp %p /var/lib/postgresql/wal-archive/%f';\nALTER SYSTEM SET max_wal_senders = 3;\nALTER SYSTEM SET wal_keep_segments = 64;\n\n-- Reload configuration\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"installation/database/#backup-user-setup","title":"Backup User Setup","text":"<pre><code>-- Create backup user with minimal privileges\nCREATE ROLE backup_user WITH LOGIN PASSWORD 'secure_backup_password';\nGRANT CONNECT ON DATABASE prs_production TO backup_user;\nGRANT USAGE ON SCHEMA public TO backup_user;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO backup_user;\n</code></pre>"},{"location":"installation/database/#security-configuration","title":"Security Configuration","text":""},{"location":"installation/database/#user-management","title":"User Management","text":"<pre><code>-- Create application-specific roles\nCREATE ROLE prs_app_read;\nCREATE ROLE prs_app_write;\nCREATE ROLE prs_app_admin;\n\n-- Grant appropriate permissions\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO prs_app_read;\nGRANT SELECT, INSERT, UPDATE ON ALL TABLES IN SCHEMA public TO prs_app_write;\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO prs_app_admin;\n\n-- Create application user\nCREATE USER prs_application WITH PASSWORD 'secure_app_password';\nGRANT prs_app_write TO prs_application;\n</code></pre>"},{"location":"installation/database/#connection-security","title":"Connection Security","text":"<pre><code>-- Configure connection limits\nALTER ROLE prs_admin CONNECTION LIMIT 10;\nALTER ROLE prs_application CONNECTION LIMIT 50;\nALTER ROLE backup_user CONNECTION LIMIT 2;\n\n-- Set password policies\nALTER ROLE prs_admin VALID UNTIL 'infinity';\nALTER ROLE prs_application VALID UNTIL 'infinity';\n</code></pre>"},{"location":"installation/database/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/database/#common-issues","title":"Common Issues","text":""},{"location":"installation/database/#database-wont-start","title":"Database Won't Start","text":"<pre><code># Check container logs\ndocker logs prs-onprem-postgres-timescale\n\n# Check storage permissions\nls -la /mnt/ssd/postgresql-hot\nls -la /mnt/hdd/postgresql-cold\n\n# Fix permissions if needed\nsudo chown -R 999:999 /mnt/ssd/postgresql-hot /mnt/hdd/postgresql-cold\n</code></pre>"},{"location":"installation/database/#connection-issues","title":"Connection Issues","text":"<pre><code># Test connectivity\ndocker exec prs-onprem-postgres-timescale pg_isready -U prs_admin\n\n# Check connection count\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT count(*) FROM pg_stat_activity;\n\"\n\n# Kill idle connections if needed\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT pg_terminate_backend(pid) \nFROM pg_stat_activity \nWHERE state = 'idle' \nAND query_start &lt; NOW() - INTERVAL '1 hour';\n\"\n</code></pre>"},{"location":"installation/database/#performance-issues","title":"Performance Issues","text":"<pre><code>-- Check slow queries\nSELECT query, calls, total_time, mean_time \nFROM pg_stat_statements \nORDER BY total_time DESC \nLIMIT 10;\n\n-- Check table bloat\nSELECT \n    schemaname,\n    tablename,\n    n_tup_ins,\n    n_tup_upd,\n    n_tup_del,\n    n_dead_tup\nFROM pg_stat_user_tables\nWHERE n_dead_tup &gt; 1000\nORDER BY n_dead_tup DESC;\n</code></pre> <p>Database Ready</p> <p>Once all setup steps are completed, your TimescaleDB database is ready for production use with dual storage architecture and zero-deletion compliance.</p> <p>Next Steps</p> <p>Proceed to SSL Configuration to secure your deployment with HTTPS.</p>"},{"location":"installation/docker/","title":"Docker Configuration","text":""},{"location":"installation/docker/#overview","title":"Overview","text":"<p>This guide covers the Docker configuration for the PRS on-premises deployment, including container orchestration, networking, and resource management.</p>"},{"location":"installation/docker/#container-architecture","title":"Container Architecture","text":"<pre><code>graph TB\n    subgraph \"Docker Host (192.168.0.100)\"\n        subgraph \"Application Containers\"\n            NGINX[Nginx Proxy&lt;br/&gt;172.20.0.5]\n            FRONTEND[Frontend&lt;br/&gt;172.20.0.15]\n            BACKEND[Backend API&lt;br/&gt;172.20.0.20]\n            WORKER[Redis Worker&lt;br/&gt;172.20.0.40]\n        end\n\n        subgraph \"Data Containers\"\n            POSTGRES[PostgreSQL&lt;br/&gt;172.20.0.30]\n            REDIS[Redis Cache&lt;br/&gt;172.20.0.35]\n        end\n\n        subgraph \"Management Containers\"\n            ADMINER[Adminer&lt;br/&gt;172.20.0.10]\n            GRAFANA[Grafana&lt;br/&gt;172.20.0.25]\n            PROMETHEUS[Prometheus&lt;br/&gt;172.20.0.45]\n            PORTAINER[Portainer&lt;br/&gt;172.20.0.55]\n        end\n    end\n\n    subgraph \"Storage Volumes\"\n        SSD[SSD Storage&lt;br/&gt;/mnt/ssd]\n        HDD[HDD Storage&lt;br/&gt;/mnt/hdd]\n    end\n\n    NGINX --&gt; FRONTEND\n    NGINX --&gt; BACKEND\n    BACKEND --&gt; POSTGRES\n    BACKEND --&gt; REDIS\n    WORKER --&gt; POSTGRES\n    WORKER --&gt; REDIS\n\n    POSTGRES --&gt; SSD\n    POSTGRES --&gt; HDD\n    REDIS --&gt; SSD\n\n    style NGINX fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style POSTGRES fill:#e8f5e8,stroke:#4caf50,stroke-width:2px\n    style SSD fill:#fff3e0,stroke:#ff9800,stroke-width:2px\n    style HDD fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px</code></pre>"},{"location":"installation/docker/#service-configuration","title":"Service Configuration","text":""},{"location":"installation/docker/#application-services","title":"Application Services","text":""},{"location":"installation/docker/#reverse-proxy","title":"Reverse Proxy","text":"<pre><code>nginx:\n  image: nginx:1.24-alpine\n  container_name: prs-onprem-nginx\n  restart: unless-stopped\n  ports:\n    - \"${SERVER_IP:-192.168.0.100}:80:80\"\n    - \"${SERVER_IP:-192.168.0.100}:443:443\"\n  volumes:\n    - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro\n    - ./nginx/sites-enabled:/etc/nginx/sites-enabled:ro\n    - ./ssl:/etc/nginx/ssl:ro\n    - uploads_data:/var/www/uploads:ro\n    - /mnt/ssd/nginx-cache:/var/cache/nginx/ssd\n  networks:\n    prs_onprem_network:\n      ipv4_address: 172.20.0.5\n  deploy:\n    resources:\n      limits:\n        memory: 256m\n        cpus: '0.5'\n</code></pre>"},{"location":"installation/docker/#api","title":"API","text":"<pre><code>backend:\n  image: prs-backend:latest\n  container_name: prs-onprem-backend\n  restart: unless-stopped\n  environment:\n    - NODE_ENV=production\n    - PORT=4000\n    - POSTGRES_HOST=postgres\n    - REDIS_HOST=redis\n    - NODEJS_MAX_OLD_SPACE_SIZE=2048\n  volumes:\n    - uploads_data:/usr/app/upload\n    - logs_data:/usr/app/logs\n    - /mnt/hdd/app-logs-archive:/usr/app/logs/archive\n  networks:\n    prs_onprem_network:\n      ipv4_address: 172.20.0.20\n  deploy:\n    resources:\n      limits:\n        memory: 4g\n        cpus: '2.0'\n</code></pre>"},{"location":"installation/docker/#application","title":"Application","text":"<pre><code>frontend:\n  image: prs-frontend:latest\n  container_name: prs-onprem-frontend\n  restart: unless-stopped\n  environment:\n    - NODE_ENV=production\n    - VITE_APP_API_URL=https://${DOMAIN}/api\n  networks:\n    prs_onprem_network:\n      ipv4_address: 172.20.0.15\n  deploy:\n    resources:\n      limits:\n        memory: 1g\n        cpus: '1.0'\n</code></pre>"},{"location":"installation/docker/#services","title":"Services","text":""},{"location":"installation/docker/#with-timescaledb","title":"with TimescaleDB","text":"<pre><code>postgres:\n  image: timescale/timescaledb:latest-pg15\n  container_name: prs-onprem-postgres-timescale\n  restart: unless-stopped\n  environment:\n    - POSTGRES_DB=${POSTGRES_DB}\n    - POSTGRES_USER=${POSTGRES_USER}\n    - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n    - TIMESCALEDB_TELEMETRY=off\n  command: &gt;\n    postgres\n    -c max_connections=150\n    -c shared_buffers=2GB\n    -c effective_cache_size=4GB\n    -c work_mem=32MB\n    -c maintenance_work_mem=512MB\n    -c random_page_cost=1.1\n    -c effective_io_concurrency=200\n    -c shared_preload_libraries=timescaledb\n  volumes:\n    - database_data:/var/lib/postgresql/data\n    - /mnt/ssd/postgresql-hot:/mnt/ssd/postgresql-hot\n    - /mnt/hdd/postgresql-cold:/mnt/hdd/postgresql-cold\n    - /mnt/hdd/postgres-backups:/var/lib/postgresql/backups\n  networks:\n    prs_onprem_network:\n      ipv4_address: 172.20.0.30\n  deploy:\n    resources:\n      limits:\n        memory: 6g\n        cpus: '3.0'\n</code></pre>"},{"location":"installation/docker/#cache","title":"Cache","text":"<pre><code>redis:\n  image: redis:7-alpine\n  container_name: prs-onprem-redis\n  restart: unless-stopped\n  command: &gt;\n    redis-server\n    --requirepass ${REDIS_PASSWORD}\n    --appendonly yes\n    --maxmemory 2g\n    --maxmemory-policy allkeys-lru\n  volumes:\n    - redis_data:/data\n    - /mnt/ssd/redis-persistence:/data/ssd\n    - /mnt/hdd/redis-backups:/data/backups\n  networks:\n    prs_onprem_network:\n      ipv4_address: 172.20.0.35\n  deploy:\n    resources:\n      limits:\n        memory: 2g\n        cpus: '1.0'\n</code></pre>"},{"location":"installation/docker/#network-configuration","title":"Network Configuration","text":""},{"location":"installation/docker/#bridge-network","title":"Bridge Network","text":"<pre><code>networks:\n  prs_onprem_network:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.0.0/24\n          gateway: 172.20.0.1\n    driver_opts:\n      com.docker.network.bridge.name: prs-onprem-br0\n      com.docker.network.bridge.enable_icc: \"true\"\n      com.docker.network.bridge.enable_ip_masquerade: \"true\"\n      com.docker.network.driver.mtu: 1500\n</code></pre>"},{"location":"installation/docker/#discovery","title":"Discovery","text":"<p>Services communicate using container names: - <code>postgres</code> - Database server - <code>redis</code> - Cache server - <code>backend</code> - API server - <code>frontend</code> - Web application - <code>nginx</code> - Reverse proxy</p>"},{"location":"installation/docker/#volume-configuration","title":"Volume Configuration","text":""},{"location":"installation/docker/#volumes","title":"Volumes","text":"<pre><code>volumes:\n  # Database volume (managed by Docker)\n  database_data:\n    driver: local\n\n  # Redis data (SSD for performance)\n  redis_data:\n    driver: local\n    driver_opts:\n      type: none\n      o: bind\n      device: /mnt/ssd/redis-data\n\n  # Upload files (SSD for fast access)\n  uploads_data:\n    driver: local\n    driver_opts:\n      type: none\n      o: bind\n      device: /mnt/ssd/uploads\n\n  # Application logs (SSD for active logs)\n  logs_data:\n    driver: local\n    driver_opts:\n      type: none\n      o: bind\n      device: /mnt/ssd/logs\n\n  # Nginx cache (SSD for performance)\n  nginx_cache:\n    driver: local\n    driver_opts:\n      type: none\n      o: bind\n      device: /mnt/ssd/nginx-cache\n</code></pre>"},{"location":"installation/docker/#mounts","title":"Mounts","text":"<p>Direct mounts for dual storage: - <code>/mnt/ssd/postgresql-hot</code> - Hot database data - <code>/mnt/hdd/postgresql-cold</code> - Cold database data - <code>/mnt/hdd/postgres-backups</code> - Database backups - <code>/mnt/hdd/app-logs-archive</code> - Archived logs</p>"},{"location":"installation/docker/#resource-management","title":"Resource Management","text":""},{"location":"installation/docker/#allocation","title":"Allocation","text":"Service Memory Limit CPU Limit Purpose PostgreSQL 6GB 3.0 cores Database operations Backend API 4GB 2.0 cores Application logic Redis 2GB 1.0 core Caching and sessions Frontend 1GB 1.0 core Static file serving Nginx 256MB 0.5 core Reverse proxy Monitoring 2GB 1.5 cores Metrics and dashboards"},{"location":"installation/docker/#checks","title":"Checks","text":"<pre><code># PostgreSQL health check\nhealthcheck:\n  test: [\"CMD-SHELL\", \"pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}\"]\n  interval: 30s\n  timeout: 10s\n  retries: 5\n  start_period: 120s\n\n# Redis health check\nhealthcheck:\n  test: [\"CMD\", \"redis-cli\", \"--no-auth-warning\", \"-a\", \"${REDIS_PASSWORD}\", \"ping\"]\n  interval: 30s\n  timeout: 10s\n  retries: 3\n  start_period: 30s\n\n# Backend API health check\nhealthcheck:\n  test: [\"CMD-SHELL\", \"curl -f http://localhost:4000/health || exit 1\"]\n  interval: 30s\n  timeout: 15s\n  retries: 5\n  start_period: 120s\n</code></pre>"},{"location":"installation/docker/#container-lifecycle","title":"Container Lifecycle","text":""},{"location":"installation/docker/#order","title":"Order","text":"<ol> <li>Data Services: PostgreSQL, Redis</li> <li>Application Services: Backend API, Frontend</li> <li>Proxy Services: Nginx</li> <li>Management Services: Adminer, Grafana, Prometheus</li> </ol>"},{"location":"installation/docker/#policies","title":"Policies","text":"<ul> <li>unless-stopped: Restart containers unless explicitly stopped</li> <li>on-failure: Restart only on failure (for development)</li> <li>always: Always restart (for critical services)</li> </ul>"},{"location":"installation/docker/#management","title":"Management","text":"<pre><code>depends_on:\n  postgres:\n    condition: service_healthy\n  redis:\n    condition: service_healthy\n</code></pre>"},{"location":"installation/docker/#monitoring-integration","title":"Monitoring Integration","text":""},{"location":"installation/docker/#metrics","title":"Metrics","text":"<pre><code>prometheus:\n  image: prom/prometheus:latest\n  container_name: prs-onprem-prometheus\n  command:\n    - '--config.file=/etc/prometheus/prometheus.yml'\n    - '--storage.tsdb.path=/prometheus'\n    - '--storage.tsdb.retention.time=30d'\n    - '--storage.tsdb.retention.size=10GB'\n  volumes:\n    - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro\n    - prometheus_data:/prometheus\n    - /mnt/ssd/prometheus-data:/prometheus/ssd\n    - /mnt/hdd/prometheus-archive:/prometheus/archive\n  networks:\n    prs_onprem_network:\n      ipv4_address: 172.20.0.45\n</code></pre>"},{"location":"installation/docker/#dashboards","title":"Dashboards","text":"<pre><code>grafana:\n  image: grafana/grafana:latest\n  container_name: prs-onprem-grafana\n  environment:\n    - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}\n    - GF_USERS_ALLOW_SIGN_UP=false\n    - GF_DATABASE_TYPE=postgres\n    - GF_DATABASE_HOST=postgres:5432\n    - GF_DATABASE_NAME=grafana\n  volumes:\n    - grafana_data:/var/lib/grafana\n    - ./config/grafana/provisioning:/etc/grafana/provisioning:ro\n    - /mnt/ssd/grafana-data:/var/lib/grafana/ssd\n  networks:\n    prs_onprem_network:\n      ipv4_address: 172.20.0.25\n</code></pre>"},{"location":"installation/docker/#security-configuration","title":"Security Configuration","text":""},{"location":"installation/docker/#security","title":"Security","text":"<pre><code># Security options for containers\nsecurity_opt:\n  - no-new-privileges:true\n\n# Read-only root filesystem (where possible)\nread_only: true\n\n# Temporary filesystem for writable areas\ntmpfs:\n  - /tmp\n  - /var/tmp\n\n# User namespace mapping\nuser: \"1000:1000\"\n</code></pre>"},{"location":"installation/docker/#security_1","title":"Security","text":"<pre><code># Disable inter-container communication (where not needed)\ndriver_opts:\n  com.docker.network.bridge.enable_icc: \"false\"\n\n# Custom firewall rules\niptables:\n  - \"-A DOCKER-USER -i prs-onprem-br0 -j ACCEPT\"\n  - \"-A DOCKER-USER -j DROP\"\n</code></pre>"},{"location":"installation/docker/#docker-management","title":"Docker Management","text":""},{"location":"installation/docker/#operations","title":"Operations","text":"<pre><code># Start all services\ndocker-compose -f docker-compose.onprem.yml up -d\n\n# Stop all services\ndocker-compose -f docker-compose.onprem.yml down\n\n# Restart specific service\ndocker-compose -f docker-compose.onprem.yml restart backend\n\n# View logs\ndocker-compose -f docker-compose.onprem.yml logs -f backend\n\n# Check service status\ndocker-compose -f docker-compose.onprem.yml ps\n</code></pre>"},{"location":"installation/docker/#management_1","title":"Management","text":"<pre><code># Build images\ndocker-compose -f docker-compose.onprem.yml build\n\n# Pull latest images\ndocker-compose -f docker-compose.onprem.yml pull\n\n# Remove unused images\ndocker image prune -f\n\n# Remove unused volumes\ndocker volume prune -f\n</code></pre>"},{"location":"installation/docker/#monitoring","title":"Monitoring","text":"<pre><code># Monitor container resources\ndocker stats\n\n# Check container health\ndocker inspect prs-onprem-backend | grep Health -A 10\n\n# View container logs\ndocker logs prs-onprem-backend --tail 100 -f\n</code></pre>"},{"location":"installation/docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/docker/#issues","title":"Issues","text":""},{"location":"installation/docker/#wont-start","title":"Won't Start","text":"<pre><code># Check container logs\ndocker logs prs-onprem-backend\n\n# Check container configuration\ndocker inspect prs-onprem-backend\n\n# Check resource usage\ndocker stats prs-onprem-backend\n</code></pre>"},{"location":"installation/docker/#issues_1","title":"Issues","text":"<pre><code># Check network configuration\ndocker network inspect prs_onprem_network\n\n# Test container connectivity\ndocker exec prs-onprem-backend ping prs-onprem-postgres-timescale\n\n# Check port bindings\ndocker port prs-onprem-nginx\n</code></pre>"},{"location":"installation/docker/#issues_2","title":"Issues","text":"<pre><code># Check volume mounts\ndocker inspect prs-onprem-backend | grep Mounts -A 20\n\n# Check volume permissions\nls -la /mnt/ssd/uploads\nls -la /mnt/hdd/postgres-backups\n\n# Fix volume permissions\nsudo chown -R 999:999 /mnt/ssd/postgresql-hot\n</code></pre>"},{"location":"installation/docker/#issues_3","title":"Issues","text":"<pre><code># Monitor container performance\ndocker stats --no-stream\n\n# Check container resource limits\ndocker inspect prs-onprem-backend | grep -A 10 Resources\n\n# Monitor I/O performance\ndocker exec prs-onprem-backend iostat -x 1\n</code></pre> <p>Docker Ready</p> <p>With proper Docker configuration, the PRS system can efficiently manage resources and provide high availability.</p> <p>Next Steps</p> <p>Proceed to Database Setup to configure TimescaleDB with dual storage.</p>"},{"location":"installation/environment/","title":"Environment Setup","text":""},{"location":"installation/environment/#overview","title":"Overview","text":"<p>This guide covers the complete environment setup for the PRS on-premises deployment, including system preparation, dependency installation, and initial configuration.</p>"},{"location":"installation/environment/#system-preparation","title":"System Preparation","text":""},{"location":"installation/environment/#system-requirements","title":"System Requirements","text":"<p>Supported Operating Systems: - Ubuntu 20.04 LTS or later (Recommended: Ubuntu 22.04 LTS) - CentOS 8 or later - RHEL 8 or later - Debian 11 or later</p>"},{"location":"installation/environment/#updates","title":"Updates","text":"<pre><code># Update package lists and system\nsudo apt update &amp;&amp; sudo apt upgrade -y\n\n# Install essential packages\nsudo apt install -y curl wget git vim htop tree unzip\n\n# Install build tools\nsudo apt install -y build-essential software-properties-common apt-transport-https ca-certificates gnupg lsb-release\n</code></pre>"},{"location":"installation/environment/#setup","title":"Setup","text":"<pre><code># Create deployment user (if not exists)\nsudo useradd -m -s /bin/bash prs-deploy\nsudo usermod -aG sudo prs-deploy\n\n# Setup SSH key authentication\nsudo mkdir -p /home/prs-deploy/.ssh\nsudo cp ~/.ssh/authorized_keys /home/prs-deploy/.ssh/\nsudo chown -R prs-deploy:prs-deploy /home/prs-deploy/.ssh\nsudo chmod 700 /home/prs-deploy/.ssh\nsudo chmod 600 /home/prs-deploy/.ssh/authorized_keys\n\n# Switch to deployment user\nsudo su - prs-deploy\n</code></pre>"},{"location":"installation/environment/#docker-installation","title":"Docker Installation","text":""},{"location":"installation/environment/#docker-engine","title":"Docker Engine","text":"<pre><code># Remove old Docker versions\nsudo apt remove docker docker-engine docker.io containerd runc\n\n# Add Docker's official GPG key\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\n# Add Docker repository\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n# Install Docker\nsudo apt update\nsudo apt install -y docker-ce docker-ce-cli containerd.io\n\n# Add user to docker group\nsudo usermod -aG docker $USER\nnewgrp docker\n\n# Verify installation\ndocker --version\ndocker run hello-world\n</code></pre>"},{"location":"installation/environment/#docker-compose","title":"Docker Compose","text":"<pre><code># Download Docker Compose\nsudo curl -L \"https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n\n# Make executable\nsudo chmod +x /usr/local/bin/docker-compose\n\n# Create symlink for easier access\nsudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\n\n# Verify installation\ndocker-compose --version\n</code></pre>"},{"location":"installation/environment/#configuration","title":"Configuration","text":"<pre><code># Configure Docker daemon\nsudo tee /etc/docker/daemon.json &lt;&lt; EOF\n{\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"100m\",\n    \"max-file\": \"5\"\n  },\n  \"storage-driver\": \"overlay2\",\n  \"storage-opts\": [\n    \"overlay2.override_kernel_check=true\"\n  ],\n  \"default-address-pools\": [\n    {\n      \"base\": \"172.20.0.0/16\",\n      \"size\": 24\n    }\n  ]\n}\nEOF\n\n# Restart Docker\nsudo systemctl restart docker\nsudo systemctl enable docker\n</code></pre>"},{"location":"installation/environment/#repository-setup","title":"Repository Setup","text":""},{"location":"installation/environment/#deployment-repository","title":"Deployment Repository","text":"<pre><code># Navigate to deployment directory\ncd /opt\n\n# Clone deployment repository\nsudo git clone https://github.com/your-org/prs-deployment.git\nsudo chown -R $USER:$USER /opt/prs-deployment\ncd /opt/prs-deployment\n\n# Verify repository structure\ntree -L 2\n</code></pre>"},{"location":"installation/environment/#repository-configuration","title":"Repository Configuration","text":"<pre><code># Copy repository configuration template\ncp scripts/repo-config.example.sh scripts/repo-config.sh\n\n# Edit repository configuration\nnano scripts/repo-config.sh\n</code></pre> <p>Repository Configuration: <pre><code>#!/bin/bash\n\n# Repository Configuration for PRS Deployment\n\n# Base directory for repositories\nexport REPOS_BASE_DIR=\"/opt/prs\"\n\n# Repository names\nexport BACKEND_REPO_NAME=\"prs-backend-a\"\nexport FRONTEND_REPO_NAME=\"prs-frontend-a\"\n\n# Repository URLs\nexport BACKEND_REPO_URL=\"https://github.com/your-org/prs-backend-a.git\"\nexport FRONTEND_REPO_URL=\"https://github.com/your-org/prs-frontend-a.git\"\n\n# Branch configuration\nexport BACKEND_BRANCH=\"main\"\nexport FRONTEND_BRANCH=\"main\"\n\n# Build configuration\nexport NODE_ENV=\"production\"\nexport BUILD_TARGET=\"production\"\n</code></pre></p>"},{"location":"installation/environment/#environment-configuration","title":"Environment Configuration","text":""},{"location":"installation/environment/#environment-setup_1","title":"Environment Setup","text":"<pre><code># Run automated environment setup\ncd /opt/prs-deployment/scripts\nchmod +x setup-env.sh\n./setup-env.sh\n</code></pre> <p>The script will: 1. Create environment file from template 2. Generate secure passwords and secrets 3. Configure basic settings 4. Prompt for custom configuration</p>"},{"location":"installation/environment/#environment-configuration_1","title":"Environment Configuration","text":"<pre><code># Copy environment template\ncp 02-docker-configuration/.env.example 02-docker-configuration/.env\n\n# Edit environment file\nnano 02-docker-configuration/.env\n</code></pre>"},{"location":"installation/environment/#environment-variables","title":"Environment Variables","text":""},{"location":"installation/environment/#and-network-configuration","title":"and Network Configuration","text":"<pre><code># Domain configuration\nDOMAIN=your-domain.com\nSERVER_IP=192.168.0.100\nNETWORK_SUBNET=192.168.0.0/20\nNETWORK_GATEWAY=192.168.0.1\n\n# SSL configuration\nSSL_EMAIL=admin@your-domain.com\nENABLE_SSL=true\n</code></pre>"},{"location":"installation/environment/#configuration_1","title":"Configuration","text":"<pre><code># PostgreSQL configuration\nPOSTGRES_DB=prs_production\nPOSTGRES_USER=prs_admin\nPOSTGRES_PASSWORD=your_secure_password_here\nPOSTGRES_PORT=5432\n\n# Database performance settings\nPOSTGRES_MAX_CONNECTIONS=150\nPOSTGRES_SHARED_BUFFERS=2GB\nPOSTGRES_EFFECTIVE_CACHE_SIZE=4GB\nPOSTGRES_WORK_MEM=32MB\nPOSTGRES_MAINTENANCE_WORK_MEM=512MB\n</code></pre>"},{"location":"installation/environment/#secrets","title":"Secrets","text":"<pre><code># Generate secure secrets\nJWT_SECRET=$(openssl rand -base64 32)\nENCRYPTION_KEY=$(openssl rand -base64 32)\nOTP_KEY=$(openssl rand -base64 16)\nPASS_SECRET=$(openssl rand -base64 32)\n\n# Redis configuration\nREDIS_PASSWORD=$(openssl rand -base64 32)\nREDIS_MEMORY_LIMIT=2g\n</code></pre>"},{"location":"installation/environment/#api-configuration","title":"API Configuration","text":"<pre><code># Cityland API integration\nCITYLAND_API_URL=https://your-api-endpoint.com\nCITYLAND_ACCOUNTING_URL=https://your-accounting-endpoint.com\nCITYLAND_API_USERNAME=your_api_username\nCITYLAND_API_PASSWORD=your_api_password\n</code></pre>"},{"location":"installation/environment/#configuration_2","title":"Configuration","text":"<pre><code># Grafana configuration\nGRAFANA_ADMIN_PASSWORD=$(openssl rand -base64 32)\nGRAFANA_PORT=3001\n\n# Prometheus configuration\nPROMETHEUS_RETENTION_TIME=30d\nPROMETHEUS_RETENTION_SIZE=10GB\n\n# Enable monitoring\nPROMETHEUS_ENABLED=true\nENABLE_METRICS=true\nENABLE_HEALTH_CHECKS=true\n</code></pre>"},{"location":"installation/environment/#storage-setup","title":"Storage Setup","text":""},{"location":"installation/environment/#storage-setup_1","title":"Storage Setup","text":"<pre><code># Run storage setup script\ncd /opt/prs-deployment/scripts\nsudo ./setup-storage.sh\n</code></pre>"},{"location":"installation/environment/#storage-setup_2","title":"Storage Setup","text":"<pre><code># Create SSD storage directories\nsudo mkdir -p /mnt/ssd/{postgresql-hot,redis-data,uploads,logs,nginx-cache,prometheus-data,grafana-data,portainer-data}\n\n# Create HDD storage directories\nsudo mkdir -p /mnt/hdd/{postgresql-cold,postgres-backups,app-logs-archive,redis-backups,prometheus-archive}\n\n# Set ownership for PostgreSQL\nsudo chown -R 999:999 /mnt/ssd/postgresql-hot /mnt/hdd/postgresql-cold /mnt/hdd/postgres-backups\n\n# Set ownership for Redis\nsudo chown -R 999:999 /mnt/ssd/redis-data /mnt/hdd/redis-backups\n\n# Set ownership for Grafana\nsudo chown -R 472:472 /mnt/ssd/grafana-data\n\n# Set ownership for Prometheus\nsudo chown -R 65534:65534 /mnt/ssd/prometheus-data /mnt/hdd/prometheus-archive\n\n# Set ownership for Nginx\nsudo chown -R www-data:www-data /mnt/ssd/nginx-cache /mnt/ssd/uploads\n\n# Set ownership for application logs\nsudo chown -R 1000:1000 /mnt/ssd/logs /mnt/hdd/app-logs-archive\n\n# Set permissions\nsudo chmod 700 /mnt/ssd/postgresql-hot /mnt/hdd/postgresql-cold\nsudo chmod 755 /mnt/ssd/redis-data /mnt/ssd/uploads /mnt/ssd/logs\nsudo chmod 755 /mnt/hdd/postgres-backups /mnt/hdd/app-logs-archive\n</code></pre>"},{"location":"installation/environment/#storage-setup_3","title":"Storage Setup","text":"<pre><code># Check storage structure\ntree /mnt/ssd /mnt/hdd\n\n# Check permissions\nls -la /mnt/ssd/\nls -la /mnt/hdd/\n\n# Check disk space\ndf -h /mnt/ssd /mnt/hdd\n</code></pre>"},{"location":"installation/environment/#security-setup","title":"Security Setup","text":""},{"location":"installation/environment/#configuration_3","title":"Configuration","text":"<pre><code># Install and configure UFW\nsudo apt install ufw\n\n# Reset firewall rules\nsudo ufw --force reset\n\n# Set default policies\nsudo ufw default deny incoming\nsudo ufw default allow outgoing\n\n# Allow SSH (restrict to admin network)\nsudo ufw allow from 192.168.0.201/24 to any port 22\n\n# Allow HTTP/HTTPS (internal network)\nsudo ufw allow from 192.168.0.0/20 to any port 80\nsudo ufw allow from 192.168.0.0/20 to any port 443\n\n# Allow management interfaces (admin only)\nsudo ufw allow from 192.168.0.201/24 to any port 8080  # Adminer\nsudo ufw allow from 192.168.0.201/24 to any port 3001  # Grafana\nsudo ufw allow from 192.168.0.201/24 to any port 9000  # Portainer\nsudo ufw allow from 192.168.0.201/24 to any port 9090  # Prometheus\n\n# Enable firewall\nsudo ufw --force enable\n\n# Check status\nsudo ufw status verbose\n</code></pre>"},{"location":"installation/environment/#hardening","title":"Hardening","text":"<pre><code># Install fail2ban\nsudo apt install fail2ban\n\n# Configure fail2ban\nsudo tee /etc/fail2ban/jail.local &lt;&lt; EOF\n[DEFAULT]\nbantime = 3600\nfindtime = 600\nmaxretry = 3\n\n[sshd]\nenabled = true\nport = ssh\nfilter = sshd\nlogpath = /var/log/auth.log\nmaxretry = 3\nbantime = 3600\nEOF\n\n# Start fail2ban\nsudo systemctl enable fail2ban\nsudo systemctl start fail2ban\n</code></pre>"},{"location":"installation/environment/#certificate-preparation","title":"Certificate Preparation","text":"<pre><code># Create SSL directory\nsudo mkdir -p /opt/prs-deployment/02-docker-configuration/ssl\n\n# Set permissions\nsudo chmod 755 /opt/prs-deployment/02-docker-configuration/ssl\n\n# Option 1: Copy existing certificates\nsudo cp /path/to/your/certificate.crt /opt/prs-deployment/02-docker-configuration/ssl/\nsudo cp /path/to/your/private.key /opt/prs-deployment/02-docker-configuration/ssl/\nsudo cp /path/to/your/ca-bundle.crt /opt/prs-deployment/02-docker-configuration/ssl/\n\n# Option 2: Generate self-signed certificate (development only)\nsudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 \\\n  -keyout /opt/prs-deployment/02-docker-configuration/ssl/private.key \\\n  -out /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt \\\n  -subj \"/C=US/ST=State/L=City/O=Organization/CN=your-domain.com\"\n</code></pre>"},{"location":"installation/environment/#system-optimization","title":"System Optimization","text":""},{"location":"installation/environment/#parameters","title":"Parameters","text":"<pre><code># Optimize for database workloads\nsudo tee -a /etc/sysctl.conf &lt;&lt; EOF\n# Memory management\nvm.swappiness = 10\nvm.dirty_ratio = 15\nvm.dirty_background_ratio = 5\nvm.overcommit_memory = 2\nvm.overcommit_ratio = 80\n\n# Network optimization\nnet.core.rmem_max = 16777216\nnet.core.wmem_max = 16777216\nnet.ipv4.tcp_rmem = 4096 87380 16777216\nnet.ipv4.tcp_wmem = 4096 65536 16777216\nnet.core.somaxconn = 65536\nnet.core.netdev_max_backlog = 5000\n\n# File system\nfs.file-max = 2097152\nfs.nr_open = 1048576\nEOF\n\n# Apply changes\nsudo sysctl -p\n</code></pre>"},{"location":"installation/environment/#limits","title":"Limits","text":"<pre><code># Increase file descriptor limits\nsudo tee -a /etc/security/limits.conf &lt;&lt; EOF\n* soft nofile 65536\n* hard nofile 65536\nroot soft nofile 65536\nroot hard nofile 65536\nEOF\n\n# Update systemd limits\nsudo tee /etc/systemd/system.conf.d/limits.conf &lt;&lt; EOF\n[Manager]\nDefaultLimitNOFILE=65536\nEOF\n</code></pre>"},{"location":"installation/environment/#environment-validation","title":"Environment Validation","text":""},{"location":"installation/environment/#validation","title":"Validation","text":"<pre><code># Check system resources\nfree -h\ndf -h\nlscpu\nlsblk\n\n# Check network configuration\nip addr show\nip route show\nping -c 4 8.8.8.8\n\n# Check Docker installation\ndocker --version\ndocker-compose --version\ndocker run hello-world\n</code></pre>"},{"location":"installation/environment/#validation_1","title":"Validation","text":"<pre><code># Check storage mounts\nmount | grep -E \"(ssd|hdd)\"\n\n# Check storage permissions\nls -la /mnt/ssd/\nls -la /mnt/hdd/\n\n# Test storage performance\nsudo fio --name=test --filename=/mnt/ssd/test --size=1G --rw=randwrite --bs=4k --numjobs=1 --time_based --runtime=30\n</code></pre>"},{"location":"installation/environment/#validation_2","title":"Validation","text":"<pre><code># Check firewall status\nsudo ufw status verbose\n\n# Check fail2ban status\nsudo systemctl status fail2ban\n\n# Check SSL certificates\nopenssl x509 -in /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt -text -noout\n</code></pre>"},{"location":"installation/environment/#file-validation","title":"File Validation","text":"<pre><code># Check environment file\ncat /opt/prs-deployment/02-docker-configuration/.env | grep -v PASSWORD\n\n# Validate required variables\ngrep -E \"(DOMAIN|POSTGRES_DB|JWT_SECRET)\" /opt/prs-deployment/02-docker-configuration/.env\n</code></pre>"},{"location":"installation/environment/#pre-deployment-checklist","title":"Pre-Deployment Checklist","text":"<ul> <li>[ ] Operating System: Ubuntu 20.04+ installed and updated</li> <li>[ ] Docker: Docker and Docker Compose installed and working</li> <li>[ ] Storage: SSD and HDD storage mounted and configured</li> <li>[ ] Network: Network interfaces configured and tested</li> <li>[ ] Security: Firewall and fail2ban configured</li> <li>[ ] SSL: SSL certificates available and configured</li> <li>[ ] Environment: Environment variables configured</li> <li>[ ] Repositories: Repository configuration completed</li> <li>[ ] Permissions: File and directory permissions set correctly</li> <li>[ ] Performance: System optimization applied</li> </ul> <p>Environment Ready</p> <p>Once all validation steps pass, your environment is ready for Docker configuration and deployment.</p> <p>Next Steps</p> <p>Proceed to Docker Configuration to configure the container environment.</p>"},{"location":"installation/ssl/","title":"SSL Configuration","text":""},{"location":"installation/ssl/#overview","title":"Overview","text":"<p>This guide covers SSL/TLS certificate setup for the PRS on-premises deployment, including automated certificate management and security configuration.</p>"},{"location":"installation/ssl/#ssl-architecture","title":"SSL Architecture","text":"<pre><code>graph TB\n    subgraph \"External Access\"\n        CLIENT[Client Browser] --&gt; HTTPS[HTTPS Request&lt;br/&gt;Port 443]\n        HTTP[HTTP Request&lt;br/&gt;Port 80] --&gt; REDIRECT[301 Redirect to HTTPS]\n    end\n\n    subgraph \"SSL Termination\"\n        HTTPS --&gt; NGINX[Nginx Reverse Proxy&lt;br/&gt;SSL Termination]\n        REDIRECT --&gt; NGINX\n    end\n\n    subgraph \"Certificate Management\"\n        CERT[SSL Certificates&lt;br/&gt;/ssl/certificate.crt] --&gt; NGINX\n        KEY[Private Key&lt;br/&gt;/ssl/private.key] --&gt; NGINX\n        CA[CA Bundle&lt;br/&gt;/ssl/ca-bundle.crt] --&gt; NGINX\n    end\n\n    subgraph \"Backend Services\"\n        NGINX --&gt; FRONTEND[Frontend&lt;br/&gt;HTTP Internal]\n        NGINX --&gt; BACKEND[Backend API&lt;br/&gt;HTTP Internal]\n    end\n\n    style NGINX fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style CERT fill:#e8f5e8,stroke:#4caf50,stroke-width:2px\n    style HTTPS fill:#fff3e0,stroke:#ff9800,stroke-width:2px</code></pre>"},{"location":"installation/ssl/#certificate-options","title":"Certificate Options","text":""},{"location":"installation/ssl/#option-1-lets-encrypt-automated","title":"Option 1: Let's Encrypt (Automated)","text":""},{"location":"installation/ssl/#prerequisites","title":"Prerequisites","text":"<pre><code># Install Certbot\nsudo apt update\nsudo apt install certbot\n\n# Verify domain DNS points to server\nnslookup your-domain.com\n</code></pre>"},{"location":"installation/ssl/#automated-certificate-generation","title":"Automated Certificate Generation","text":"<pre><code># Run SSL automation script\ncd /opt/prs-deployment/scripts\n./ssl-automation-citylandcondo.sh\n\n# The script will:\n# 1. Stop nginx temporarily\n# 2. Generate certificate using Certbot\n# 3. Configure nginx with new certificate\n# 4. Setup auto-renewal\n# 5. Restart nginx\n</code></pre>"},{"location":"installation/ssl/#manual-lets-encrypt-setup","title":"Manual Let's Encrypt Setup","text":"<pre><code># Stop nginx for standalone mode\ndocker-compose -f ../02-docker-configuration/docker-compose.onprem.yml stop nginx\n\n# Generate certificate\nsudo certbot certonly \\\n  --standalone \\\n  --email admin@your-domain.com \\\n  --agree-tos \\\n  --no-eff-email \\\n  --domains your-domain.com\n\n# Copy certificates to SSL directory\nsudo cp /etc/letsencrypt/live/your-domain.com/fullchain.pem \\\n  /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt\n\nsudo cp /etc/letsencrypt/live/your-domain.com/privkey.pem \\\n  /opt/prs-deployment/02-docker-configuration/ssl/private.key\n\nsudo cp /etc/letsencrypt/live/your-domain.com/chain.pem \\\n  /opt/prs-deployment/02-docker-configuration/ssl/ca-bundle.crt\n\n# Start nginx\ndocker-compose -f ../02-docker-configuration/docker-compose.onprem.yml start nginx\n</code></pre>"},{"location":"installation/ssl/#option-2-existing-certificates","title":"Option 2: Existing Certificates","text":""},{"location":"installation/ssl/#copy-existing-certificates","title":"Copy Existing Certificates","text":"<pre><code># Create SSL directory\nsudo mkdir -p /opt/prs-deployment/02-docker-configuration/ssl\n\n# Copy your existing certificates\nsudo cp /path/to/your/certificate.crt \\\n  /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt\n\nsudo cp /path/to/your/private.key \\\n  /opt/prs-deployment/02-docker-configuration/ssl/private.key\n\nsudo cp /path/to/your/ca-bundle.crt \\\n  /opt/prs-deployment/02-docker-configuration/ssl/ca-bundle.crt\n\n# Set proper permissions\nsudo chmod 644 /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt\nsudo chmod 600 /opt/prs-deployment/02-docker-configuration/ssl/private.key\nsudo chmod 644 /opt/prs-deployment/02-docker-configuration/ssl/ca-bundle.crt\n</code></pre>"},{"location":"installation/ssl/#option-3-self-signed-certificate-development-only","title":"Option 3: Self-Signed Certificate (Development Only)","text":"<pre><code># Generate self-signed certificate (NOT for production)\nsudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 \\\n  -keyout /opt/prs-deployment/02-docker-configuration/ssl/private.key \\\n  -out /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt \\\n  -subj \"/C=US/ST=State/L=City/O=Organization/CN=your-domain.com\"\n\n# Create empty CA bundle\nsudo touch /opt/prs-deployment/02-docker-configuration/ssl/ca-bundle.crt\n</code></pre>"},{"location":"installation/ssl/#nginx-ssl-configuration","title":"Nginx SSL Configuration","text":""},{"location":"installation/ssl/#ssl-virtual-host","title":"SSL Virtual Host","text":"<pre><code># HTTP to HTTPS redirect\nserver {\n    listen 80;\n    server_name your-domain.com;\n    return 301 https://$server_name$request_uri;\n}\n\n# Main HTTPS server\nserver {\n    listen 443 ssl http2;\n    server_name your-domain.com;\n\n    # SSL Configuration\n    ssl_certificate /etc/nginx/ssl/certificate.crt;\n    ssl_certificate_key /etc/nginx/ssl/private.key;\n    ssl_trusted_certificate /etc/nginx/ssl/ca-bundle.crt;\n\n    # SSL Security Settings\n    ssl_protocols TLSv1.2 TLSv1.3;\n    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384;\n    ssl_prefer_server_ciphers off;\n    ssl_session_cache shared:SSL:10m;\n    ssl_session_timeout 10m;\n    ssl_session_tickets off;\n\n    # HSTS (HTTP Strict Transport Security)\n    add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\" always;\n\n    # Security Headers\n    add_header X-Frame-Options DENY always;\n    add_header X-Content-Type-Options nosniff always;\n    add_header X-XSS-Protection \"1; mode=block\" always;\n    add_header Referrer-Policy \"strict-origin-when-cross-origin\" always;\n\n    # OCSP Stapling\n    ssl_stapling on;\n    ssl_stapling_verify on;\n    resolver 8.8.8.8 8.8.4.4 valid=300s;\n    resolver_timeout 5s;\n\n    # Application routes\n    location / {\n        proxy_pass http://frontend:3000;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n\n    location /api/ {\n        proxy_pass http://backend:4000;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre>"},{"location":"installation/ssl/#ssl-security-optimization","title":"SSL Security Optimization","text":"<pre><code># SSL optimization in main nginx.conf\nhttp {\n    # SSL session cache\n    ssl_session_cache shared:SSL:50m;\n    ssl_session_timeout 1d;\n    ssl_session_tickets off;\n\n    # Modern configuration\n    ssl_protocols TLSv1.2 TLSv1.3;\n    ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384;\n    ssl_prefer_server_ciphers off;\n\n    # HSTS preload\n    add_header Strict-Transport-Security \"max-age=63072000; includeSubDomains; preload\" always;\n}\n</code></pre>"},{"location":"installation/ssl/#certificate-management","title":"Certificate Management","text":""},{"location":"installation/ssl/#automatic-renewal-setup","title":"Automatic Renewal Setup","text":"<pre><code># Create renewal script\ncat &gt; /opt/prs-deployment/scripts/ssl-renewal.sh &lt;&lt; 'EOF'\n#!/bin/bash\nset -e\n\n# Renew certificates\ncertbot renew --quiet\n\n# Copy renewed certificates\nif [ -f /etc/letsencrypt/live/your-domain.com/fullchain.pem ]; then\n    cp /etc/letsencrypt/live/your-domain.com/fullchain.pem \\\n       /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt\n\n    cp /etc/letsencrypt/live/your-domain.com/privkey.pem \\\n       /opt/prs-deployment/02-docker-configuration/ssl/private.key\n\n    cp /etc/letsencrypt/live/your-domain.com/chain.pem \\\n       /opt/prs-deployment/02-docker-configuration/ssl/ca-bundle.crt\n\n    # Reload nginx\n    docker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml exec nginx nginx -s reload\n\n    echo \"$(date): SSL certificates renewed successfully\" &gt;&gt; /var/log/ssl-renewal.log\nfi\nEOF\n\n# Make executable\nchmod +x /opt/prs-deployment/scripts/ssl-renewal.sh\n\n# Add to crontab (runs twice daily)\n(crontab -l 2&gt;/dev/null; echo \"0 3,15 * * * /opt/prs-deployment/scripts/ssl-renewal.sh\") | crontab -\n</code></pre>"},{"location":"installation/ssl/#certificate-monitoring","title":"Certificate Monitoring","text":"<pre><code># Check certificate expiration\nopenssl x509 -in /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt -noout -dates\n\n# Check certificate details\nopenssl x509 -in /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt -text -noout\n\n# Verify certificate chain\nopenssl verify -CAfile /opt/prs-deployment/02-docker-configuration/ssl/ca-bundle.crt \\\n  /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt\n</code></pre>"},{"location":"installation/ssl/#ssl-testing-and-validation","title":"SSL Testing and Validation","text":""},{"location":"installation/ssl/#certificate-validation","title":"Certificate Validation","text":"<pre><code># Test SSL connection\nopenssl s_client -connect your-domain.com:443 -servername your-domain.com\n\n# Check certificate chain\ncurl -vI https://your-domain.com/\n\n# Test with SSL Labs (external)\n# Visit: https://www.ssllabs.com/ssltest/analyze.html?d=your-domain.com\n</code></pre>"},{"location":"installation/ssl/#security-headers-testing","title":"Security Headers Testing","text":"<pre><code># Test security headers\ncurl -I https://your-domain.com/\n\n# Expected headers:\n# Strict-Transport-Security: max-age=31536000; includeSubDomains\n# X-Frame-Options: DENY\n# X-Content-Type-Options: nosniff\n# X-XSS-Protection: 1; mode=block\n</code></pre>"},{"location":"installation/ssl/#performance-testing","title":"Performance Testing","text":"<pre><code># Test SSL handshake time\ncurl -w \"@curl-format.txt\" -o /dev/null -s https://your-domain.com/\n\n# Create curl-format.txt\ncat &gt; curl-format.txt &lt;&lt; 'EOF'\n     time_namelookup:  %{time_namelookup}\\n\n        time_connect:  %{time_connect}\\n\n     time_appconnect:  %{time_appconnect}\\n\n    time_pretransfer:  %{time_pretransfer}\\n\n       time_redirect:  %{time_redirect}\\n\n  time_starttransfer:  %{time_starttransfer}\\n\n                     ----------\\n\n          time_total:  %{time_total}\\n\nEOF\n</code></pre>"},{"location":"installation/ssl/#management-interfaces-ssl","title":"Management Interfaces SSL","text":""},{"location":"installation/ssl/#grafana-ssl-access","title":"Grafana SSL Access","text":"<pre><code># Grafana with SSL\nserver {\n    listen 3001 ssl http2;\n    server_name your-domain.com;\n\n    ssl_certificate /etc/nginx/ssl/certificate.crt;\n    ssl_certificate_key /etc/nginx/ssl/private.key;\n\n    location / {\n        proxy_pass http://grafana:3000;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre>"},{"location":"installation/ssl/#adminer-ssl-access","title":"Adminer SSL Access","text":"<pre><code># Adminer with SSL\nserver {\n    listen 8080 ssl http2;\n    server_name your-domain.com;\n\n    ssl_certificate /etc/nginx/ssl/certificate.crt;\n    ssl_certificate_key /etc/nginx/ssl/private.key;\n\n    # Restrict access to admin network\n    allow 192.168.0.201/24;\n    deny all;\n\n    location / {\n        proxy_pass http://adminer:8080;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre>"},{"location":"installation/ssl/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/ssl/#common-ssl-issues","title":"Common SSL Issues","text":""},{"location":"installation/ssl/#certificate-not-found","title":"Certificate Not Found","text":"<pre><code># Check certificate files exist\nls -la /opt/prs-deployment/02-docker-configuration/ssl/\n\n# Check file permissions\nls -la /opt/prs-deployment/02-docker-configuration/ssl/\n# certificate.crt should be 644\n# private.key should be 600\n# ca-bundle.crt should be 644\n</code></pre>"},{"location":"installation/ssl/#certificate-expired","title":"Certificate Expired","text":"<pre><code># Check expiration date\nopenssl x509 -in /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt -noout -dates\n\n# Renew Let's Encrypt certificate\nsudo certbot renew --force-renewal\n\n# Copy renewed certificates\nsudo cp /etc/letsencrypt/live/your-domain.com/fullchain.pem \\\n  /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt\n</code></pre>"},{"location":"installation/ssl/#ssl-handshake-failures","title":"SSL Handshake Failures","text":"<pre><code># Check nginx SSL configuration\ndocker exec prs-onprem-nginx nginx -t\n\n# Check nginx logs\ndocker logs prs-onprem-nginx\n\n# Test SSL configuration\nopenssl s_client -connect your-domain.com:443 -servername your-domain.com\n</code></pre>"},{"location":"installation/ssl/#mixed-content-warnings","title":"Mixed Content Warnings","text":"<pre><code># Ensure all internal URLs use HTTPS\ngrep -r \"http://\" /opt/prs/prs-frontend-a/src/\n\n# Update environment variables\n# VITE_APP_API_URL=https://your-domain.com/api\n</code></pre>"},{"location":"installation/ssl/#ssl-security-audit","title":"SSL Security Audit","text":"<pre><code># Check SSL configuration strength\nnmap --script ssl-enum-ciphers -p 443 your-domain.com\n\n# Test for vulnerabilities\nnmap --script ssl-heartbleed,ssl-poodle,ssl-dh-params -p 443 your-domain.com\n\n# Check certificate transparency logs\n# Visit: https://crt.sh/?q=your-domain.com\n</code></pre> <p>SSL Configured</p> <p>Your PRS deployment now has enterprise-grade SSL/TLS security with automatic certificate renewal.</p> <p>Security Best Practices</p> <ul> <li>Use strong cipher suites (TLSv1.2 and TLSv1.3 only)</li> <li>Enable HSTS with preload</li> <li>Implement proper security headers</li> <li>Monitor certificate expiration dates</li> <li>Regular security audits</li> </ul> <p>Certificate Management</p> <p>Always test certificate renewal procedures and monitor expiration dates to prevent service interruptions.</p>"},{"location":"maintenance/capacity/","title":"Capacity Planning","text":""},{"location":"maintenance/capacity/#overview","title":"Overview","text":"<p>This guide covers capacity planning procedures for the PRS on-premises deployment, including growth analysis, resource forecasting, and scaling recommendations to ensure optimal performance as your organization grows.</p>"},{"location":"maintenance/capacity/#capacity-planning-framework","title":"Capacity Planning Framework","text":""},{"location":"maintenance/capacity/#planning-horizons","title":"Planning Horizons","text":"Timeframe Focus Actions Short-term (1-3 months) Immediate needs Resource monitoring, quick optimizations Medium-term (3-12 months) Growth trends Hardware planning, budget allocation Long-term (1-3 years) Strategic planning Architecture evolution, major upgrades"},{"location":"maintenance/capacity/#key-metrics-for-capacity-planning","title":"Key Metrics for Capacity Planning","text":"<pre><code>graph TB\n    subgraph \"Performance Metrics\"\n        CPU[CPU Utilization&lt;br/&gt;Target: &lt;70%]\n        MEM[Memory Usage&lt;br/&gt;Target: &lt;80%]\n        DISK[Disk Usage&lt;br/&gt;Target: &lt;85%]\n        NET[Network I/O&lt;br/&gt;Monitor bandwidth]\n    end\n\n    subgraph \"Application Metrics\"\n        USERS[Concurrent Users&lt;br/&gt;Current: 100+]\n        RESP[Response Time&lt;br/&gt;Target: &lt;200ms]\n        THRU[Throughput&lt;br/&gt;Target: 500+ QPS]\n        SESS[Active Sessions&lt;br/&gt;Monitor growth]\n    end\n\n    subgraph \"Data Growth\"\n        DBSIZE[Database Size&lt;br/&gt;Monitor growth rate]\n        UPLOADS[File Storage&lt;br/&gt;Monitor upload volume]\n        LOGS[Log Volume&lt;br/&gt;Monitor retention]\n        BACKUP[Backup Size&lt;br/&gt;Plan storage needs]\n    end\n\n    subgraph \"Business Metrics\"\n        REQS[Requisitions/Day&lt;br/&gt;Track volume growth]\n        DEPTS[Departments&lt;br/&gt;Track expansion]\n        VENDORS[Vendors&lt;br/&gt;Monitor additions]\n        REPORTS[Report Usage&lt;br/&gt;Track complexity]\n    end\n\n    style CPU fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style USERS fill:#e8f5e8,stroke:#4caf50,stroke-width:2px,stroke:#4caf50,stroke-width:2px\n    style DBSIZE fill:#fff3e0,stroke:#ff9800,stroke-width:2px,stroke:#ff9800,stroke-width:2px\n    style REQS fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px,stroke:#9c27b0,stroke-width:2px</code></pre>"},{"location":"maintenance/capacity/#current-capacity-assessment","title":"Current Capacity Assessment","text":""},{"location":"maintenance/capacity/#system-resource-analysis","title":"System Resource Analysis","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/capacity-assessment.sh\n\nset -euo pipefail\n\nREPORT_FILE=\"/tmp/capacity-assessment-$(date +%Y%m%d).txt\"\n\ngenerate_capacity_report() {\n    cat &gt; \"$REPORT_FILE\" &lt;&lt; EOF\nPRS Capacity Assessment Report\n==============================\nGenerated: $(date)\nAssessment Period: Last 30 days\n\nCURRENT SYSTEM SPECIFICATIONS\n-----------------------------\nEOF\n\n    # Hardware specifications\n    echo \"Hardware Configuration:\" &gt;&gt; \"$REPORT_FILE\"\n    echo \"- CPU Cores: $(nproc)\" &gt;&gt; \"$REPORT_FILE\"\n    echo \"- Total Memory: $(free -h | grep Mem | awk '{print $2}')\" &gt;&gt; \"$REPORT_FILE\"\n    echo \"- SSD Storage: $(df -h /mnt/ssd | awk 'NR==2 {print $2}')\" &gt;&gt; \"$REPORT_FILE\"\n    echo \"- HDD Storage: $(df -h /mnt/hdd | awk 'NR==2 {print $2}')\" &gt;&gt; \"$REPORT_FILE\"\n\n    # Current utilization\n    echo \"\" &gt;&gt; \"$REPORT_FILE\"\n    echo \"CURRENT RESOURCE UTILIZATION\" &gt;&gt; \"$REPORT_FILE\"\n    echo \"----------------------------\" &gt;&gt; \"$REPORT_FILE\"\n    echo \"- CPU Usage: $(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | sed 's/%us,//')%\" &gt;&gt; \"$REPORT_FILE\"\n    echo \"- Memory Usage: $(free | grep Mem | awk '{printf \"%.1f\", $3/$2 * 100.0}')%\" &gt;&gt; \"$REPORT_FILE\"\n    echo \"- SSD Usage: $(df /mnt/ssd | awk 'NR==2 {print $5}')\" &gt;&gt; \"$REPORT_FILE\"\n    echo \"- HDD Usage: $(df /mnt/hdd | awk 'NR==2 {print $5}')\" &gt;&gt; \"$REPORT_FILE\"\n\n    # Application metrics\n    echo \"\" &gt;&gt; \"$REPORT_FILE\"\n    echo \"APPLICATION PERFORMANCE METRICS\" &gt;&gt; \"$REPORT_FILE\"\n    echo \"-------------------------------\" &gt;&gt; \"$REPORT_FILE\"\n\n    # Database metrics\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n    SELECT\n        'Database Size: ' || pg_size_pretty(pg_database_size('prs_production'))\n    UNION ALL\n    SELECT\n        'Active Connections: ' || count(*)::text\n    FROM pg_stat_activity\n    UNION ALL\n    SELECT\n        'Cache Hit Ratio: ' || round(100.0 * sum(blks_hit) / nullif(sum(blks_hit) + sum(blks_read), 0), 2)::text || '%'\n    FROM pg_stat_database\n    WHERE datname = 'prs_production'\n    UNION ALL\n    SELECT\n        'Total Users: ' || count(*)::text\n    FROM users\n    UNION ALL\n    SELECT\n        'Active Users (30d): ' || count(DISTINCT user_id)::text\n    FROM audit_logs\n    WHERE created_at &gt;= NOW() - INTERVAL '30 days'\n    UNION ALL\n    SELECT\n        'Requisitions (30d): ' || count(*)::text\n    FROM requisitions\n    WHERE created_at &gt;= NOW() - INTERVAL '30 days';\n    \" &gt;&gt; \"$REPORT_FILE\"\n\n    # Growth analysis\n    echo \"\" &gt;&gt; \"$REPORT_FILE\"\n    echo \"GROWTH ANALYSIS (30-day trends)\" &gt;&gt; \"$REPORT_FILE\"\n    echo \"-------------------------------\" &gt;&gt; \"$REPORT_FILE\"\n\n    # Calculate growth rates\n    analyze_growth_trends &gt;&gt; \"$REPORT_FILE\"\n\n    # Capacity recommendations\n    echo \"\" &gt;&gt; \"$REPORT_FILE\"\n    echo \"CAPACITY RECOMMENDATIONS\" &gt;&gt; \"$REPORT_FILE\"\n    echo \"------------------------\" &gt;&gt; \"$REPORT_FILE\"\n\n    generate_recommendations &gt;&gt; \"$REPORT_FILE\"\n\n    echo \"Capacity assessment completed: $REPORT_FILE\"\n}\n\nanalyze_growth_trends() {\n    # User growth\n    USER_GROWTH=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    WITH monthly_users AS (\n        SELECT\n            DATE_TRUNC('month', created_at) as month,\n            COUNT(*) as new_users\n        FROM users\n        WHERE created_at &gt;= NOW() - INTERVAL '3 months'\n        GROUP BY DATE_TRUNC('month', created_at)\n        ORDER BY month\n    )\n    SELECT COALESCE(\n        ROUND(\n            (LAST_VALUE(new_users) OVER (ORDER BY month ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) -\n             FIRST_VALUE(new_users) OVER (ORDER BY month ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING))::numeric /\n            NULLIF(FIRST_VALUE(new_users) OVER (ORDER BY month ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING), 0) * 100, 2\n        ), 0\n    ) as growth_rate\n    FROM monthly_users\n    LIMIT 1;\n    \" | xargs)\n\n    echo \"- User Growth Rate: ${USER_GROWTH}% per month\"\n\n    # Data growth\n    DATA_GROWTH=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT\n        pg_size_pretty(\n            (SELECT pg_database_size('prs_production')) -\n            (SELECT COALESCE(SUM(chunk_size), 0) FROM timescaledb_information.chunks\n             WHERE range_start &lt; NOW() - INTERVAL '30 days')\n        ) as monthly_growth;\n    \" | xargs)\n\n    echo \"- Data Growth (30d): $DATA_GROWTH\"\n\n    # Transaction growth\n    TRANSACTION_GROWTH=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    WITH daily_transactions AS (\n        SELECT\n            DATE(created_at) as day,\n            COUNT(*) as transactions\n        FROM audit_logs\n        WHERE created_at &gt;= NOW() - INTERVAL '30 days'\n        GROUP BY DATE(created_at)\n    )\n    SELECT ROUND(AVG(transactions), 0) as avg_daily_transactions\n    FROM daily_transactions;\n    \" | xargs)\n\n    echo \"- Average Daily Transactions: $TRANSACTION_GROWTH\"\n}\n\ngenerate_recommendations() {\n    # CPU recommendations\n    CPU_USAGE=$(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | sed 's/%us,//' | cut -d. -f1)\n    if [ \"$CPU_USAGE\" -gt 70 ]; then\n        echo \"- CPU: Consider upgrading to more cores (current usage: ${CPU_USAGE}%)\"\n    else\n        echo \"- CPU: Current capacity sufficient (usage: ${CPU_USAGE}%)\"\n    fi\n\n    # Memory recommendations\n    MEMORY_USAGE=$(free | grep Mem | awk '{printf \"%.0f\", $3/$2 * 100.0}')\n    if [ \"$MEMORY_USAGE\" -gt 80 ]; then\n        echo \"- Memory: Upgrade recommended (current usage: ${MEMORY_USAGE}%)\"\n    else\n        echo \"- Memory: Current capacity sufficient (usage: ${MEMORY_USAGE}%)\"\n    fi\n\n    # Storage recommendations\n    SSD_USAGE=$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\n    HDD_USAGE=$(df /mnt/hdd | awk 'NR==2 {print $5}' | sed 's/%//')\n\n    if [ \"$SSD_USAGE\" -gt 85 ]; then\n        echo \"- SSD Storage: Expansion needed (current usage: ${SSD_USAGE}%)\"\n    elif [ \"$SSD_USAGE\" -gt 70 ]; then\n        echo \"- SSD Storage: Plan expansion within 3 months (current usage: ${SSD_USAGE}%)\"\n    else\n        echo \"- SSD Storage: Current capacity sufficient (usage: ${SSD_USAGE}%)\"\n    fi\n\n    if [ \"$HDD_USAGE\" -gt 80 ]; then\n        echo \"- HDD Storage: Expansion recommended (current usage: ${HDD_USAGE}%)\"\n    else\n        echo \"- HDD Storage: Current capacity sufficient (usage: ${HDD_USAGE}%)\"\n    fi\n\n    # Database recommendations\n    DB_SIZE_GB=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT ROUND(pg_database_size('prs_production') / 1024.0 / 1024.0 / 1024.0, 2);\n    \" | xargs)\n\n    if (( $(echo \"$DB_SIZE_GB &gt; 100\" | bc -l) )); then\n        echo \"- Database: Consider partitioning optimization (size: ${DB_SIZE_GB}GB)\"\n    fi\n\n    # Connection recommendations\n    ACTIVE_CONNECTIONS=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT count(*) FROM pg_stat_activity;\n    \" | xargs)\n\n    if [ \"$ACTIVE_CONNECTIONS\" -gt 100 ]; then\n        echo \"- Database Connections: Consider connection pooling optimization (active: $ACTIVE_CONNECTIONS)\"\n    fi\n}\n\n# Execute capacity assessment\ngenerate_capacity_report\n\n# Email report if configured\nif command -v mail &gt;/dev/null 2&gt;&amp;1; then\n    mail -s \"PRS Capacity Assessment Report\" admin@your-domain.com &lt; \"$REPORT_FILE\"\nfi\n</code></pre>"},{"location":"maintenance/capacity/#growth-forecasting","title":"Growth Forecasting","text":""},{"location":"maintenance/capacity/#user-growth-projection","title":"User Growth Projection","text":"<pre><code>#!/bin/bash\n# User growth forecasting\n\nforecast_user_growth() {\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n    WITH monthly_growth AS (\n        SELECT\n            DATE_TRUNC('month', created_at) as month,\n            COUNT(*) as new_users,\n            SUM(COUNT(*)) OVER (ORDER BY DATE_TRUNC('month', created_at)) as cumulative_users\n        FROM users\n        WHERE created_at &gt;= NOW() - INTERVAL '12 months'\n        GROUP BY DATE_TRUNC('month', created_at)\n        ORDER BY month\n    ),\n    growth_rate AS (\n        SELECT\n            AVG(\n                CASE\n                    WHEN LAG(new_users) OVER (ORDER BY month) &gt; 0\n                    THEN (new_users::float / LAG(new_users) OVER (ORDER BY month) - 1) * 100\n                    ELSE 0\n                END\n            ) as avg_monthly_growth_rate\n        FROM monthly_growth\n    )\n    SELECT\n        'Current Users: ' || (SELECT COUNT(*) FROM users)::text ||\n        ', Avg Monthly Growth: ' || ROUND(avg_monthly_growth_rate, 2)::text || '%' ||\n        ', Projected 6-month: ' || ROUND((SELECT COUNT(*) FROM users) * POWER(1 + avg_monthly_growth_rate/100, 6))::text ||\n        ', Projected 12-month: ' || ROUND((SELECT COUNT(*) FROM users) * POWER(1 + avg_monthly_growth_rate/100, 12))::text\n    FROM growth_rate;\n    \"\n}\n</code></pre>"},{"location":"maintenance/capacity/#data-growth-projection","title":"Data Growth Projection","text":"<pre><code>#!/bin/bash\n# Data growth forecasting\n\nforecast_data_growth() {\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n    WITH monthly_data AS (\n        SELECT\n            DATE_TRUNC('month', range_start) as month,\n            SUM(chunk_size) as monthly_data_size\n        FROM timescaledb_information.chunks\n        WHERE range_start &gt;= NOW() - INTERVAL '6 months'\n        GROUP BY DATE_TRUNC('month', range_start)\n        ORDER BY month\n    ),\n    growth_analysis AS (\n        SELECT\n            AVG(monthly_data_size) as avg_monthly_growth,\n            pg_size_pretty(AVG(monthly_data_size)::bigint) as avg_monthly_growth_pretty\n        FROM monthly_data\n    )\n    SELECT\n        'Current DB Size: ' || pg_size_pretty(pg_database_size('prs_production')) ||\n        ', Avg Monthly Growth: ' || avg_monthly_growth_pretty ||\n        ', Projected 6-month: ' || pg_size_pretty((pg_database_size('prs_production') + avg_monthly_growth * 6)::bigint) ||\n        ', Projected 12-month: ' || pg_size_pretty((pg_database_size('prs_production') + avg_monthly_growth * 12)::bigint)\n    FROM growth_analysis;\n    \"\n}\n</code></pre>"},{"location":"maintenance/capacity/#scaling-recommendations","title":"Scaling Recommendations","text":""},{"location":"maintenance/capacity/#vertical-scaling-scale-up","title":"Vertical Scaling (Scale Up)","text":""},{"location":"maintenance/capacity/#cpu-scaling-guidelines","title":"CPU Scaling Guidelines","text":"<pre><code># CPU scaling decision matrix\ncheck_cpu_scaling_needs() {\n    CPU_USAGE=$(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | sed 's/%us,//' | cut -d. -f1)\n    CURRENT_CORES=$(nproc)\n\n    echo \"CPU Scaling Assessment:\"\n    echo \"Current cores: $CURRENT_CORES\"\n    echo \"Current usage: ${CPU_USAGE}%\"\n\n    if [ \"$CPU_USAGE\" -gt 80 ]; then\n        RECOMMENDED_CORES=$((CURRENT_CORES * 2))\n        echo \"RECOMMENDATION: Immediate CPU upgrade to $RECOMMENDED_CORES cores\"\n        echo \"REASON: High CPU utilization (${CPU_USAGE}%)\"\n    elif [ \"$CPU_USAGE\" -gt 70 ]; then\n        RECOMMENDED_CORES=$((CURRENT_CORES + CURRENT_CORES / 2))\n        echo \"RECOMMENDATION: Plan CPU upgrade to $RECOMMENDED_CORES cores within 3 months\"\n        echo \"REASON: Approaching CPU capacity (${CPU_USAGE}%)\"\n    else\n        echo \"RECOMMENDATION: Current CPU capacity sufficient\"\n    fi\n}\n</code></pre>"},{"location":"maintenance/capacity/#memory-scaling-guidelines","title":"Memory Scaling Guidelines","text":"<pre><code># Memory scaling decision matrix\ncheck_memory_scaling_needs() {\n    MEMORY_USAGE=$(free | grep Mem | awk '{printf \"%.0f\", $3/$2 * 100.0}')\n    CURRENT_MEMORY_GB=$(free -g | grep Mem | awk '{print $2}')\n\n    echo \"Memory Scaling Assessment:\"\n    echo \"Current memory: ${CURRENT_MEMORY_GB}GB\"\n    echo \"Current usage: ${MEMORY_USAGE}%\"\n\n    if [ \"$MEMORY_USAGE\" -gt 85 ]; then\n        RECOMMENDED_MEMORY=$((CURRENT_MEMORY_GB * 2))\n        echo \"RECOMMENDATION: Immediate memory upgrade to ${RECOMMENDED_MEMORY}GB\"\n        echo \"REASON: High memory utilization (${MEMORY_USAGE}%)\"\n    elif [ \"$MEMORY_USAGE\" -gt 75 ]; then\n        RECOMMENDED_MEMORY=$((CURRENT_MEMORY_GB + CURRENT_MEMORY_GB / 2))\n        echo \"RECOMMENDATION: Plan memory upgrade to ${RECOMMENDED_MEMORY}GB within 3 months\"\n        echo \"REASON: Approaching memory capacity (${MEMORY_USAGE}%)\"\n    else\n        echo \"RECOMMENDATION: Current memory capacity sufficient\"\n    fi\n}\n</code></pre>"},{"location":"maintenance/capacity/#storage-scaling-guidelines","title":"Storage Scaling Guidelines","text":"<pre><code># Storage scaling decision matrix\ncheck_storage_scaling_needs() {\n    SSD_USAGE=$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\n    HDD_USAGE=$(df /mnt/hdd | awk 'NR==2 {print $5}' | sed 's/%//')\n    SSD_SIZE_GB=$(df -BG /mnt/ssd | awk 'NR==2 {print $2}' | sed 's/G//')\n    HDD_SIZE_TB=$(df -BT /mnt/hdd | awk 'NR==2 {print $2}' | sed 's/T//')\n\n    echo \"Storage Scaling Assessment:\"\n    echo \"SSD: ${SSD_SIZE_GB}GB (${SSD_USAGE}% used)\"\n    echo \"HDD: ${HDD_SIZE_TB}TB (${HDD_USAGE}% used)\"\n\n    # SSD recommendations\n    if [ \"$SSD_USAGE\" -gt 90 ]; then\n        RECOMMENDED_SSD=$((SSD_SIZE_GB * 2))\n        echo \"RECOMMENDATION: Immediate SSD expansion to ${RECOMMENDED_SSD}GB\"\n        echo \"REASON: Critical SSD usage (${SSD_USAGE}%)\"\n    elif [ \"$SSD_USAGE\" -gt 80 ]; then\n        RECOMMENDED_SSD=$((SSD_SIZE_GB + SSD_SIZE_GB / 2))\n        echo \"RECOMMENDATION: Plan SSD expansion to ${RECOMMENDED_SSD}GB within 2 months\"\n        echo \"REASON: High SSD usage (${SSD_USAGE}%)\"\n    fi\n\n    # HDD recommendations\n    if [ \"$HDD_USAGE\" -gt 85 ]; then\n        RECOMMENDED_HDD=$((HDD_SIZE_TB + 1))\n        echo \"RECOMMENDATION: Plan HDD expansion to ${RECOMMENDED_HDD}TB\"\n        echo \"REASON: High HDD usage (${HDD_USAGE}%)\"\n    fi\n}\n</code></pre>"},{"location":"maintenance/capacity/#horizontal-scaling-scale-out","title":"Horizontal Scaling (Scale Out)","text":""},{"location":"maintenance/capacity/#load-balancing-setup","title":"Load Balancing Setup","text":"<pre><code># Horizontal scaling preparation\nprepare_horizontal_scaling() {\n    echo \"Horizontal Scaling Readiness Assessment:\"\n\n    # Check current load\n    CONCURRENT_USERS=$(docker exec prs-onprem-redis redis-cli -a \"$REDIS_PASSWORD\" eval \"return #redis.call('keys', 'session:*')\" 0)\n    echo \"Current concurrent users: $CONCURRENT_USERS\"\n\n    # Database connection analysis\n    DB_CONNECTIONS=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT count(*) FROM pg_stat_activity;\n    \" | xargs)\n    echo \"Current database connections: $DB_CONNECTIONS\"\n\n    # Recommendations\n    if [ \"$CONCURRENT_USERS\" -gt 150 ]; then\n        echo \"RECOMMENDATION: Consider horizontal scaling\"\n        echo \"- Add load balancer\"\n        echo \"- Deploy additional application instances\"\n        echo \"- Implement database read replicas\"\n    elif [ \"$CONCURRENT_USERS\" -gt 100 ]; then\n        echo \"RECOMMENDATION: Prepare for horizontal scaling\"\n        echo \"- Plan load balancer implementation\"\n        echo \"- Test application clustering\"\n    else\n        echo \"RECOMMENDATION: Vertical scaling sufficient for current load\"\n    fi\n}\n</code></pre>"},{"location":"maintenance/capacity/#resource-planning","title":"Resource Planning","text":""},{"location":"maintenance/capacity/#hardware-upgrade-planning","title":"Hardware Upgrade Planning","text":"<pre><code>#!/bin/bash\n# Hardware upgrade planning\n\ngenerate_upgrade_plan() {\n    CURRENT_DATE=$(date +%Y-%m-%d)\n    UPGRADE_PLAN=\"/tmp/hardware-upgrade-plan-$CURRENT_DATE.txt\"\n\n    cat &gt; \"$UPGRADE_PLAN\" &lt;&lt; EOF\nPRS Hardware Upgrade Plan\n=========================\nGenerated: $CURRENT_DATE\nPlanning Horizon: 12 months\n\nCURRENT CONFIGURATION\n--------------------\n- CPU: $(nproc) cores\n- Memory: $(free -h | grep Mem | awk '{print $2}')\n- SSD: $(df -h /mnt/ssd | awk 'NR==2 {print $2}')\n- HDD: $(df -h /mnt/hdd | awk 'NR==2 {print $2}')\n\nUPGRADE TIMELINE\n---------------\nEOF\n\n    # Generate upgrade timeline based on current usage\n    check_cpu_scaling_needs &gt;&gt; \"$UPGRADE_PLAN\"\n    echo \"\" &gt;&gt; \"$UPGRADE_PLAN\"\n    check_memory_scaling_needs &gt;&gt; \"$UPGRADE_PLAN\"\n    echo \"\" &gt;&gt; \"$UPGRADE_PLAN\"\n    check_storage_scaling_needs &gt;&gt; \"$UPGRADE_PLAN\"\n\n    # Budget estimation\n    echo \"\" &gt;&gt; \"$UPGRADE_PLAN\"\n    echo \"BUDGET ESTIMATION\" &gt;&gt; \"$UPGRADE_PLAN\"\n    echo \"-----------------\" &gt;&gt; \"$UPGRADE_PLAN\"\n    echo \"CPU Upgrade: \\$2,000 - \\$5,000\" &gt;&gt; \"$UPGRADE_PLAN\"\n    echo \"Memory Upgrade: \\$500 - \\$2,000\" &gt;&gt; \"$UPGRADE_PLAN\"\n    echo \"SSD Expansion: \\$1,000 - \\$3,000\" &gt;&gt; \"$UPGRADE_PLAN\"\n    echo \"HDD Expansion: \\$500 - \\$1,500\" &gt;&gt; \"$UPGRADE_PLAN\"\n    echo \"Load Balancer Setup: \\$3,000 - \\$8,000\" &gt;&gt; \"$UPGRADE_PLAN\"\n\n    echo \"Hardware upgrade plan generated: $UPGRADE_PLAN\"\n}\n</code></pre>"},{"location":"maintenance/capacity/#capacity-monitoring-dashboard","title":"Capacity Monitoring Dashboard","text":"<pre><code>#!/bin/bash\n# Capacity monitoring dashboard\n\ndisplay_capacity_dashboard() {\n    clear\n    echo \"======================================\"\n    echo \"    PRS Capacity Monitoring Dashboard\"\n    echo \"======================================\"\n    echo \"Last Updated: $(date)\"\n    echo \"\"\n\n    # System resources\n    echo \"SYSTEM RESOURCES\"\n    echo \"----------------\"\n    printf \"CPU Usage:    %3s%% \" \"$(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | sed 's/%us,//' | cut -d. -f1)\"\n    [ \"$(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | sed 's/%us,//' | cut -d. -f1)\" -gt 70 ] &amp;&amp; echo \"[WARNING]\" || echo \"[OK]\"\n\n    printf \"Memory Usage: %3s%% \" \"$(free | grep Mem | awk '{printf \"%.0f\", $3/$2 * 100.0}')\"\n    [ \"$(free | grep Mem | awk '{printf \"%.0f\", $3/$2 * 100.0}')\" -gt 80 ] &amp;&amp; echo \"[WARNING]\" || echo \"[OK]\"\n\n    printf \"SSD Usage:    %3s%% \" \"$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\"\n    [ \"$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\" -gt 85 ] &amp;&amp; echo \"[WARNING]\" || echo \"[OK]\"\n\n    printf \"HDD Usage:    %3s%% \" \"$(df /mnt/hdd | awk 'NR==2 {print $5}' | sed 's/%//')\"\n    [ \"$(df /mnt/hdd | awk 'NR==2 {print $5}' | sed 's/%//')\" -gt 80 ] &amp;&amp; echo \"[WARNING]\" || echo \"[OK]\"\n\n    echo \"\"\n\n    # Application metrics\n    echo \"APPLICATION METRICS\"\n    echo \"-------------------\"\n    ACTIVE_SESSIONS=$(docker exec prs-onprem-redis redis-cli -a \"$REDIS_PASSWORD\" eval \"return #redis.call('keys', 'session:*')\" 0 2&gt;/dev/null || echo \"N/A\")\n    echo \"Active Sessions: $ACTIVE_SESSIONS\"\n\n    DB_CONNECTIONS=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT count(*) FROM pg_stat_activity;\" 2&gt;/dev/null | xargs || echo \"N/A\")\n    echo \"DB Connections: $DB_CONNECTIONS\"\n\n    DB_SIZE=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT pg_size_pretty(pg_database_size('prs_production'));\" 2&gt;/dev/null | xargs || echo \"N/A\")\n    echo \"Database Size: $DB_SIZE\"\n\n    echo \"\"\n    echo \"Press Ctrl+C to exit\"\n    sleep 5\n}\n\n# Run dashboard in loop\nwhile true; do\n    display_capacity_dashboard\ndone\n</code></pre> <p>Capacity Planning Ready</p> <p>Your PRS deployment now has comprehensive capacity planning tools to monitor growth, forecast needs, and plan for scaling requirements.</p> <p>Proactive Planning</p> <p>Regular capacity assessments help prevent performance issues and ensure smooth scaling as your organization grows.</p> <p>Growth Monitoring</p> <p>Monitor growth trends closely and plan upgrades well in advance to avoid capacity constraints that could impact system performance.</p>"},{"location":"maintenance/routine/","title":"Routine Maintenance","text":""},{"location":"maintenance/routine/#overview","title":"Overview","text":"<p>This guide covers routine maintenance procedures for the PRS on-premises deployment to ensure optimal performance, reliability, and security through regular preventive maintenance tasks.</p>"},{"location":"maintenance/routine/#maintenance-schedule-overview","title":"Maintenance Schedule Overview","text":""},{"location":"maintenance/routine/#daily-tasks-automated","title":"Daily Tasks (Automated)","text":"<ul> <li>System Health Checks - Monitor system resources and service status</li> <li>Database Backups - Full daily backups with verification</li> <li>Log Rotation - Manage log file sizes and retention</li> <li>Performance Monitoring - Track key performance indicators</li> <li>Security Scans - Basic security health checks</li> </ul>"},{"location":"maintenance/routine/#weekly-tasks-semi-automated","title":"Weekly Tasks (Semi-automated)","text":"<ul> <li>Database Maintenance - VACUUM, ANALYZE, and index optimization</li> <li>TimescaleDB Optimization - Automatic compression, chunk optimization, and multi-tier storage management</li> <li>Storage Cleanup - Remove temporary files and compress old logs</li> <li>Security Updates - Apply critical security patches</li> <li>Performance Review - Analyze performance trends</li> <li>Backup Verification - Test backup restoration procedures</li> </ul>"},{"location":"maintenance/routine/#monthly-tasks-manual","title":"Monthly Tasks (Manual)","text":"<ul> <li>Comprehensive System Review - Full system health assessment</li> <li>Capacity Planning - Storage and performance capacity analysis</li> <li>Security Audit - Complete security configuration review</li> <li>Documentation Updates - Update procedures and configurations</li> <li>Disaster Recovery Testing - Test complete recovery procedures</li> </ul>"},{"location":"maintenance/routine/#daily-maintenance-procedures","title":"Daily Maintenance Procedures","text":""},{"location":"maintenance/routine/#automated-daily-tasks","title":"Automated Daily Tasks","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/daily-routine-maintenance.sh\n\nset -euo pipefail\n\nLOG_FILE=\"/var/log/prs-maintenance.log\"\nDATE=$(date +%Y%m%d_%H%M%S)\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nlog_message \"Starting daily routine maintenance\"\n\n# 1. System Health Check\nlog_message \"Running system health check\"\n/opt/prs-deployment/scripts/system-health-check.sh all &gt; /tmp/daily-health-$DATE.log\n\n# Check for critical issues\nif grep -q \"ERROR\" /tmp/daily-health-$DATE.log; then\n    log_message \"CRITICAL: Health check found errors\"\n    grep \"ERROR\" /tmp/daily-health-$DATE.log | mail -s \"PRS Critical Health Issues\" admin@your-domain.com\nfi\n\n# 2. Database Statistics Update\nlog_message \"Updating database statistics\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nANALYZE notifications;\nANALYZE audit_logs;\nANALYZE requisitions;\nANALYZE purchase_orders;\n\"\n\n# 3. Check Database Performance\nlog_message \"Checking database performance\"\nSLOW_QUERIES=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\nSELECT count(*) FROM pg_stat_statements\nWHERE mean_time &gt; 1000 AND calls &gt; 10;\n\" | xargs)\n\nif [ \"$SLOW_QUERIES\" -gt 5 ]; then\n    log_message \"WARNING: $SLOW_QUERIES slow queries detected\"\nfi\n\n# 4. Storage Monitoring\nlog_message \"Monitoring storage usage\"\nSSD_USAGE=$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\nHDD_USAGE=$(df /mnt/hdd | awk 'NR==2 {print $5}' | sed 's/%//')\n\nlog_message \"Storage usage - SSD: ${SSD_USAGE}%, HDD: ${HDD_USAGE}%\"\n\n# Trigger data movement if SSD usage is high\nif [ \"$SSD_USAGE\" -gt 85 ]; then\n    log_message \"High SSD usage, triggering data movement\"\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n    SELECT move_chunk(chunk_name, 'hdd_cold')\n    FROM timescaledb_information.chunks\n    WHERE range_start &lt; NOW() - INTERVAL '14 days'\n    AND tablespace_name = 'ssd_hot'\n    LIMIT 5;\n    \"\nfi\n\n# 5. Log File Management\nlog_message \"Managing log files\"\n# Compress logs older than 1 day\nfind /mnt/ssd/logs -name \"*.log\" -mtime +1 -exec gzip {} \\;\n\n# Move compressed logs older than 7 days to HDD\nfind /mnt/ssd/logs -name \"*.log.gz\" -mtime +7 -exec mv {} /mnt/hdd/logs/ \\;\n\n# Remove logs older than 90 days\nfind /mnt/hdd/logs -name \"*.log.gz\" -mtime +90 -delete\n\n# 6. Container Health Check\nlog_message \"Checking container health\"\nUNHEALTHY_CONTAINERS=$(docker ps --filter \"health=unhealthy\" --format \"{{.Names}}\" | wc -l)\n\nif [ \"$UNHEALTHY_CONTAINERS\" -gt 0 ]; then\n    log_message \"WARNING: $UNHEALTHY_CONTAINERS unhealthy containers\"\n    docker ps --filter \"health=unhealthy\" --format \"table {{.Names}}\\t{{.Status}}\"\nfi\n\n# 7. Security Check\nlog_message \"Running basic security check\"\n# Check for failed login attempts\nFAILED_LOGINS=$(grep \"Failed password\" /var/log/auth.log | grep \"$(date +%Y-%m-%d)\" | wc -l)\nif [ \"$FAILED_LOGINS\" -gt 20 ]; then\n    log_message \"WARNING: High number of failed login attempts: $FAILED_LOGINS\"\nfi\n\n# Check SSL certificate expiration\nCERT_DAYS=$(openssl x509 -in /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt -noout -checkend $((30*24*3600)) &amp;&amp; echo \"OK\" || echo \"EXPIRING\")\nif [ \"$CERT_DAYS\" = \"EXPIRING\" ]; then\n    log_message \"WARNING: SSL certificate expires within 30 days\"\nfi\n\nlog_message \"Daily routine maintenance completed\"\n\n# Generate daily summary\ncat &gt; /tmp/daily-summary-$DATE.txt &lt;&lt; EOF\nPRS Daily Maintenance Summary - $(date)\n========================================\n\nSystem Status:\n- SSD Usage: ${SSD_USAGE}%\n- HDD Usage: ${HDD_USAGE}%\n- Slow Queries: $SLOW_QUERIES\n- Unhealthy Containers: $UNHEALTHY_CONTAINERS\n- Failed Logins: $FAILED_LOGINS\n- SSL Certificate: $CERT_DAYS\n\nHealth Check: See /tmp/daily-health-$DATE.log\n\nNext Actions:\n$(if [ \"$SSD_USAGE\" -gt 85 ]; then echo \"- Monitor SSD usage closely\"; fi)\n$(if [ \"$SLOW_QUERIES\" -gt 5 ]; then echo \"- Review slow queries\"; fi)\n$(if [ \"$UNHEALTHY_CONTAINERS\" -gt 0 ]; then echo \"- Investigate unhealthy containers\"; fi)\nEOF\n\n# Email summary if configured\nif command -v mail &gt;/dev/null 2&gt;&amp;1; then\n    mail -s \"PRS Daily Maintenance Summary\" admin@your-domain.com &lt; /tmp/daily-summary-$DATE.txt\nfi\n</code></pre>"},{"location":"maintenance/routine/#daily-checklist","title":"Daily Checklist","text":"<pre><code>#!/bin/bash\n# Daily maintenance checklist\n\necho \"PRS Daily Maintenance Checklist - $(date)\"\necho \"==========================================\"\n\n# System Resources\necho \"1. System Resources:\"\necho \"   CPU: $(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | sed 's/%us,//')% usage\"\necho \"   Memory: $(free | grep Mem | awk '{printf \"%.1f\", $3/$2 * 100.0}')% usage\"\necho \"   SSD: $(df /mnt/ssd | awk 'NR==2 {print $5}') usage\"\necho \"   HDD: $(df /mnt/hdd | awk 'NR==2 {print $5}') usage\"\n\n# Service Status\necho -e \"\\n2. Service Status:\"\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml ps --format \"table {{.Service}}\\t{{.State}}\\t{{.Status}}\"\n\n# Database Health\necho -e \"\\n3. Database Health:\"\nDB_CONNECTIONS=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT count(*) FROM pg_stat_activity;\" | xargs)\necho \"   Active Connections: $DB_CONNECTIONS\"\n\nCACHE_HIT=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\nSELECT round(100.0 * sum(blks_hit) / nullif(sum(blks_hit) + sum(blks_read), 0), 2)\nFROM pg_stat_database WHERE datname = 'prs_production';\" | xargs)\necho \"   Cache Hit Ratio: ${CACHE_HIT}%\"\n\n# Application Health\necho -e \"\\n4. Application Health:\"\nAPI_STATUS=$(curl -s -o /dev/null -w \"%{http_code}\" https://localhost/api/health)\necho \"   API Status: $API_STATUS\"\n\nRESPONSE_TIME=$(curl -w \"%{time_total}\" -o /dev/null -s https://localhost/api/health)\necho \"   API Response Time: ${RESPONSE_TIME}s\"\n\n# Recent Errors\necho -e \"\\n5. Recent Errors (24h):\"\nERROR_COUNT=$(docker logs prs-onprem-backend --since 24h 2&gt;&amp;1 | grep -i error | wc -l)\necho \"   Backend Errors: $ERROR_COUNT\"\n\n# Backup Status\necho -e \"\\n6. Backup Status:\"\nLATEST_BACKUP=$(ls -t /mnt/hdd/postgres-backups/daily/*.sql* 2&gt;/dev/null | head -1)\nif [ -n \"$LATEST_BACKUP\" ]; then\n    BACKUP_AGE=$(( ($(date +%s) - $(stat -c %Y \"$LATEST_BACKUP\")) / 3600 ))\n    echo \"   Latest Backup: ${BACKUP_AGE}h ago\"\nelse\n    echo \"   Latest Backup: NOT FOUND\"\nfi\n\necho -e \"\\nDaily checklist completed.\"\n</code></pre>"},{"location":"maintenance/routine/#weekly-maintenance-procedures","title":"Weekly Maintenance Procedures","text":""},{"location":"maintenance/routine/#weekly-maintenance-script","title":"Weekly Maintenance Script","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/weekly-routine-maintenance.sh\n\nset -euo pipefail\n\nLOG_FILE=\"/var/log/prs-maintenance.log\"\nWEEK_NUMBER=$(date +%V)\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nlog_message \"Starting weekly routine maintenance (Week $WEEK_NUMBER)\"\n\n# 1. Database Maintenance\nlog_message \"Performing database maintenance\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSET maintenance_work_mem = '1GB';\nVACUUM (ANALYZE, VERBOSE) notifications;\nVACUUM (ANALYZE, VERBOSE) audit_logs;\nVACUUM (ANALYZE, VERBOSE) requisitions;\nVACUUM (ANALYZE, VERBOSE) purchase_orders;\n\"\n\n# 2. TimescaleDB Automated Maintenance\nlog_message \"Running TimescaleDB automated maintenance\"\n/opt/prs/prs-deployment/scripts/deploy-onprem.sh weekly-maintenance\n\n# 3. Index Maintenance\nlog_message \"Checking index usage\"\nUNUSED_INDEXES=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\nSELECT count(*) FROM pg_stat_user_indexes\nWHERE idx_scan = 0 AND pg_relation_size(indexrelid) &gt; 1024*1024;\n\" | xargs)\n\nlog_message \"Found $UNUSED_INDEXES unused indexes\"\n\n# 3. Storage Cleanup\nlog_message \"Performing storage cleanup\"\n# Clean Docker system\ndocker system prune -f\n\n# Clean temporary files\nfind /tmp -type f -mtime +7 -delete 2&gt;/dev/null || true\nfind /var/tmp -type f -mtime +7 -delete 2&gt;/dev/null || true\n\n# Compress old application logs\nfind /mnt/ssd/logs -name \"*.log\" -mtime +3 -exec gzip {} \\;\n\n# 4. TimescaleDB Optimization\nlog_message \"Optimizing TimescaleDB\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT compress_chunk(chunk_name)\nFROM timescaledb_information.chunks\nWHERE range_start &lt; NOW() - INTERVAL '7 days'\nAND NOT is_compressed\nAND hypertable_name IN ('notifications', 'audit_logs')\nLIMIT 10;\n\"\n\n# 5. Security Updates Check\nlog_message \"Checking for security updates\"\nSECURITY_UPDATES=$(apt list --upgradable 2&gt;/dev/null | grep -c security || echo \"0\")\nlog_message \"Available security updates: $SECURITY_UPDATES\"\n\nif [ \"$SECURITY_UPDATES\" -gt 0 ]; then\n    log_message \"Security updates available - manual review required\"\n    apt list --upgradable 2&gt;/dev/null | grep security &gt; /tmp/security-updates.txt\n\n    if command -v mail &gt;/dev/null 2&gt;&amp;1; then\n        mail -s \"PRS Security Updates Available\" admin@your-domain.com &lt; /tmp/security-updates.txt\n    fi\nfi\n\n# 6. Performance Analysis\nlog_message \"Analyzing performance trends\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT\n    'Top 5 Slow Queries (7 days)' as analysis;\n\nSELECT\n    left(query, 80) as query_snippet,\n    calls,\n    round(mean_time::numeric, 2) as avg_time_ms,\n    round(total_time::numeric, 2) as total_time_ms\nFROM pg_stat_statements\nWHERE calls &gt; 100\nORDER BY total_time DESC\nLIMIT 5;\n\" &gt; /tmp/weekly-performance-analysis.txt\n\n# 7. Backup Verification\nlog_message \"Verifying backup integrity\"\nLATEST_BACKUP=$(ls -t /mnt/hdd/postgres-backups/daily/*.sql* 2&gt;/dev/null | head -1)\nif [ -n \"$LATEST_BACKUP\" ]; then\n    if [ -f \"${LATEST_BACKUP}.sha256\" ]; then\n        if sha256sum -c \"${LATEST_BACKUP}.sha256\" &gt;/dev/null 2&gt;&amp;1; then\n            log_message \"Backup integrity verification passed\"\n        else\n            log_message \"ERROR: Backup integrity verification failed\"\n        fi\n    else\n        log_message \"WARNING: No checksum file for latest backup\"\n    fi\nfi\n\n# 8. Generate Weekly Report\nREPORT_FILE=\"/tmp/weekly-maintenance-report-week$WEEK_NUMBER.txt\"\ncat &gt; \"$REPORT_FILE\" &lt;&lt; EOF\nPRS Weekly Maintenance Report - Week $WEEK_NUMBER\nGenerated: $(date)\n================================================\n\nDatabase Maintenance:\n- VACUUM ANALYZE completed for main tables\n- Found $UNUSED_INDEXES unused indexes\n- TimescaleDB automated maintenance completed\n- Chunk optimization and compression updated\n- Multi-tier storage management active\n\nStorage Management:\n- Docker system cleanup completed\n- Temporary files cleaned\n- Log compression updated\n\nSecurity:\n- $SECURITY_UPDATES security updates available\n- SSL certificate status: $(openssl x509 -in /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt -noout -checkend $((30*24*3600)) &amp;&amp; echo \"Valid\" || echo \"Expiring soon\")\n\nPerformance:\n$(cat /tmp/weekly-performance-analysis.txt)\n\nSystem Health:\n- SSD Usage: $(df /mnt/ssd | awk 'NR==2 {print $5}')\n- HDD Usage: $(df /mnt/hdd | awk 'NR==2 {print $5}')\n- Memory Usage: $(free | grep Mem | awk '{printf \"%.1f\", $3/$2 * 100.0}')%\n- Load Average: $(uptime | awk -F'load average:' '{print $2}')\n\nRecommendations:\n$(if [ \"$UNUSED_INDEXES\" -gt 3 ]; then echo \"- Review and remove unused indexes\"; fi)\n$(if [ \"$SECURITY_UPDATES\" -gt 0 ]; then echo \"- Apply security updates during next maintenance window\"; fi)\n$(if [ \"$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\" -gt 80 ]; then echo \"- Monitor SSD usage and consider data archival\"; fi)\nEOF\n\nlog_message \"Weekly maintenance report generated: $REPORT_FILE\"\n\n# Email report\nif command -v mail &gt;/dev/null 2&gt;&amp;1; then\n    mail -s \"PRS Weekly Maintenance Report - Week $WEEK_NUMBER\" admin@your-domain.com &lt; \"$REPORT_FILE\"\nfi\n\nlog_message \"Weekly routine maintenance completed\"\n</code></pre>"},{"location":"maintenance/routine/#monthly-maintenance-procedures","title":"Monthly Maintenance Procedures","text":""},{"location":"maintenance/routine/#monthly-comprehensive-review","title":"Monthly Comprehensive Review","text":"<pre><code>#!/bin/bash\n# Monthly comprehensive maintenance\n\nMONTH=$(date +%Y%m)\nREPORT_FILE=\"/tmp/monthly-maintenance-report-$MONTH.txt\"\n\necho \"PRS Monthly Maintenance Report - $MONTH\" &gt; \"$REPORT_FILE\"\necho \"=======================================\" &gt;&gt; \"$REPORT_FILE\"\necho \"Generated: $(date)\" &gt;&gt; \"$REPORT_FILE\"\necho \"\" &gt;&gt; \"$REPORT_FILE\"\n\n# System Overview\necho \"SYSTEM OVERVIEW\" &gt;&gt; \"$REPORT_FILE\"\necho \"---------------\" &gt;&gt; \"$REPORT_FILE\"\necho \"Hostname: $(hostname)\" &gt;&gt; \"$REPORT_FILE\"\necho \"OS Version: $(lsb_release -d | cut -f2)\" &gt;&gt; \"$REPORT_FILE\"\necho \"Kernel: $(uname -r)\" &gt;&gt; \"$REPORT_FILE\"\necho \"Uptime: $(uptime -p)\" &gt;&gt; \"$REPORT_FILE\"\necho \"Docker Version: $(docker --version)\" &gt;&gt; \"$REPORT_FILE\"\necho \"\" &gt;&gt; \"$REPORT_FILE\"\n\n# Capacity Analysis\necho \"CAPACITY ANALYSIS\" &gt;&gt; \"$REPORT_FILE\"\necho \"-----------------\" &gt;&gt; \"$REPORT_FILE\"\necho \"Storage Usage:\" &gt;&gt; \"$REPORT_FILE\"\ndf -h /mnt/ssd /mnt/hdd &gt;&gt; \"$REPORT_FILE\"\necho \"\" &gt;&gt; \"$REPORT_FILE\"\n\necho \"Database Size Growth:\" &gt;&gt; \"$REPORT_FILE\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT\n    'Total Database Size: ' || pg_size_pretty(pg_database_size('prs_production'))\nUNION ALL\nSELECT\n    'Largest Tables:'\nUNION ALL\nSELECT\n    '  ' || tablename || ': ' || pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename))\nFROM pg_tables\nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\nLIMIT 5;\n\" &gt;&gt; \"$REPORT_FILE\"\n\n# Performance Summary\necho \"\" &gt;&gt; \"$REPORT_FILE\"\necho \"PERFORMANCE SUMMARY\" &gt;&gt; \"$REPORT_FILE\"\necho \"-------------------\" &gt;&gt; \"$REPORT_FILE\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT\n    'Cache Hit Ratio: ' || round(100.0 * sum(blks_hit) / nullif(sum(blks_hit) + sum(blks_read), 0), 2) || '%'\nFROM pg_stat_database\nWHERE datname = 'prs_production'\nUNION ALL\nSELECT\n    'Active Connections: ' || count(*)::text\nFROM pg_stat_activity\nUNION ALL\nSELECT\n    'Slow Queries (&gt;1s): ' || count(*)::text\nFROM pg_stat_statements\nWHERE mean_time &gt; 1000;\n\" &gt;&gt; \"$REPORT_FILE\"\n\n# Security Review\necho \"\" &gt;&gt; \"$REPORT_FILE\"\necho \"SECURITY REVIEW\" &gt;&gt; \"$REPORT_FILE\"\necho \"---------------\" &gt;&gt; \"$REPORT_FILE\"\necho \"SSL Certificate:\" &gt;&gt; \"$REPORT_FILE\"\nopenssl x509 -in /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt -noout -dates &gt;&gt; \"$REPORT_FILE\"\n\necho \"\" &gt;&gt; \"$REPORT_FILE\"\necho \"Failed Login Attempts (30 days):\" &gt;&gt; \"$REPORT_FILE\"\ngrep \"Failed password\" /var/log/auth.log | grep \"$(date +%Y-%m)\" | wc -l &gt;&gt; \"$REPORT_FILE\"\n\n# Recommendations\necho \"\" &gt;&gt; \"$REPORT_FILE\"\necho \"RECOMMENDATIONS\" &gt;&gt; \"$REPORT_FILE\"\necho \"---------------\" &gt;&gt; \"$REPORT_FILE\"\n\nSSD_USAGE=$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\nif [ \"$SSD_USAGE\" -gt 75 ]; then\n    echo \"- Consider SSD capacity expansion (current: ${SSD_USAGE}%)\" &gt;&gt; \"$REPORT_FILE\"\nfi\n\nSECURITY_UPDATES=$(apt list --upgradable 2&gt;/dev/null | grep -c security || echo \"0\")\nif [ \"$SECURITY_UPDATES\" -gt 0 ]; then\n    echo \"- Apply $SECURITY_UPDATES pending security updates\" &gt;&gt; \"$REPORT_FILE\"\nfi\n\necho \"- Review and update documentation\" &gt;&gt; \"$REPORT_FILE\"\necho \"- Test disaster recovery procedures\" &gt;&gt; \"$REPORT_FILE\"\necho \"- Review user access permissions\" &gt;&gt; \"$REPORT_FILE\"\n\necho \"\" &gt;&gt; \"$REPORT_FILE\"\necho \"Report completed: $(date)\" &gt;&gt; \"$REPORT_FILE\"\n\n# Email monthly report\nif command -v mail &gt;/dev/null 2&gt;&amp;1; then\n    mail -s \"PRS Monthly Maintenance Report - $MONTH\" admin@your-domain.com &lt; \"$REPORT_FILE\"\nfi\n\necho \"Monthly maintenance report generated: $REPORT_FILE\"\n</code></pre>"},{"location":"maintenance/routine/#maintenance-automation","title":"Maintenance Automation","text":""},{"location":"maintenance/routine/#cron-schedule-setup","title":"Cron Schedule Setup","text":"<pre><code>#!/bin/bash\n# Setup comprehensive maintenance schedule\n\n(crontab -l 2&gt;/dev/null; cat &lt;&lt; 'EOF'\n# PRS Routine Maintenance Schedule\n\n# Daily maintenance at 1:00 AM\n0 1 * * * /opt/prs-deployment/scripts/daily-routine-maintenance.sh\n\n# Weekly maintenance on Sunday at 2:00 AM (includes TimescaleDB automation)\n0 2 * * 0 /opt/prs-deployment/scripts/weekly-routine-maintenance.sh\n\n# TimescaleDB status check daily at 8:00 AM\n0 8 * * * /opt/prs/prs-deployment/scripts/deploy-onprem.sh timescaledb-status &gt;&gt; /var/log/timescaledb-status.log 2&gt;&amp;1\n\n# Monthly maintenance on first Sunday at 3:00 AM\n0 3 1-7 * 0 /opt/prs-deployment/scripts/monthly-comprehensive-review.sh\n\n# Health checks every 5 minutes\n*/5 * * * * /opt/prs-deployment/scripts/system-health-check.sh &gt;/dev/null 2&gt;&amp;1\n\n# Performance monitoring every hour\n0 * * * * /opt/prs-deployment/scripts/performance-monitor.sh\n\n# Log rotation daily at 6:00 AM\n0 6 * * * /opt/prs-deployment/scripts/log-rotation.sh\nEOF\n) | crontab -\n\necho \"Maintenance schedule configured successfully\"\ncrontab -l\n</code></pre>"},{"location":"maintenance/routine/#maintenance-status-dashboard","title":"Maintenance Status Dashboard","text":"<pre><code>#!/bin/bash\n# Maintenance status dashboard\n\necho \"PRS Maintenance Status Dashboard\"\necho \"================================\"\necho \"Generated: $(date)\"\necho \"\"\n\n# Last maintenance runs\necho \"LAST MAINTENANCE RUNS:\"\necho \"----------------------\"\nif [ -f /var/log/prs-maintenance.log ]; then\n    echo \"Daily: $(grep \"daily routine maintenance completed\" /var/log/prs-maintenance.log | tail -1 | cut -d' ' -f1-2)\"\n    echo \"Weekly: $(grep \"weekly routine maintenance completed\" /var/log/prs-maintenance.log | tail -1 | cut -d' ' -f1-2)\"\n    echo \"Monthly: $(grep \"monthly.*completed\" /var/log/prs-maintenance.log | tail -1 | cut -d' ' -f1-2)\"\nelse\n    echo \"No maintenance log found\"\nfi\n\necho \"\"\n\n# Current system status\necho \"CURRENT SYSTEM STATUS:\"\necho \"----------------------\"\necho \"Uptime: $(uptime -p)\"\necho \"Load: $(uptime | awk -F'load average:' '{print $2}')\"\necho \"Memory: $(free -h | grep Mem | awk '{print $3 \"/\" $2}')\"\necho \"SSD: $(df -h /mnt/ssd | awk 'NR==2 {print $3 \"/\" $2 \" (\" $5 \")\"}')\"\necho \"HDD: $(df -h /mnt/hdd | awk 'NR==2 {print $3 \"/\" $2 \" (\" $5 \")\"}')\"\n\necho \"\"\n\n# Service status\necho \"SERVICE STATUS:\"\necho \"---------------\"\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml ps --format \"table {{.Service}}\\t{{.State}}\"\n\necho \"\"\n\n# Next scheduled maintenance\necho \"NEXT SCHEDULED MAINTENANCE:\"\necho \"---------------------------\"\necho \"Daily: Tomorrow at 1:00 AM\"\necho \"Weekly: Next Sunday at 2:00 AM\"\necho \"Monthly: First Sunday of next month at 3:00 AM\"\n</code></pre>"},{"location":"maintenance/routine/#timescaledb-specific-maintenance","title":"TimescaleDB-Specific Maintenance","text":"<p>For detailed TimescaleDB maintenance procedures, automation, and troubleshooting:</p> <ul> <li>TimescaleDB Automation Guide - Comprehensive automation strategy</li> <li>TimescaleDB Quick Reference - Quick commands and troubleshooting</li> </ul>"},{"location":"maintenance/routine/#key-timescaledb-maintenance-points","title":"Key TimescaleDB Maintenance Points","text":"<ul> <li>Automated Weekly: Chunk optimization, compression, and multi-tier storage management</li> <li>Monitoring: Daily status checks and performance monitoring</li> <li>Manual Intervention: Rarely needed - only for troubleshooting or configuration changes</li> </ul> <p>Routine Maintenance Configured</p> <p>Your PRS deployment now has comprehensive routine maintenance procedures with automated daily, weekly, and monthly tasks to ensure optimal system health, including intelligent TimescaleDB automation.</p> <p>Maintenance Windows</p> <p>Schedule intensive maintenance tasks during low-usage periods and always notify users of planned maintenance activities.</p> <p>Monitoring Required</p> <p>Always monitor maintenance job execution and review logs regularly to ensure all tasks complete successfully and identify any issues early.</p>"},{"location":"maintenance/security/","title":"Security Maintenance","text":""},{"location":"maintenance/security/#overview","title":"Overview","text":"<p>This guide covers security maintenance procedures for the PRS on-premises deployment, including security updates, vulnerability assessments, access reviews, and compliance monitoring.</p>"},{"location":"maintenance/security/#security-maintenance-schedule","title":"Security Maintenance Schedule","text":""},{"location":"maintenance/security/#daily-security-tasks","title":"Daily Security Tasks","text":"<ul> <li>Security Log Review - Monitor authentication and access logs</li> <li>Failed Login Analysis - Investigate suspicious login attempts</li> <li>Certificate Monitoring - Check SSL certificate status</li> <li>Vulnerability Scanning - Automated security scans</li> <li>Backup Verification - Ensure security of backup data</li> </ul>"},{"location":"maintenance/security/#weekly-security-tasks","title":"Weekly Security Tasks","text":"<ul> <li>Access Review - Review user permissions and access rights</li> <li>Security Updates - Apply critical security patches</li> <li>Firewall Log Analysis - Review network security logs</li> <li>Intrusion Detection - Check for security incidents</li> <li>Security Configuration Audit - Verify security settings</li> </ul>"},{"location":"maintenance/security/#monthly-security-tasks","title":"Monthly Security Tasks","text":"<ul> <li>Comprehensive Security Audit - Full security assessment</li> <li>Penetration Testing - Security vulnerability testing</li> <li>Compliance Review - Ensure regulatory compliance</li> <li>Security Training - Update security procedures</li> <li>Incident Response Testing - Test security response procedures</li> </ul>"},{"location":"maintenance/security/#daily-security-procedures","title":"Daily Security Procedures","text":""},{"location":"maintenance/security/#security-log-monitoring","title":"Security Log Monitoring","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/daily-security-check.sh\n\nset -euo pipefail\n\nLOG_FILE=\"/var/log/prs-security.log\"\nALERT_EMAIL=\"security@your-domain.com\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\n# Check failed login attempts\ncheck_failed_logins() {\n    log_message \"Checking failed login attempts\"\n\n    # Check system authentication logs\n    FAILED_SSH=$(grep \"Failed password\" /var/log/auth.log | grep \"$(date +%Y-%m-%d)\" | wc -l)\n    FAILED_APP=$(docker logs prs-onprem-backend --since 24h 2&gt;&amp;1 | grep -i \"authentication failed\" | wc -l)\n\n    log_message \"Failed SSH logins: $FAILED_SSH\"\n    log_message \"Failed app logins: $FAILED_APP\"\n\n    # Alert on high failure rates\n    if [ \"$FAILED_SSH\" -gt 20 ]; then\n        log_message \"WARNING: High SSH login failures detected\"\n        echo \"High SSH login failures: $FAILED_SSH attempts\" | \\\n        mail -s \"PRS Security Alert: High SSH Failures\" \"$ALERT_EMAIL\"\n    fi\n\n    if [ \"$FAILED_APP\" -gt 50 ]; then\n        log_message \"WARNING: High application login failures detected\"\n        echo \"High application login failures: $FAILED_APP attempts\" | \\\n        mail -s \"PRS Security Alert: High App Failures\" \"$ALERT_EMAIL\"\n    fi\n}\n\n# Check SSL certificate status\ncheck_ssl_certificates() {\n    log_message \"Checking SSL certificate status\"\n\n    CERT_FILE=\"/opt/prs/prs-deployment/02-docker-configuration/ssl/certificate.crt\"\n    if [ -f \"$CERT_FILE\" ]; then\n        EXPIRY_DATE=$(openssl x509 -in \"$CERT_FILE\" -noout -enddate | cut -d= -f2)\n        EXPIRY_EPOCH=$(date -d \"$EXPIRY_DATE\" +%s)\n        CURRENT_EPOCH=$(date +%s)\n        DAYS_UNTIL_EXPIRY=$(( (EXPIRY_EPOCH - CURRENT_EPOCH) / 86400 ))\n\n        log_message \"SSL certificate expires in $DAYS_UNTIL_EXPIRY days\"\n\n        if [ \"$DAYS_UNTIL_EXPIRY\" -lt 7 ]; then\n            log_message \"CRITICAL: SSL certificate expires in $DAYS_UNTIL_EXPIRY days\"\n            echo \"SSL certificate expires in $DAYS_UNTIL_EXPIRY days\" | \\\n            mail -s \"PRS Security Alert: SSL Certificate Expiring\" \"$ALERT_EMAIL\"\n        elif [ \"$DAYS_UNTIL_EXPIRY\" -lt 30 ]; then\n            log_message \"WARNING: SSL certificate expires in $DAYS_UNTIL_EXPIRY days\"\n        fi\n    else\n        log_message \"ERROR: SSL certificate file not found\"\n    fi\n}\n\n# Check for suspicious database activity\ncheck_database_security() {\n    log_message \"Checking database security\"\n\n    # Check for unusual connection patterns\n    UNUSUAL_CONNECTIONS=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT COUNT(*) FROM pg_stat_activity \n    WHERE client_addr IS NOT NULL \n    AND client_addr NOT IN ('127.0.0.1', '::1')\n    AND application_name NOT LIKE 'prs-%';\n    \" | xargs)\n\n    if [ \"$UNUSUAL_CONNECTIONS\" -gt 0 ]; then\n        log_message \"WARNING: $UNUSUAL_CONNECTIONS unusual database connections detected\"\n    fi\n\n    # Check for failed database connections\n    FAILED_DB_CONNECTIONS=$(docker logs prs-onprem-postgres-timescale --since 24h 2&gt;&amp;1 | grep -i \"authentication failed\" | wc -l)\n    log_message \"Failed database connections: $FAILED_DB_CONNECTIONS\"\n\n    if [ \"$FAILED_DB_CONNECTIONS\" -gt 10 ]; then\n        log_message \"WARNING: High database authentication failures\"\n        echo \"High database authentication failures: $FAILED_DB_CONNECTIONS\" | \\\n        mail -s \"PRS Security Alert: Database Auth Failures\" \"$ALERT_EMAIL\"\n    fi\n}\n\n# Check system integrity\ncheck_system_integrity() {\n    log_message \"Checking system integrity\"\n\n    # Check for unauthorized file changes\n    if command -v aide &gt;/dev/null 2&gt;&amp;1; then\n        aide --check &gt; /tmp/aide-check.log 2&gt;&amp;1\n        if [ $? -ne 0 ]; then\n            log_message \"WARNING: System integrity check failed\"\n            mail -s \"PRS Security Alert: System Integrity\" \"$ALERT_EMAIL\" &lt; /tmp/aide-check.log\n        fi\n    fi\n\n    # Check for rootkits\n    if command -v rkhunter &gt;/dev/null 2&gt;&amp;1; then\n        rkhunter --check --skip-keypress &gt; /tmp/rkhunter.log 2&gt;&amp;1\n        if grep -q \"Warning\" /tmp/rkhunter.log; then\n            log_message \"WARNING: Rootkit scanner found issues\"\n            mail -s \"PRS Security Alert: Rootkit Check\" \"$ALERT_EMAIL\" &lt; /tmp/rkhunter.log\n        fi\n    fi\n}\n\n# Main execution\nmain() {\n    log_message \"Starting daily security check\"\n\n    check_failed_logins\n    check_ssl_certificates\n    check_database_security\n    check_system_integrity\n\n    log_message \"Daily security check completed\"\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"maintenance/security/#access-control-monitoring","title":"Access Control Monitoring","text":"<pre><code>#!/bin/bash\n# Monitor user access and permissions\n\n# Check active user sessions\nACTIVE_SESSIONS=$(docker exec prs-onprem-redis redis-cli -a \"$REDIS_PASSWORD\" eval \"return #redis.call('keys', 'session:*')\" 0)\necho \"Active user sessions: $ACTIVE_SESSIONS\"\n\n# Check privileged user activity\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT \n    u.username,\n    COUNT(*) as actions_today\nFROM audit_logs al\nJOIN users u ON al.user_id = u.id\nWHERE al.created_at &gt;= CURRENT_DATE\nAND u.role IN ('admin', 'super_admin')\nGROUP BY u.username\nORDER BY actions_today DESC;\n\"\n\n# Check for new user registrations\nNEW_USERS=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\nSELECT COUNT(*) FROM users WHERE created_at &gt;= CURRENT_DATE;\n\" | xargs)\n\necho \"New user registrations today: $NEW_USERS\"\n</code></pre>"},{"location":"maintenance/security/#weekly-security-procedures","title":"Weekly Security Procedures","text":""},{"location":"maintenance/security/#security-updates-management","title":"Security Updates Management","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/weekly-security-updates.sh\n\nset -euo pipefail\n\nLOG_FILE=\"/var/log/prs-security.log\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nmain() {\n    log_message \"Starting weekly security updates\"\n\n    # Check for security updates\n    apt update\n    SECURITY_UPDATES=$(apt list --upgradable 2&gt;/dev/null | grep security | wc -l)\n\n    log_message \"Available security updates: $SECURITY_UPDATES\"\n\n    if [ \"$SECURITY_UPDATES\" -gt 0 ]; then\n        log_message \"Applying security updates\"\n\n        # Create pre-update backup\n        /opt/prs-deployment/scripts/backup-full.sh\n\n        # Apply security updates\n        DEBIAN_FRONTEND=noninteractive apt-get -y upgrade\n\n        # Check if reboot required\n        if [ -f /var/run/reboot-required ]; then\n            log_message \"System reboot required after security updates\"\n            echo \"System reboot required after security updates\" | \\\n            mail -s \"PRS Security: Reboot Required\" admin@your-domain.com\n        fi\n\n        # Restart services\n        systemctl restart docker\n        docker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml restart\n\n        log_message \"Security updates completed\"\n    else\n        log_message \"No security updates available\"\n    fi\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"maintenance/security/#access-review-procedures","title":"Access Review Procedures","text":"<pre><code>#!/bin/bash\n# Weekly access review\n\n# Generate user access report\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT \n    u.username,\n    u.email,\n    u.role,\n    u.last_login_at,\n    CASE \n        WHEN u.last_login_at &lt; NOW() - INTERVAL '30 days' THEN 'Inactive'\n        WHEN u.last_login_at &lt; NOW() - INTERVAL '7 days' THEN 'Low Activity'\n        ELSE 'Active'\n    END as activity_status,\n    u.created_at\nFROM users u\nORDER BY u.last_login_at DESC NULLS LAST;\n\" &gt; /tmp/user-access-report.csv\n\n# Check for inactive users\nINACTIVE_USERS=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\nSELECT COUNT(*) FROM users \nWHERE last_login_at &lt; NOW() - INTERVAL '90 days'\nOR last_login_at IS NULL;\n\" | xargs)\n\necho \"Inactive users (90+ days): $INACTIVE_USERS\"\n\n# Email access report\nif command -v mail &gt;/dev/null 2&gt;&amp;1; then\n    mail -s \"PRS Weekly Access Review\" admin@your-domain.com &lt; /tmp/user-access-report.csv\nfi\n</code></pre>"},{"location":"maintenance/security/#monthly-security-procedures","title":"Monthly Security Procedures","text":""},{"location":"maintenance/security/#comprehensive-security-audit","title":"Comprehensive Security Audit","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/monthly-security-audit.sh\n\nset -euo pipefail\n\nAUDIT_DATE=$(date +%Y%m)\nAUDIT_REPORT=\"/tmp/security-audit-$AUDIT_DATE.txt\"\n\n# Generate comprehensive security audit report\ncat &gt; \"$AUDIT_REPORT\" &lt;&lt; EOF\nPRS Security Audit Report - $AUDIT_DATE\n========================================\nGenerated: $(date)\n\nSYSTEM SECURITY STATUS\n----------------------\nEOF\n\n# Check system hardening\necho \"System Hardening:\" &gt;&gt; \"$AUDIT_REPORT\"\necho \"- SSH root login: $(grep \"^PermitRootLogin\" /etc/ssh/sshd_config || echo \"Not configured\")\" &gt;&gt; \"$AUDIT_REPORT\"\necho \"- Password authentication: $(grep \"^PasswordAuthentication\" /etc/ssh/sshd_config || echo \"Not configured\")\" &gt;&gt; \"$AUDIT_REPORT\"\necho \"- Firewall status: $(ufw status | head -1)\" &gt;&gt; \"$AUDIT_REPORT\"\n\n# Check SSL configuration\necho \"\" &gt;&gt; \"$AUDIT_REPORT\"\necho \"SSL/TLS Configuration:\" &gt;&gt; \"$AUDIT_REPORT\"\nif [ -f \"/opt/prs-deployment/02-docker-configuration/ssl/certificate.crt\" ]; then\n    CERT_SUBJECT=$(openssl x509 -in /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt -noout -subject)\n    CERT_EXPIRY=$(openssl x509 -in /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt -noout -enddate)\n    echo \"- Certificate subject: $CERT_SUBJECT\" &gt;&gt; \"$AUDIT_REPORT\"\n    echo \"- Certificate expiry: $CERT_EXPIRY\" &gt;&gt; \"$AUDIT_REPORT\"\nelse\n    echo \"- SSL certificate: NOT FOUND\" &gt;&gt; \"$AUDIT_REPORT\"\nfi\n\n# Check database security\necho \"\" &gt;&gt; \"$AUDIT_REPORT\"\necho \"Database Security:\" &gt;&gt; \"$AUDIT_REPORT\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT \n    'Total users: ' || COUNT(*)\nFROM users\nUNION ALL\nSELECT \n    'Admin users: ' || COUNT(*)\nFROM users \nWHERE role IN ('admin', 'super_admin')\nUNION ALL\nSELECT \n    'Inactive users (90+ days): ' || COUNT(*)\nFROM users \nWHERE last_login_at &lt; NOW() - INTERVAL '90 days'\nOR last_login_at IS NULL;\n\" &gt;&gt; \"$AUDIT_REPORT\"\n\n# Check application security\necho \"\" &gt;&gt; \"$AUDIT_REPORT\"\necho \"Application Security:\" &gt;&gt; \"$AUDIT_REPORT\"\necho \"- Authentication failures (30 days): $(docker logs prs-onprem-backend --since 720h 2&gt;&amp;1 | grep -i \"authentication failed\" | wc -l)\" &gt;&gt; \"$AUDIT_REPORT\"\necho \"- Active sessions: $(docker exec prs-onprem-redis redis-cli -a \"$REDIS_PASSWORD\" eval \"return #redis.call('keys', 'session:*')\" 0)\" &gt;&gt; \"$AUDIT_REPORT\"\n\n# Security recommendations\necho \"\" &gt;&gt; \"$AUDIT_REPORT\"\necho \"SECURITY RECOMMENDATIONS\" &gt;&gt; \"$AUDIT_REPORT\"\necho \"------------------------\" &gt;&gt; \"$AUDIT_REPORT\"\n\n# Check for common security issues\nif grep -q \"PermitRootLogin yes\" /etc/ssh/sshd_config 2&gt;/dev/null; then\n    echo \"- CRITICAL: Disable SSH root login\" &gt;&gt; \"$AUDIT_REPORT\"\nfi\n\nif [ \"$INACTIVE_USERS\" -gt 5 ]; then\n    echo \"- WARNING: Review and disable inactive user accounts\" &gt;&gt; \"$AUDIT_REPORT\"\nfi\n\n# Email security audit report\nif command -v mail &gt;/dev/null 2&gt;&amp;1; then\n    mail -s \"PRS Monthly Security Audit Report\" admin@your-domain.com &lt; \"$AUDIT_REPORT\"\nfi\n\necho \"Security audit completed: $AUDIT_REPORT\"\n</code></pre>"},{"location":"maintenance/security/#vulnerability-assessment","title":"Vulnerability Assessment","text":"<pre><code>#!/bin/bash\n# Vulnerability assessment procedures\n\n# Check for known vulnerabilities\nif command -v nmap &gt;/dev/null 2&gt;&amp;1; then\n    echo \"Running network vulnerability scan...\"\n    nmap -sV --script vuln localhost &gt; /tmp/vuln-scan.txt\nfi\n\n# Check Docker security\necho \"Checking Docker security...\"\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\\n    -v /usr/local/bin/docker:/usr/local/bin/docker \\\n    aquasec/trivy image prs-backend:latest &gt; /tmp/docker-security.txt\n\n# Check for outdated packages\necho \"Checking for outdated packages...\"\napt list --upgradable &gt; /tmp/outdated-packages.txt\n\n# Generate vulnerability report\ncat &gt; /tmp/vulnerability-report.txt &lt;&lt; EOF\nPRS Vulnerability Assessment Report\n===================================\nDate: $(date)\n\nNetwork Vulnerabilities:\n$(cat /tmp/vuln-scan.txt 2&gt;/dev/null || echo \"Network scan not available\")\n\nDocker Security Issues:\n$(cat /tmp/docker-security.txt 2&gt;/dev/null || echo \"Docker scan not available\")\n\nOutdated Packages:\n$(cat /tmp/outdated-packages.txt)\nEOF\n\n# Email vulnerability report\nif command -v mail &gt;/dev/null 2&gt;&amp;1; then\n    mail -s \"PRS Vulnerability Assessment\" security@your-domain.com &lt; /tmp/vulnerability-report.txt\nfi\n</code></pre>"},{"location":"maintenance/security/#security-incident-response","title":"Security Incident Response","text":""},{"location":"maintenance/security/#incident-detection","title":"Incident Detection","text":"<pre><code>#!/bin/bash\n# Security incident detection\n\n# Check for brute force attacks\nBRUTE_FORCE_THRESHOLD=50\nFAILED_LOGINS=$(grep \"Failed password\" /var/log/auth.log | grep \"$(date +%Y-%m-%d)\" | wc -l)\n\nif [ \"$FAILED_LOGINS\" -gt \"$BRUTE_FORCE_THRESHOLD\" ]; then\n    echo \"SECURITY INCIDENT: Possible brute force attack detected\"\n    echo \"Failed login attempts: $FAILED_LOGINS\"\n\n    # Block suspicious IPs\n    grep \"Failed password\" /var/log/auth.log | grep \"$(date +%Y-%m-%d)\" | \\\n    awk '{print $(NF-3)}' | sort | uniq -c | sort -nr | head -10 | \\\n    while read count ip; do\n        if [ \"$count\" -gt 10 ]; then\n            echo \"Blocking IP: $ip (attempts: $count)\"\n            ufw deny from \"$ip\"\n        fi\n    done\nfi\n\n# Check for unusual database activity\nUNUSUAL_QUERIES=$(docker logs prs-onprem-postgres-timescale --since 1h 2&gt;&amp;1 | grep -i \"drop\\|delete\\|truncate\" | wc -l)\nif [ \"$UNUSUAL_QUERIES\" -gt 5 ]; then\n    echo \"SECURITY INCIDENT: Unusual database activity detected\"\n    echo \"Destructive queries in last hour: $UNUSUAL_QUERIES\"\nfi\n</code></pre>"},{"location":"maintenance/security/#incident-response-procedures","title":"Incident Response Procedures","text":"<pre><code>#!/bin/bash\n# Security incident response\n\nincident_response() {\n    local incident_type=\"$1\"\n    local severity=\"$2\"\n\n    echo \"SECURITY INCIDENT DETECTED\"\n    echo \"Type: $incident_type\"\n    echo \"Severity: $severity\"\n    echo \"Time: $(date)\"\n\n    # Log incident\n    echo \"$(date): INCIDENT [$severity] $incident_type\" &gt;&gt; /var/log/prs-security-incidents.log\n\n    # Immediate response actions\n    case \"$severity\" in\n        \"CRITICAL\")\n            # Isolate system\n            echo \"Implementing emergency security measures...\"\n\n            # Block all external access\n            ufw --force reset\n            ufw default deny incoming\n            ufw default deny outgoing\n            ufw allow from 192.168.0.0/16\n            ufw --force enable\n\n            # Stop non-essential services\n            docker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml stop frontend\n\n            # Create forensic backup\n            /opt/prs-deployment/scripts/backup-full.sh\n            ;;\n        \"HIGH\")\n            # Enhanced monitoring\n            echo \"Implementing enhanced security monitoring...\"\n\n            # Reduce session timeouts\n            # Force password resets for admin users\n            ;;\n        \"MEDIUM\")\n            # Standard response\n            echo \"Implementing standard security response...\"\n            ;;\n    esac\n\n    # Notify security team\n    echo \"Security incident: $incident_type ($severity)\" | \\\n    mail -s \"PRS SECURITY INCIDENT: $severity\" security@your-domain.com\n}\n</code></pre>"},{"location":"maintenance/security/#compliance-and-auditing","title":"Compliance and Auditing","text":""},{"location":"maintenance/security/#compliance-monitoring","title":"Compliance Monitoring","text":"<pre><code>#!/bin/bash\n# Compliance monitoring procedures\n\n# Check data retention compliance\necho \"Checking data retention compliance...\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT \n    'Audit logs older than 7 years: ' || COUNT(*)\nFROM audit_logs \nWHERE created_at &lt; NOW() - INTERVAL '7 years'\nUNION ALL\nSELECT \n    'User data without consent: ' || COUNT(*)\nFROM users \nWHERE consent_date IS NULL\nAND created_at &lt; NOW() - INTERVAL '30 days';\n\"\n\n# Check access control compliance\necho \"Checking access control compliance...\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT \n    'Users without role assignment: ' || COUNT(*)\nFROM users \nWHERE role IS NULL\nUNION ALL\nSELECT \n    'Admin users without MFA: ' || COUNT(*)\nFROM users \nWHERE role IN ('admin', 'super_admin')\nAND mfa_enabled = false;\n\"\n</code></pre> <p>Security Maintenance Ready</p> <p>Your PRS deployment now has comprehensive security maintenance procedures covering daily monitoring, weekly updates, monthly audits, and incident response.</p> <p>Proactive Security</p> <p>Regular security maintenance helps prevent incidents and ensures compliance with security standards and regulations.</p> <p>Incident Response</p> <p>Ensure all team members are familiar with incident response procedures and have access to emergency contact information.</p>"},{"location":"maintenance/timescaledb-automation/","title":"TimescaleDB Automation &amp; Maintenance","text":""},{"location":"maintenance/timescaledb-automation/#overview","title":"Overview","text":"<p>This document provides comprehensive guidance for automating TimescaleDB maintenance in production environments. The PRS deployment includes intelligent automation for multi-tier storage management, compression, and data lifecycle policies.</p>"},{"location":"maintenance/timescaledb-automation/#automation-strategy","title":"Automation Strategy","text":""},{"location":"maintenance/timescaledb-automation/#what-timescaledb-does-automatically","title":"What TimescaleDB Does Automatically","text":"<ul> <li>Chunk Creation - Automatic based on time intervals</li> <li>Compression - Automatic (once policies are set)</li> <li>Retention - Automatic (once policies are set)</li> <li>Background Jobs - Compression, retention, statistics</li> </ul>"},{"location":"maintenance/timescaledb-automation/#what-our-enhancement-adds","title":"What Our Enhancement Adds","text":"<ul> <li>Multi-tier Storage - SSD for hot data, HDD for cold data</li> <li>Compression-aware Placement - Optimal storage based on compression status</li> <li>Automated Maintenance - Weekly optimization without manual intervention</li> </ul>"},{"location":"maintenance/timescaledb-automation/#production-setup","title":"Production Setup","text":""},{"location":"maintenance/timescaledb-automation/#1-one-time-initial-setup","title":"1. One-Time Initial Setup","text":"<p>Run the complete optimization setup once after deployment:</p> <pre><code># Complete initial TimescaleDB optimization\n./deploy-onprem.sh optimize-timescaledb\n</code></pre> <p>This command: - Sets up SSD and HDD tablespaces - Configures compression policies (7-14 days) - Sets up retention policies (2-7 years) - Optimizes PostgreSQL settings - Creates monitoring views - Moves existing chunks to optimal locations</p>"},{"location":"maintenance/timescaledb-automation/#2-weekly-automation-cron-job","title":"2. Weekly Automation (Cron Job)","text":"<p>Set up automated weekly maintenance:</p> <pre><code># Edit crontab\ncrontab -e\n\n# Add weekly maintenance (Sundays at 2 AM)\n0 2 * * 0 /opt/prs/prs-deployment/scripts/deploy-onprem.sh weekly-maintenance &gt;&gt; /var/log/timescaledb-maintenance.log 2&gt;&amp;1\n\n# Optional: Daily status check (8 AM)\n0 8 * * * /opt/prs/prs-deployment/scripts/deploy-onprem.sh timescaledb-status &gt;&gt; /var/log/timescaledb-status.log 2&gt;&amp;1\n</code></pre>"},{"location":"maintenance/timescaledb-automation/#3-monitoring-commands","title":"3. Monitoring Commands","text":"<pre><code># Check current status\n./deploy-onprem.sh timescaledb-status\n\n# Manual maintenance (if needed)\n./deploy-onprem.sh weekly-maintenance\n\n# Move chunks manually (rarely needed)\n./deploy-onprem.sh move-chunks\n</code></pre>"},{"location":"maintenance/timescaledb-automation/#storage-tier-strategy","title":"Storage Tier Strategy","text":""},{"location":"maintenance/timescaledb-automation/#multi-tier-data-placement","title":"Multi-Tier Data Placement","text":"<pre><code>STORAGE TIERS:\n\u251c\u2500\u2500 SSD Hot Storage (/mnt/ssd/postgresql-hot)\n\u2502   \u251c\u2500\u2500 Recent uncompressed data (&lt; 7 days)\n\u2502   \u251c\u2500\u2500 Recent compressed data (&lt; 30 days)\n\u2502   \u2514\u2500\u2500 New tables and indexes\n\u2502\n\u251c\u2500\u2500 HDD Cold Storage (/mnt/hdd/postgresql-cold)\n\u2502   \u2514\u2500\u2500 Old data (&gt; 30 days, any compression state)\n\u2502\n\u2514\u2500\u2500 Container Volume (/var/lib/postgresql/data)\n    \u251c\u2500\u2500 System catalogs (pg_global)\n    \u251c\u2500\u2500 Default tablespace (pg_default) - fallback\n    \u2514\u2500\u2500 PostgreSQL configuration files\n</code></pre>"},{"location":"maintenance/timescaledb-automation/#intelligent-placement-logic","title":"Intelligent Placement Logic","text":"<pre><code>-- Hot Data Strategy: Recent uncompressed data on SSD for fast writes\nWHEN chunk_age &lt; 7 days AND NOT compressed THEN 'ssd_hot'\n\n-- Warm Data Strategy: Recent compressed data on SSD for fast queries\nWHEN chunk_age &lt; 30 days AND compressed THEN 'ssd_hot'\n\n-- Cold Data Strategy: Old data goes to HDD for cost-effective storage\nELSE 'hdd_cold'\n</code></pre>"},{"location":"maintenance/timescaledb-automation/#data-lifecycle","title":"Data Lifecycle","text":""},{"location":"maintenance/timescaledb-automation/#automatic-data-flow","title":"Automatic Data Flow","text":"<ol> <li>New Data (0-7 days)</li> <li>Location: SSD Hot Tablespace</li> <li>State: Uncompressed</li> <li> <p>Purpose: Fast writes, recent queries</p> </li> <li> <p>Compression Trigger (7 days)</p> </li> <li>Action: Auto-compress chunks</li> <li>Location: Stay on SSD (warm data)</li> <li> <p>Purpose: Space efficiency + fast queries</p> </li> <li> <p>Aging Data (30+ days)</p> </li> <li>Action: Move to HDD Cold Tablespace</li> <li>State: Compressed</li> <li> <p>Purpose: Cost-effective archival</p> </li> <li> <p>Retention Policies (2-7 years)</p> </li> <li>Action: Auto-delete old data</li> <li>Tables: Based on compliance requirements</li> <li>Purpose: Data lifecycle management</li> </ol>"},{"location":"maintenance/timescaledb-automation/#monitoring-verification","title":"Monitoring &amp; Verification","text":""},{"location":"maintenance/timescaledb-automation/#status-checks","title":"Status Checks","text":"<pre><code># Comprehensive status report\n./deploy-onprem.sh timescaledb-status\n</code></pre>"},{"location":"maintenance/timescaledb-automation/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"<ol> <li>Chunk Distribution</li> <li>Chunks in correct tablespaces</li> <li>Compression ratios</li> <li> <p>Age distribution</p> </li> <li> <p>Active Policies</p> </li> <li>Compression policies status</li> <li>Retention policies status</li> <li> <p>Background job health</p> </li> <li> <p>Storage Utilization</p> </li> <li>SSD vs HDD usage</li> <li>Compression effectiveness</li> <li>Growth trends</li> </ol>"},{"location":"maintenance/timescaledb-automation/#sql-monitoring-queries","title":"SQL Monitoring Queries","text":"<pre><code>-- Chunk distribution by tablespace and compression\nSELECT\n    COALESCE(t.tablespace, 'pg_default') as tablespace,\n    CASE WHEN c.is_compressed THEN 'Compressed' ELSE 'Uncompressed' END as compression_status,\n    COUNT(*) as chunk_count,\n    ROUND(AVG(EXTRACT(EPOCH FROM (NOW() - c.range_end))/86400), 1) as avg_age_days\nFROM timescaledb_information.chunks c\nLEFT JOIN pg_tables t ON t.schemaname = c.chunk_schema AND t.tablename = c.chunk_name\nGROUP BY COALESCE(t.tablespace, 'pg_default'), c.is_compressed\nORDER BY tablespace, compression_status;\n\n-- Active policies\nSELECT\n    job_id,\n    application_name,\n    schedule_interval,\n    CASE WHEN scheduled THEN 'Active' ELSE 'Inactive' END as status\nFROM timescaledb_information.jobs\nWHERE application_name LIKE '%Compression%' OR application_name LIKE '%Retention%'\nORDER BY application_name;\n</code></pre>"},{"location":"maintenance/timescaledb-automation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"maintenance/timescaledb-automation/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Chunks in Wrong Tablespaces <pre><code># Check status\n./deploy-onprem.sh timescaledb-status\n\n# Fix automatically\n./deploy-onprem.sh weekly-maintenance\n</code></pre></p> </li> <li> <p>Compression Not Working <pre><code>-- Check compression policies\nSELECT * FROM timescaledb_information.jobs\nWHERE application_name LIKE '%Compression%';\n\n-- Check uncompressed chunks\nSELECT COUNT(*) FROM timescaledb_information.chunks\nWHERE NOT is_compressed AND range_end &lt; NOW() - INTERVAL '7 days';\n</code></pre></p> </li> <li> <p>Storage Issues <pre><code># Check disk usage\ndf -h /mnt/ssd /mnt/hdd\n\n# Check tablespace sizes\nSELECT\n    spcname,\n    pg_size_pretty(pg_tablespace_size(spcname)) as size\nFROM pg_tablespace\nWHERE spcname IN ('ssd_hot', 'hdd_cold');\n</code></pre></p> </li> </ol>"},{"location":"maintenance/timescaledb-automation/#manual-intervention-scenarios","title":"Manual Intervention Scenarios","text":"<p>Only manual intervention is needed when: - Status shows chunks needing attention - Major configuration changes required - Performance troubleshooting needed - Storage capacity issues</p>"},{"location":"maintenance/timescaledb-automation/#performance-benefits","title":"Performance Benefits","text":""},{"location":"maintenance/timescaledb-automation/#write-performance","title":"Write Performance","text":"<ul> <li>New data \u2192 SSD (fast writes)</li> <li>Uncompressed chunks \u2192 SSD (no compression overhead)</li> </ul>"},{"location":"maintenance/timescaledb-automation/#query-performance","title":"Query Performance","text":"<ul> <li>Recent queries \u2192 SSD (hot + warm data)</li> <li>Compressed data \u2192 Better cache utilization</li> <li>Old data \u2192 HDD (acceptable for archival queries)</li> </ul>"},{"location":"maintenance/timescaledb-automation/#storage-efficiency","title":"Storage Efficiency","text":"<ul> <li>Compression \u2192 Reduces storage requirements</li> <li>Tiered storage \u2192 Cost optimization</li> <li>Retention policies \u2192 Automatic cleanup</li> </ul>"},{"location":"maintenance/timescaledb-automation/#configuration-files","title":"Configuration Files","text":""},{"location":"maintenance/timescaledb-automation/#compression-policies","title":"Compression Policies","text":"<p>High-volume tables: 7-day compression - <code>audit_logs</code> - <code>notifications</code> - <code>notes</code> - <code>comments</code> - <code>force_close_logs</code> - <code>transaction_logs</code></p> <p>History tables: 14-day compression - <code>requisition_canvass_histories</code> - <code>requisition_item_histories</code> - <code>requisition_order_histories</code> - <code>requisition_delivery_histories</code> - <code>requisition_payment_histories</code> - <code>requisition_return_histories</code> - <code>non_requisition_histories</code> - <code>invoice_report_histories</code> - <code>delivery_receipt_items_history</code></p>"},{"location":"maintenance/timescaledb-automation/#retention-policies","title":"Retention Policies","text":"<ul> <li>High-volume tables: 2 years</li> <li>History tables: 7 years (compliance)</li> </ul>"},{"location":"maintenance/timescaledb-automation/#best-practices","title":"Best Practices","text":"<ol> <li>Monitor Weekly</li> <li>Review maintenance logs</li> <li>Check storage utilization</li> <li> <p>Verify policy effectiveness</p> </li> <li> <p>Plan Capacity</p> </li> <li>Monitor growth trends</li> <li>Plan SSD/HDD capacity</li> <li> <p>Consider compression ratios</p> </li> <li> <p>Test Changes</p> </li> <li>Test configuration changes in staging</li> <li>Monitor performance impact</li> <li> <p>Have rollback procedures</p> </li> <li> <p>Document Changes</p> </li> <li>Log configuration modifications</li> <li>Track performance improvements</li> <li>Maintain operational runbooks</li> </ol>"},{"location":"maintenance/timescaledb-automation/#support","title":"Support","text":"<p>For issues with TimescaleDB automation: 1. Check logs: <code>/var/log/timescaledb-*.log</code> 2. Run status check: <code>./deploy-onprem.sh timescaledb-status</code> 3. Review this documentation 4. Contact system administrator</p> <p>This automation ensures your TimescaleDB deployment requires minimal manual intervention while maintaining optimal performance and cost efficiency.</p>"},{"location":"maintenance/timescaledb-quick-reference/","title":"TimescaleDB Quick Reference","text":""},{"location":"maintenance/timescaledb-quick-reference/#quick-start-commands","title":"Quick Start Commands","text":""},{"location":"maintenance/timescaledb-quick-reference/#initial-setup-run-once","title":"Initial Setup (Run Once)","text":"<pre><code># Complete TimescaleDB optimization setup\n./deploy-onprem.sh optimize-timescaledb\n</code></pre>"},{"location":"maintenance/timescaledb-quick-reference/#production-automation","title":"Production Automation","text":"<pre><code># Set up weekly maintenance cron job\necho \"0 2 * * 0 /opt/prs/prs-deployment/scripts/deploy-onprem.sh weekly-maintenance\" | crontab -\n</code></pre>"},{"location":"maintenance/timescaledb-quick-reference/#daily-operations","title":"Daily Operations","text":"<pre><code># Check status\n./deploy-onprem.sh timescaledb-status\n\n# Manual maintenance (if needed)\n./deploy-onprem.sh weekly-maintenance\n\n# Move chunks manually (rarely needed)\n./deploy-onprem.sh move-chunks\n</code></pre>"},{"location":"maintenance/timescaledb-quick-reference/#status-interpretation","title":"Status Interpretation","text":""},{"location":"maintenance/timescaledb-quick-reference/#healthy-system-output","title":"Healthy System Output","text":"<pre><code>=== CHUNK DISTRIBUTION ===\n tablespace | compression_status | chunk_count | avg_age_days\n------------+--------------------+-------------+--------------\n ssd_hot    | Uncompressed       |          5  |         -2.0\n ssd_hot    | Compressed         |         10  |        15.0\n hdd_cold   | Compressed         |         25  |        45.0\n\n=== CHUNKS NEEDING ATTENTION ===\nAll chunks are in optimal tablespaces\n</code></pre>"},{"location":"maintenance/timescaledb-quick-reference/#issues-to-address","title":"Issues to Address","text":"<pre><code>=== CHUNKS NEEDING ATTENTION ===\nWARNING: 15 chunks need tablespace movement\n  Run: ./deploy-onprem.sh weekly-maintenance\n</code></pre>"},{"location":"maintenance/timescaledb-quick-reference/#common-commands","title":"Common Commands","text":""},{"location":"maintenance/timescaledb-quick-reference/#tablespace-management","title":"Tablespace Management","text":"<pre><code># Setup tablespaces\n./deploy-onprem.sh timescaledb-setup\n\n# Check tablespace usage\ndocker exec -e PGPASSWORD=\"$POSTGRES_PASSWORD\" prs-onprem-postgres-timescale psql -U prs_user -d prs_production -c \"\nSELECT\n    spcname,\n    pg_size_pretty(pg_tablespace_size(spcname)) as size\nFROM pg_tablespace\nWHERE spcname IN ('ssd_hot', 'hdd_cold');\n\"\n</code></pre>"},{"location":"maintenance/timescaledb-quick-reference/#monitoring-queries","title":"Monitoring Queries","text":"<pre><code>-- Chunk distribution\nSELECT\n    COALESCE(t.tablespace, 'pg_default') as tablespace,\n    CASE WHEN c.is_compressed THEN 'Compressed' ELSE 'Uncompressed' END as status,\n    COUNT(*) as chunks\nFROM timescaledb_information.chunks c\nLEFT JOIN pg_tables t ON t.schemaname = c.chunk_schema AND t.tablename = c.chunk_name\nGROUP BY COALESCE(t.tablespace, 'pg_default'), c.is_compressed;\n\n-- Active policies\nSELECT job_id, application_name, schedule_interval, scheduled\nFROM timescaledb_information.jobs\nWHERE application_name LIKE '%Compression%' OR application_name LIKE '%Retention%';\n\n-- Compression effectiveness\nSELECT\n    hypertable_name,\n    COUNT(*) as total_chunks,\n    COUNT(CASE WHEN is_compressed THEN 1 END) as compressed_chunks,\n    ROUND(COUNT(CASE WHEN is_compressed THEN 1 END)::numeric / COUNT(*) * 100, 1) as compression_pct\nFROM timescaledb_information.chunks\nGROUP BY hypertable_name\nORDER BY hypertable_name;\n</code></pre>"},{"location":"maintenance/timescaledb-quick-reference/#troubleshooting","title":"Troubleshooting","text":""},{"location":"maintenance/timescaledb-quick-reference/#problem-chunks-in-wrong-tablespaces","title":"Problem: Chunks in Wrong Tablespaces","text":"<pre><code># Check what needs moving\n./deploy-onprem.sh timescaledb-status\n\n# Fix automatically\n./deploy-onprem.sh weekly-maintenance\n</code></pre>"},{"location":"maintenance/timescaledb-quick-reference/#problem-low-compression-ratio","title":"Problem: Low Compression Ratio","text":"<pre><code>-- Find uncompressed old chunks\nSELECT\n    hypertable_name,\n    chunk_name,\n    range_end,\n    is_compressed\nFROM timescaledb_information.chunks\nWHERE NOT is_compressed\nAND range_end &lt; NOW() - INTERVAL '7 days'\nORDER BY range_end;\n</code></pre>"},{"location":"maintenance/timescaledb-quick-reference/#problem-storage-full","title":"Problem: Storage Full","text":"<pre><code># Check disk usage\ndf -h /mnt/ssd /mnt/hdd\n\n# Check largest tables\ndocker exec -e PGPASSWORD=\"$POSTGRES_PASSWORD\" prs-onprem-postgres-timescale psql -U prs_user -d prs_production -c \"\nSELECT\n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\nFROM pg_tables\nWHERE schemaname = '_timescaledb_internal'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\nLIMIT 10;\n\"\n</code></pre>"},{"location":"maintenance/timescaledb-quick-reference/#maintenance-checklist","title":"Maintenance Checklist","text":""},{"location":"maintenance/timescaledb-quick-reference/#weekly-automated","title":"Weekly (Automated)","text":"<ul> <li>Chunk movement optimization</li> <li>Compression of eligible chunks</li> <li>Background job health check</li> <li>Storage utilization review</li> </ul>"},{"location":"maintenance/timescaledb-quick-reference/#monthly-manual-review","title":"Monthly (Manual Review)","text":"<ul> <li>Review maintenance logs</li> <li>Check storage growth trends</li> <li>Verify retention policy effectiveness</li> <li>Plan capacity if needed</li> </ul>"},{"location":"maintenance/timescaledb-quick-reference/#quarterly-performance-review","title":"Quarterly (Performance Review)","text":"<ul> <li>Analyze query performance</li> <li>Review compression ratios</li> <li>Optimize chunk intervals if needed</li> <li>Update retention policies if required</li> </ul>"},{"location":"maintenance/timescaledb-quick-reference/#log-locations","title":"Log Locations","text":"<pre><code># Maintenance logs\ntail -f /var/log/timescaledb-maintenance.log\n\n# Status logs\ntail -f /var/log/timescaledb-status.log\n\n# PostgreSQL logs\ndocker logs prs-onprem-postgres-timescale\n</code></pre>"},{"location":"maintenance/timescaledb-quick-reference/#emergency-procedures","title":"Emergency Procedures","text":""},{"location":"maintenance/timescaledb-quick-reference/#database-performance-issues","title":"Database Performance Issues","text":"<ol> <li>Check current status: <code>./deploy-onprem.sh timescaledb-status</code></li> <li>Review recent maintenance: <code>tail -100 /var/log/timescaledb-maintenance.log</code></li> <li>Check disk space: <code>df -h /mnt/ssd /mnt/hdd</code></li> <li>Run manual maintenance: <code>./deploy-onprem.sh weekly-maintenance</code></li> </ol>"},{"location":"maintenance/timescaledb-quick-reference/#storage-emergency","title":"Storage Emergency","text":"<ol> <li>Check immediate space: <code>df -h</code></li> <li>Identify largest chunks: Use storage troubleshooting query above</li> <li>Force compression: Run auto-optimizer manually</li> <li>Emergency cleanup: Review retention policies</li> </ol>"},{"location":"maintenance/timescaledb-quick-reference/#automation-failure","title":"Automation Failure","text":"<ol> <li>Check cron job: <code>crontab -l</code></li> <li>Test manual run: <code>./deploy-onprem.sh weekly-maintenance</code></li> <li>Check permissions: Ensure script is executable</li> <li>Review logs: Check for error messages</li> </ol>"},{"location":"maintenance/timescaledb-quick-reference/#performance-targets","title":"Performance Targets","text":""},{"location":"maintenance/timescaledb-quick-reference/#healthy-metrics","title":"Healthy Metrics","text":"<ul> <li>Compression Ratio: &gt;70% for tables older than 7 days</li> <li>SSD Usage: &lt;80% of available space</li> <li>Chunk Movement: &lt;5 chunks needing movement weekly</li> <li>Background Jobs: All compression/retention jobs active</li> </ul>"},{"location":"maintenance/timescaledb-quick-reference/#warning-thresholds","title":"Warning Thresholds","text":"<ul> <li>Compression Ratio: &lt;50% for old data</li> <li>SSD Usage: &gt;90% of available space</li> <li>Chunk Movement: &gt;20 chunks needing movement</li> <li>Background Jobs: Any critical job inactive</li> </ul>"},{"location":"maintenance/timescaledb-quick-reference/#additional-resources","title":"Additional Resources","text":"<ul> <li>TimescaleDB Automation Guide</li> <li>Capacity Planning</li> <li>Routine Maintenance</li> <li>Security Maintenance</li> </ul> <p>Keep this reference handy for daily TimescaleDB operations.</p>"},{"location":"maintenance/updates/","title":"Updates &amp; Upgrades","text":""},{"location":"maintenance/updates/#overview","title":"Overview","text":"<p>This guide covers the complete process for updating and upgrading the PRS on-premises deployment, including system updates, application updates, and database migrations.</p>"},{"location":"maintenance/updates/#update-strategy","title":"Update Strategy","text":""},{"location":"maintenance/updates/#update-types","title":"Update Types","text":"Update Type Frequency Risk Level Downtime Testing Required Security Patches As needed Low Minimal Basic System Updates Monthly Medium Moderate Comprehensive Application Updates Quarterly High Planned Extensive Database Upgrades Annually Very High Extended Full Testing"},{"location":"maintenance/updates/#update-planning","title":"Update Planning","text":"<pre><code>graph TB\n    subgraph \"Update Process\"\n        ASSESS[Assessment&lt;br/&gt;Security &amp; Impact Analysis]\n        PLAN[Planning&lt;br/&gt;Schedule &amp; Resources]\n        TEST[Testing&lt;br/&gt;Staging Environment]\n        BACKUP[Backup&lt;br/&gt;Full System Backup]\n        DEPLOY[Deployment&lt;br/&gt;Production Update]\n        VERIFY[Verification&lt;br/&gt;Post-Update Testing]\n        ROLLBACK[Rollback Plan&lt;br/&gt;If Issues Occur]\n    end\n\n    ASSESS --&gt; PLAN\n    PLAN --&gt; TEST\n    TEST --&gt; BACKUP\n    BACKUP --&gt; DEPLOY\n    DEPLOY --&gt; VERIFY\n    VERIFY --&gt; ROLLBACK\n\n    style ASSESS fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style TEST fill:#e8f5e8,stroke:#4caf50,stroke-width:2px\n    style BACKUP fill:#fff3e0,stroke:#ff9800,stroke-width:2px\n    style DEPLOY fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px</code></pre>"},{"location":"maintenance/updates/#security-updates","title":"Security Updates","text":""},{"location":"maintenance/updates/#critical-security-patches","title":"Critical Security Patches","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/security-updates.sh\n\nset -euo pipefail\n\nLOG_FILE=\"/var/log/prs-updates.log\"\nBACKUP_DIR=\"/mnt/hdd/pre-update-backups\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nlog_message \"Starting security update process\"\n\n# 1. Check for security updates\nlog_message \"Checking for available security updates\"\napt update\nSECURITY_UPDATES=$(apt list --upgradable 2&gt;/dev/null | grep security | wc -l)\n\nif [ \"$SECURITY_UPDATES\" -eq 0 ]; then\n    log_message \"No security updates available\"\n    exit 0\nfi\n\nlog_message \"Found $SECURITY_UPDATES security updates\"\n\n# 2. Create pre-update backup\nlog_message \"Creating pre-update backup\"\nmkdir -p \"$BACKUP_DIR/$(date +%Y%m%d_%H%M%S)\"\nBACKUP_PATH=\"$BACKUP_DIR/$(date +%Y%m%d_%H%M%S)\"\n\n# Backup critical configurations\ncp -r /opt/prs-deployment/02-docker-configuration \"$BACKUP_PATH/\"\ncp /etc/crontab \"$BACKUP_PATH/\" 2&gt;/dev/null || true\ncp -r /etc/nginx \"$BACKUP_PATH/\" 2&gt;/dev/null || true\n\n# Database backup\ndocker exec prs-onprem-postgres-timescale pg_dump -U prs_admin -d prs_production -Fc &gt; \"$BACKUP_PATH/database-backup.dump\"\n\nlog_message \"Backup created at: $BACKUP_PATH\"\n\n# 3. Apply security updates\nlog_message \"Applying security updates\"\napt list --upgradable 2&gt;/dev/null | grep security &gt; \"$BACKUP_PATH/applied-updates.txt\"\n\n# Apply updates with automatic restart if needed\nDEBIAN_FRONTEND=noninteractive apt-get -y upgrade\n\n# 4. Restart services if needed\nif [ -f /var/run/reboot-required ]; then\n    log_message \"System reboot required - scheduling for next maintenance window\"\n    echo \"System reboot required after security updates\" | \\\n    mail -s \"PRS Security Updates - Reboot Required\" admin@your-domain.com\nelse\n    log_message \"Restarting affected services\"\n    systemctl restart docker\n    docker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml restart\nfi\n\n# 5. Verify system health\nlog_message \"Verifying system health after updates\"\nsleep 30\n/opt/prs-deployment/scripts/system-health-check.sh all &gt; \"$BACKUP_PATH/post-update-health.log\"\n\nif grep -q \"ERROR\" \"$BACKUP_PATH/post-update-health.log\"; then\n    log_message \"ERROR: Health check failed after updates\"\n    echo \"Security updates completed but health check failed\" | \\\n    mail -s \"PRS Security Updates - Health Check Failed\" admin@your-domain.com\nelse\n    log_message \"Security updates completed successfully\"\n    echo \"Security updates applied successfully\" | \\\n    mail -s \"PRS Security Updates - Success\" admin@your-domain.com\nfi\n\nlog_message \"Security update process completed\"\n</code></pre>"},{"location":"maintenance/updates/#automated-security-update-check","title":"Automated Security Update Check","text":"<pre><code>#!/bin/bash\n# Daily security update check\n\nSECURITY_UPDATES=$(apt list --upgradable 2&gt;/dev/null | grep security | wc -l)\n\nif [ \"$SECURITY_UPDATES\" -gt 0 ]; then\n    echo \"Security updates available: $SECURITY_UPDATES\"\n    apt list --upgradable 2&gt;/dev/null | grep security | \\\n    mail -s \"PRS Security Updates Available\" admin@your-domain.com\n\n    # Auto-apply critical security updates (optional)\n    if [ \"$SECURITY_UPDATES\" -le 5 ]; then\n        echo \"Auto-applying $SECURITY_UPDATES security updates\"\n        /opt/prs-deployment/scripts/security-updates.sh\n    fi\nfi\n</code></pre>"},{"location":"maintenance/updates/#system-updates","title":"System Updates","text":""},{"location":"maintenance/updates/#monthly-system-update-process","title":"Monthly System Update Process","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/monthly-system-update.sh\n\nset -euo pipefail\n\nLOG_FILE=\"/var/log/prs-updates.log\"\nUPDATE_DATE=$(date +%Y%m%d)\nMAINTENANCE_WINDOW=\"4 hours\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nlog_message \"Starting monthly system update (Maintenance window: $MAINTENANCE_WINDOW)\"\n\n# 1. Pre-update preparation\nlog_message \"Preparing for system update\"\n\n# Create maintenance notification\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nINSERT INTO system_notifications (message, type, created_at) \nVALUES ('System maintenance in progress. Service may be temporarily unavailable.', 'maintenance', NOW());\n\"\n\n# Stop non-essential services\nlog_message \"Stopping non-essential services\"\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml stop frontend backend worker\n\n# 2. System backup\nlog_message \"Creating comprehensive system backup\"\nBACKUP_DIR=\"/mnt/hdd/system-backups/$UPDATE_DATE\"\nmkdir -p \"$BACKUP_DIR\"\n\n# Database backup\ndocker exec prs-onprem-postgres-timescale pg_dump -U prs_admin -d prs_production -Fc &gt; \"$BACKUP_DIR/database-full.dump\"\n\n# Configuration backup\ntar -czf \"$BACKUP_DIR/configuration-backup.tar.gz\" /opt/prs-deployment/02-docker-configuration/\n\n# Application data backup\ntar -czf \"$BACKUP_DIR/uploads-backup.tar.gz\" /mnt/ssd/uploads/\n\nlog_message \"System backup completed\"\n\n# 3. Update system packages\nlog_message \"Updating system packages\"\napt update\napt list --upgradable &gt; \"$BACKUP_DIR/available-updates.txt\"\n\n# Apply updates\nDEBIAN_FRONTEND=noninteractive apt-get -y upgrade\nDEBIAN_FRONTEND=noninteractive apt-get -y autoremove\nDEBIAN_FRONTEND=noninteractive apt-get -y autoclean\n\n# 4. Update Docker and Docker Compose\nlog_message \"Checking Docker updates\"\nDOCKER_VERSION=$(docker --version)\nlog_message \"Current Docker version: $DOCKER_VERSION\"\n\n# Update Docker if newer version available\nif apt list --upgradable 2&gt;/dev/null | grep -q docker; then\n    log_message \"Updating Docker\"\n    apt-get -y install docker-ce docker-ce-cli containerd.io\nfi\n\n# 5. Update container images\nlog_message \"Updating container images\"\ncd /opt/prs-deployment\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml pull\n\n# 6. Restart services\nlog_message \"Restarting services\"\nif [ -f /var/run/reboot-required ]; then\n    log_message \"System reboot required\"\n    # Schedule reboot for immediate execution\n    shutdown -r +1 \"System reboot required for updates\"\nelse\n    # Restart Docker and services\n    systemctl restart docker\n    docker-compose -f 02-docker-configuration/docker-compose.onprem.yml up -d\nfi\n\n# 7. Post-update verification\nlog_message \"Performing post-update verification\"\nsleep 60  # Wait for services to start\n\n# Health check\n/opt/prs-deployment/scripts/system-health-check.sh all &gt; \"$BACKUP_DIR/post-update-health.log\"\n\n# Test application functionality\nAPI_STATUS=$(curl -s -o /dev/null -w \"%{http_code}\" https://localhost/api/health)\nif [ \"$API_STATUS\" = \"200\" ]; then\n    log_message \"Application health check passed\"\nelse\n    log_message \"ERROR: Application health check failed (Status: $API_STATUS)\"\nfi\n\n# 8. Clear maintenance notification\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nDELETE FROM system_notifications WHERE type = 'maintenance';\n\"\n\n# 9. Generate update report\nUPDATE_REPORT=\"$BACKUP_DIR/update-report.txt\"\ncat &gt; \"$UPDATE_REPORT\" &lt;&lt; EOF\nPRS Monthly System Update Report\nDate: $UPDATE_DATE\n================================\n\nUpdates Applied:\n$(cat \"$BACKUP_DIR/available-updates.txt\")\n\nDocker Version:\nBefore: $DOCKER_VERSION\nAfter: $(docker --version)\n\nSystem Status:\n- Reboot Required: $([ -f /var/run/reboot-required ] &amp;&amp; echo \"Yes\" || echo \"No\")\n- API Status: $API_STATUS\n- Services: $(docker-compose -f 02-docker-configuration/docker-compose.onprem.yml ps --services | wc -l) running\n\nHealth Check Results:\n$(cat \"$BACKUP_DIR/post-update-health.log\")\n\nBackup Location: $BACKUP_DIR\nEOF\n\nlog_message \"Monthly system update completed\"\n\n# Send update report\nif command -v mail &gt;/dev/null 2&gt;&amp;1; then\n    mail -s \"PRS Monthly System Update Report\" admin@your-domain.com &lt; \"$UPDATE_REPORT\"\nfi\n</code></pre>"},{"location":"maintenance/updates/#application-updates","title":"Application Updates","text":""},{"location":"maintenance/updates/#application-update-process","title":"Application Update Process","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/application-update.sh\n\nset -euo pipefail\n\nNEW_VERSION=\"$1\"\nCURRENT_VERSION=$(cat /opt/prs-deployment/VERSION 2&gt;/dev/null || echo \"unknown\")\n\nif [ -z \"$NEW_VERSION\" ]; then\n    echo \"Usage: $0 &lt;new-version&gt;\"\n    echo \"Current version: $CURRENT_VERSION\"\n    exit 1\nfi\n\nLOG_FILE=\"/var/log/prs-updates.log\"\nBACKUP_DIR=\"/mnt/hdd/app-backups/v$NEW_VERSION-$(date +%Y%m%d_%H%M%S)\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nlog_message \"Starting application update from $CURRENT_VERSION to $NEW_VERSION\"\n\n# 1. Pre-update validation\nlog_message \"Performing pre-update validation\"\n\n# Check if update is available\nif ! git ls-remote --tags origin | grep -q \"v$NEW_VERSION\"; then\n    log_message \"ERROR: Version $NEW_VERSION not found in repository\"\n    exit 1\nfi\n\n# Check system resources\nFREE_SPACE=$(df /mnt/ssd | awk 'NR==2 {print $4}')\nif [ \"$FREE_SPACE\" -lt 5000000 ]; then  # 5GB\n    log_message \"ERROR: Insufficient disk space for update\"\n    exit 1\nfi\n\n# 2. Create comprehensive backup\nlog_message \"Creating comprehensive backup\"\nmkdir -p \"$BACKUP_DIR\"\n\n# Database backup\ndocker exec prs-onprem-postgres-timescale pg_dump -U prs_admin -d prs_production -Fc &gt; \"$BACKUP_DIR/database.dump\"\n\n# Application code backup\ntar -czf \"$BACKUP_DIR/application-code.tar.gz\" /opt/prs/\n\n# Configuration backup\ntar -czf \"$BACKUP_DIR/configuration.tar.gz\" /opt/prs-deployment/02-docker-configuration/\n\n# Uploads backup\ntar -czf \"$BACKUP_DIR/uploads.tar.gz\" /mnt/ssd/uploads/\n\nlog_message \"Backup completed: $BACKUP_DIR\"\n\n# 3. Download and prepare new version\nlog_message \"Downloading new version\"\ncd /opt/prs-deployment\ngit fetch origin\ngit checkout \"v$NEW_VERSION\"\n\n# 4. Check for database migrations\nlog_message \"Checking for database migrations\"\nif [ -f \"migrations/v$NEW_VERSION.sql\" ]; then\n    log_message \"Database migration required\"\n\n    # Test migration on backup database\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -c \"CREATE DATABASE prs_migration_test;\"\n    docker exec -i prs-onprem-postgres-timescale pg_restore -U prs_admin -d prs_migration_test &lt; \"$BACKUP_DIR/database.dump\"\n\n    if docker exec -i prs-onprem-postgres-timescale psql -U prs_admin -d prs_migration_test &lt; \"migrations/v$NEW_VERSION.sql\"; then\n        log_message \"Migration test successful\"\n        docker exec prs-onprem-postgres-timescale psql -U prs_admin -c \"DROP DATABASE prs_migration_test;\"\n    else\n        log_message \"ERROR: Migration test failed\"\n        exit 1\n    fi\nfi\n\n# 5. Stop application services\nlog_message \"Stopping application services\"\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml stop frontend backend worker\n\n# 6. Update application images\nlog_message \"Building new application images\"\ndocker build -t prs-frontend:$NEW_VERSION -f dockerfiles/Dockerfile.frontend /opt/prs/prs-frontend-a/\ndocker build -t prs-backend:$NEW_VERSION -f dockerfiles/Dockerfile.backend /opt/prs/prs-backend-a/\n\n# Update image tags in docker-compose\nsed -i \"s/prs-frontend:.*/prs-frontend:$NEW_VERSION/\" 02-docker-configuration/docker-compose.onprem.yml\nsed -i \"s/prs-backend:.*/prs-backend:$NEW_VERSION/\" 02-docker-configuration/docker-compose.onprem.yml\n\n# 7. Apply database migrations\nif [ -f \"migrations/v$NEW_VERSION.sql\" ]; then\n    log_message \"Applying database migration\"\n    docker exec -i prs-onprem-postgres-timescale psql -U prs_admin -d prs_production &lt; \"migrations/v$NEW_VERSION.sql\"\nfi\n\n# 8. Start updated services\nlog_message \"Starting updated services\"\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml up -d\n\n# 9. Post-update verification\nlog_message \"Performing post-update verification\"\nsleep 60\n\n# Health check\n/opt/prs-deployment/scripts/system-health-check.sh all &gt; \"$BACKUP_DIR/post-update-health.log\"\n\n# API version check\nAPI_VERSION=$(curl -s https://localhost/api/version | jq -r '.version' 2&gt;/dev/null || echo \"unknown\")\nif [ \"$API_VERSION\" = \"$NEW_VERSION\" ]; then\n    log_message \"Version verification successful: $API_VERSION\"\nelse\n    log_message \"WARNING: Version mismatch. Expected: $NEW_VERSION, Got: $API_VERSION\"\nfi\n\n# 10. Update version file\necho \"$NEW_VERSION\" &gt; /opt/prs-deployment/VERSION\n\nlog_message \"Application update completed successfully\"\n\n# Generate update report\nUPDATE_REPORT=\"$BACKUP_DIR/application-update-report.txt\"\ncat &gt; \"$UPDATE_REPORT\" &lt;&lt; EOF\nPRS Application Update Report\n=============================\nDate: $(date)\nFrom Version: $CURRENT_VERSION\nTo Version: $NEW_VERSION\n\nUpdate Process:\n- Database migration: $([ -f \"migrations/v$NEW_VERSION.sql\" ] &amp;&amp; echo \"Applied\" || echo \"Not required\")\n- API version check: $API_VERSION\n- Health check: $(grep -q \"ERROR\" \"$BACKUP_DIR/post-update-health.log\" &amp;&amp; echo \"Failed\" || echo \"Passed\")\n\nBackup Location: $BACKUP_DIR\n\n$(cat \"$BACKUP_DIR/post-update-health.log\")\nEOF\n\n# Send update report\nif command -v mail &gt;/dev/null 2&gt;&amp;1; then\n    mail -s \"PRS Application Update Report - v$NEW_VERSION\" admin@your-domain.com &lt; \"$UPDATE_REPORT\"\nfi\n</code></pre>"},{"location":"maintenance/updates/#database-upgrades","title":"Database Upgrades","text":""},{"location":"maintenance/updates/#postgresqltimescaledb-upgrade","title":"PostgreSQL/TimescaleDB Upgrade","text":"<pre><code>#!/bin/bash\n# Major database upgrade process\n\nOLD_VERSION=\"$1\"\nNEW_VERSION=\"$2\"\n\nif [ -z \"$OLD_VERSION\" ] || [ -z \"$NEW_VERSION\" ]; then\n    echo \"Usage: $0 &lt;old-version&gt; &lt;new-version&gt;\"\n    echo \"Example: $0 14 15\"\n    exit 1\nfi\n\nLOG_FILE=\"/var/log/prs-database-upgrade.log\"\nUPGRADE_DIR=\"/mnt/hdd/database-upgrade-$(date +%Y%m%d_%H%M%S)\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nlog_message \"Starting PostgreSQL upgrade from $OLD_VERSION to $NEW_VERSION\"\n\n# 1. Pre-upgrade preparation\nmkdir -p \"$UPGRADE_DIR\"\n\n# Full system backup\nlog_message \"Creating full system backup\"\ndocker exec prs-onprem-postgres-timescale pg_dumpall -U prs_admin &gt; \"$UPGRADE_DIR/full-database-backup.sql\"\n\n# Stop application services\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml stop frontend backend worker\n\n# 2. Export current data\nlog_message \"Exporting current database data\"\ndocker exec prs-onprem-postgres-timescale pg_dump -U prs_admin -d prs_production -Fc &gt; \"$UPGRADE_DIR/prs-production-backup.dump\"\n\n# 3. Stop database\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml stop postgres\n\n# 4. Backup data directory\nlog_message \"Backing up data directory\"\ntar -czf \"$UPGRADE_DIR/postgresql-data-backup.tar.gz\" /mnt/ssd/postgresql-hot/\n\n# 5. Update Docker image\nlog_message \"Updating PostgreSQL Docker image\"\nsed -i \"s/timescale\\/timescaledb:.*-pg$OLD_VERSION/timescale\\/timescaledb:latest-pg$NEW_VERSION/\" \\\n  /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml\n\n# 6. Initialize new database\nlog_message \"Initializing new database cluster\"\nrm -rf /mnt/ssd/postgresql-hot/*\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml up -d postgres\n\n# Wait for database to be ready\nsleep 60\nwhile ! docker exec prs-onprem-postgres-timescale pg_isready -U prs_admin; do\n    log_message \"Waiting for database to be ready...\"\n    sleep 10\ndone\n\n# 7. Restore data\nlog_message \"Restoring database data\"\ndocker exec -i prs-onprem-postgres-timescale pg_restore -U prs_admin -d prs_production --clean --if-exists &lt; \"$UPGRADE_DIR/prs-production-backup.dump\"\n\n# 8. Update TimescaleDB extension\nlog_message \"Updating TimescaleDB extension\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"ALTER EXTENSION timescaledb UPDATE;\"\n\n# 9. Verify upgrade\nlog_message \"Verifying database upgrade\"\nPG_VERSION=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT version();\" | head -1)\nTS_VERSION=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT extversion FROM pg_extension WHERE extname='timescaledb';\" | xargs)\n\nlog_message \"PostgreSQL version: $PG_VERSION\"\nlog_message \"TimescaleDB version: $TS_VERSION\"\n\n# 10. Start application services\nlog_message \"Starting application services\"\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml up -d\n\n# 11. Post-upgrade verification\nsleep 60\n/opt/prs-deployment/scripts/system-health-check.sh all &gt; \"$UPGRADE_DIR/post-upgrade-health.log\"\n\nlog_message \"Database upgrade completed\"\n\n# Generate upgrade report\ncat &gt; \"$UPGRADE_DIR/upgrade-report.txt\" &lt;&lt; EOF\nPostgreSQL/TimescaleDB Upgrade Report\n====================================\nDate: $(date)\nFrom: PostgreSQL $OLD_VERSION\nTo: PostgreSQL $NEW_VERSION\n\nVersions After Upgrade:\n- PostgreSQL: $PG_VERSION\n- TimescaleDB: $TS_VERSION\n\nHealth Check:\n$(cat \"$UPGRADE_DIR/post-upgrade-health.log\")\n\nBackup Location: $UPGRADE_DIR\nEOF\n\nif command -v mail &gt;/dev/null 2&gt;&amp;1; then\n    mail -s \"PRS Database Upgrade Report\" admin@your-domain.com &lt; \"$UPGRADE_DIR/upgrade-report.txt\"\nfi\n</code></pre>"},{"location":"maintenance/updates/#rollback-procedures","title":"Rollback Procedures","text":""},{"location":"maintenance/updates/#application-rollback","title":"Application Rollback","text":"<pre><code>#!/bin/bash\n# Emergency application rollback\n\nBACKUP_DIR=\"$1\"\n\nif [ -z \"$BACKUP_DIR\" ] || [ ! -d \"$BACKUP_DIR\" ]; then\n    echo \"Usage: $0 &lt;backup-directory&gt;\"\n    echo \"Available backups:\"\n    ls -la /mnt/hdd/app-backups/\n    exit 1\nfi\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a /var/log/prs-rollback.log\n}\n\nlog_message \"Starting emergency rollback from: $BACKUP_DIR\"\n\n# 1. Stop current services\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml down\n\n# 2. Restore database\nlog_message \"Restoring database\"\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml up -d postgres\nsleep 30\n\ndocker exec -i prs-onprem-postgres-timescale pg_restore -U prs_admin -d prs_production --clean --if-exists &lt; \"$BACKUP_DIR/database.dump\"\n\n# 3. Restore configuration\nlog_message \"Restoring configuration\"\ntar -xzf \"$BACKUP_DIR/configuration.tar.gz\" -C /\n\n# 4. Restore application code\nlog_message \"Restoring application code\"\ntar -xzf \"$BACKUP_DIR/application-code.tar.gz\" -C /\n\n# 5. Start services\nlog_message \"Starting services\"\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml up -d\n\nlog_message \"Rollback completed\"\n</code></pre> <p>Update Procedures Ready</p> <p>Your PRS deployment now has comprehensive update and upgrade procedures with proper backup, testing, and rollback capabilities.</p> <p>Testing Environment</p> <p>Always test updates in a staging environment that mirrors production before applying to the live system.</p> <p>Backup Verification</p> <p>Always verify backup integrity before starting any update process and ensure rollback procedures are tested and ready.</p>"},{"location":"operations/backup/","title":"Backup Procedures","text":""},{"location":"operations/backup/#overview","title":"Overview","text":"<p>This guide covers comprehensive backup and recovery procedures for the PRS on-premises deployment, ensuring zero data loss and rapid recovery capabilities.</p>"},{"location":"operations/backup/#backup-architecture","title":"Backup Architecture","text":"<pre><code>graph TB\n    subgraph \"Production System (SSD - Hot Data)\"\n        APP[PRS Application&lt;br/&gt;/mnt/ssd/uploads]\n        DB[(PostgreSQL/TimescaleDB&lt;br/&gt;/mnt/ssd/postgresql-data)]\n    end\n\n    subgraph \"Cold Storage (HDD)\"\n        DB_BACKUP[Database Backups&lt;br/&gt;/mnt/hdd/postgres-backups]\n        APP_BACKUP[Application Backups&lt;br/&gt;/mnt/hdd/app-backups]\n        ARCHIVE[Archive Storage&lt;br/&gt;/mnt/hdd/archive]\n    end\n\n    subgraph \"Backup &amp; DR (NAS)\"\n        NAS_DB[Database Backups&lt;br/&gt;/mnt/nas/postgres-backups]\n        NAS_APP[Application Backups&lt;br/&gt;/mnt/nas/app-backups]\n        NAS_DR[Disaster Recovery&lt;br/&gt;/mnt/nas/disaster-recovery]\n    end\n\n    subgraph \"Data Flow\"\n        DAILY[Daily Backup Process]\n        ARCHIVE_FLOW[Archive Process]\n        DR_SYNC[DR Synchronization]\n    end\n\n    DB --&gt; DB_BACKUP\n    APP --&gt; APP_BACKUP\n\n    DB_BACKUP --&gt; NAS_DB\n    APP_BACKUP --&gt; NAS_APP\n\n    DB_BACKUP --&gt; ARCHIVE\n    APP_BACKUP --&gt; ARCHIVE\n\n    ARCHIVE --&gt; NAS_DR\n\n    style DB fill:#ff9999,stroke:#ff0000,stroke-width:2px\n    style APP fill:#ff9999,stroke:#ff0000,stroke-width:2px\n    style DB_BACKUP fill:#99ccff,stroke:#0066cc,stroke-width:2px\n    style APP_BACKUP fill:#99ccff,stroke:#0066cc,stroke-width:2px\n    style NAS_DB fill:#99ff99,stroke:#00cc00,stroke-width:2px\n    style NAS_APP fill:#99ff99,stroke:#00cc00,stroke-width:2px</code></pre>"},{"location":"operations/backup/#backup-schedule","title":"Backup Schedule","text":""},{"location":"operations/backup/#backup-schedule_1","title":"Backup Schedule","text":"Backup Type Frequency Storage Tier Location Size Estimate Database Full Daily 2:00 AM HDD \u2192 NAS <code>/mnt/hdd/postgres-backups/</code> \u2192 <code>/mnt/nas/postgres-backups/</code> 5-20 GB Application Data Daily 3:00 AM HDD \u2192 NAS <code>/mnt/hdd/app-backups/</code> \u2192 <code>/mnt/nas/app-backups/</code> 10-50 GB Archive Storage Manual (90+ days) HDD Archive <code>/mnt/hdd/archive/</code> Growing Disaster Recovery Weekly Sync NAS DR <code>/mnt/nas/disaster-recovery/</code> Full System Logs &amp; Verification Continuous HDD <code>/var/log/prs-*.log</code> 1-100 MB <p>Three-Tier Storage Architecture</p> <p>SSD (/mnt/ssd): Hot data - Active database and application files HDD (/mnt/hdd): Cold data - Backups and archives NAS: Backup and disaster recovery - Off-site protection</p> <p>Zero Deletion Policy</p> <p>All data preserved permanently across all tiers. Data flows: SSD \u2192 HDD \u2192 NAS, never deleted.</p>"},{"location":"operations/backup/#cron-configuration","title":"Cron Configuration","text":"<pre><code># View current backup cron jobs\ncrontab -l\n\n# Actual backup schedule (ONLY EXISTING SCRIPTS):\n0 2 * * * /opt/prs-deployment/scripts/backup-full.sh\n0 3 * * * /opt/prs-deployment/scripts/backup-application-data.sh\n0 4 * * * /opt/prs-deployment/scripts/verify-backups.sh\n0 1 * * 0 /opt/prs-deployment/scripts/backup-maintenance.sh\n0 */6 * * * /opt/prs-deployment/scripts/test-nas-connection.sh\n</code></pre>"},{"location":"operations/backup/#backup-setup-and-automation","title":"Backup Setup and Automation","text":""},{"location":"operations/backup/#initial-setup","title":"Initial Setup","text":"<pre><code># Setup automated backup system (ACTUAL WORKING SCRIPT)\n/opt/prs-deployment/scripts/setup-backup-automation.sh\n\n# This script automatically:\n# \u2705 Creates all backup directories (/mnt/hdd/postgres-backups/, etc.)\n# \u2705 Configures NAS mounting if enabled in .env\n# \u2705 Installs cron jobs for automated backups\n# \u2705 Sets up log rotation and monitoring\n# \u2705 Tests backup procedures and connectivity\n# \u2705 Configures backup retention policies\n# \u2705 Sets up backup verification automation\n\n# Manual verification after setup:\ncrontab -l | grep backup\nls -la /mnt/hdd/postgres-backups/\nsystemctl status cron\n</code></pre>"},{"location":"operations/backup/#configuration-files","title":"Configuration Files","text":"<p>The backup system uses configuration from: - <code>/opt/prs-deployment/02-docker-configuration/.env</code> - Database credentials and NAS settings - <code>/etc/crontab</code> - Automated backup scheduling - <code>/var/log/prs-backup*.log</code> - Backup operation logs</p>"},{"location":"operations/backup/#database-backup-procedures","title":"Database Backup Procedures","text":""},{"location":"operations/backup/#database-backup","title":"Database Backup","text":"<pre><code>#!/bin/bash\n# Daily full database backup script (ACTUAL WORKING SCRIPT: backup-full.sh)\n\n# Run the actual backup script\n/opt/prs-deployment/scripts/backup-full.sh\n\n# The script automatically:\n# - Creates backup: /mnt/hdd/postgres-backups/daily/prs_full_backup_YYYYMMDD_HHMMSS.sql\n# - Compresses and encrypts backup\n# - Creates SHA256 checksum for verification\n# - Copies to NAS if configured\n# - Cleans up old backups (30 days retention)\n# - Logs all operations to /var/log/prs-backup.log\n\n# Manual execution example:\nBACKUP_DIR=\"/mnt/hdd/postgres-backups/daily\"\nDATE=$(date +%Y%m%d_%H%M%S)\nBACKUP_FILE=\"prs_full_backup_${DATE}.sql\"\n\n# The actual pg_dump command used by backup-full.sh:\ndocker exec prs-onprem-postgres-timescale pg_dump \\\n  -U prs_user \\\n  -d prs_production \\\n  --verbose \\\n  --format=custom \\\n  --compress=9 \\\n  --file=\"/var/lib/postgresql/data/backups/daily/${BACKUP_FILE}\"\n\n# Features included in backup-full.sh:\n# \u2705 NAS integration with automatic mounting\n# \u2705 Pre-backup space checks (5GB local, 10GB NAS)\n# \u2705 Database connectivity verification\n# \u2705 Compression and encryption\n# \u2705 Checksum verification\n# \u2705 Automatic cleanup and retention\n# \u2705 Comprehensive logging\n</code></pre>"},{"location":"operations/backup/#database-backup_1","title":"Database Backup","text":"<pre><code>#!/bin/bash\n# Incremental database backup using WAL-E or similar\n\nBACKUP_DIR=\"/mnt/hdd/postgres-backups/incremental\"\nDATE=$(date +%Y%m%d_%H%M%S)\n\n# Create backup directory\nmkdir -p \"$BACKUP_DIR\"\n\n# Perform incremental backup (WAL archiving)\ndocker exec prs-onprem-postgres-timescale pg_basebackup \\\n  -U prs_admin \\\n  -D \"/var/lib/postgresql/backups/incremental/backup_${DATE}\" \\\n  --wal-method=stream \\\n  --compress \\\n  --progress \\\n  --verbose\n\n# Log result\nif [ $? -eq 0 ]; then\n    echo \"$(date): Incremental backup completed: backup_${DATE}\" &gt;&gt; /var/log/prs-backup.log\nelse\n    echo \"$(date): Incremental backup failed!\" &gt;&gt; /var/log/prs-backup.log\nfi\n</code></pre>"},{"location":"operations/backup/#recovery-setup","title":"Recovery Setup","text":"<pre><code>-- Enable WAL archiving for point-in-time recovery\nALTER SYSTEM SET wal_level = 'replica';\nALTER SYSTEM SET archive_mode = 'on';\nALTER SYSTEM SET archive_command = 'cp %p /var/lib/postgresql/wal-archive/%f';\nALTER SYSTEM SET max_wal_senders = 3;\nALTER SYSTEM SET wal_keep_segments = 64;\n\n-- Reload configuration\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"operations/backup/#file-system-backup-procedures","title":"File System Backup Procedures","text":""},{"location":"operations/backup/#files-backup","title":"Files Backup","text":"<pre><code>#!/bin/bash\n# Daily file uploads backup script\n\nSOURCE_DIR=\"/mnt/ssd/uploads\"\nBACKUP_DIR=\"/mnt/hdd/file-backups\"\nDATE=$(date +%Y%m%d)\n\n# Create backup directory\nmkdir -p \"$BACKUP_DIR/$DATE\"\n\n# Perform incremental backup using rsync\nrsync -av \\\n  --link-dest=\"$BACKUP_DIR/latest\" \\\n  \"$SOURCE_DIR/\" \\\n  \"$BACKUP_DIR/$DATE/\"\n\n# Update latest symlink\nrm -f \"$BACKUP_DIR/latest\"\nln -s \"$DATE\" \"$BACKUP_DIR/latest\"\n\n# Log result\nif [ $? -eq 0 ]; then\n    echo \"$(date): File backup completed: $DATE\" &gt;&gt; /var/log/prs-backup.log\n\n    # Cleanup old backups (keep 90 days)\n    find \"$BACKUP_DIR\" -maxdepth 1 -type d -name \"20*\" -mtime +90 -exec rm -rf {} \\;\nelse\n    echo \"$(date): File backup failed!\" &gt;&gt; /var/log/prs-backup.log\nfi\n</code></pre> <p>Configuration Backup</p> <p>Configuration files are automatically included in the weekly maintenance backup (<code>backup-maintenance.sh</code>). No separate configuration backup script is needed.</p>"},{"location":"operations/backup/#recovery-procedures","title":"Recovery Procedures","text":""},{"location":"operations/backup/#recovery","title":"Recovery","text":""},{"location":"operations/backup/#database-restore","title":"Database Restore","text":"<pre><code>#!/bin/bash\n# Full database restore procedure (ACTUAL WORKING SCRIPT: restore-database.sh)\n\n# Usage: ./restore-database.sh &lt;backup_file&gt; [target_database]\n# Example: ./restore-database.sh /mnt/hdd/postgres-backups/daily/prs_full_backup_20250825_020000.sql\n\nBACKUP_FILE=\"$1\"\nif [ -z \"$BACKUP_FILE\" ]; then\n    echo \"Usage: $0 &lt;backup_file&gt; [target_database]\"\n    echo \"Available backups:\"\n    ls -la /mnt/hdd/postgres-backups/daily/\n    exit 1\nfi\n\n# Run the actual restore script\n/opt/prs-deployment/scripts/restore-database.sh \"$BACKUP_FILE\"\n\n# The script automatically:\n# \u2705 Verifies backup file exists\n# \u2705 Checks backup integrity (SHA256)\n# \u2705 Stops application services safely\n# \u2705 Creates backup of current database\n# \u2705 Handles encrypted/compressed backups\n# \u2705 Restores database with proper user permissions\n# \u2705 Restarts services after restoration\n# \u2705 Logs all operations to /var/log/prs-restore.log\n\n# Manual restore example (what the script does internally):\n# 1. Stop services\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml stop backend frontend\n\n# 2. Verify backup integrity\nsha256sum -c \"${BACKUP_FILE}.sha256\"\n\n# 3. Restore database\ndocker exec prs-onprem-postgres-timescale pg_restore \\\n  -U prs_user \\\n  -d prs_production \\\n  --verbose \\\n  --clean \\\n  --if-exists \\\n  \"$BACKUP_FILE\"\n\n# 4. Restart services\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml start backend frontend\n</code></pre>"},{"location":"operations/backup/#recovery_1","title":"Recovery","text":"<pre><code>#!/bin/bash\n# Point-in-time recovery procedure\n\nTARGET_TIME=\"$1\"\nif [ -z \"$TARGET_TIME\" ]; then\n    echo \"Usage: $0 'YYYY-MM-DD HH:MM:SS'\"\n    exit 1\nfi\n\n# Stop PostgreSQL\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml stop postgres\n\n# Restore base backup\nLATEST_BASE_BACKUP=$(ls -t /mnt/hdd/postgres-backups/incremental/ | head -1)\nrm -rf /mnt/ssd/postgresql-data/*\ntar -xzf \"/mnt/hdd/postgres-backups/incremental/$LATEST_BASE_BACKUP\" -C /mnt/ssd/postgresql-data/\n\n# Create recovery configuration\ncat &gt; /mnt/ssd/postgresql-data/recovery.conf &lt;&lt; EOF\nrestore_command = 'cp /mnt/hdd/postgres-backups/wal/%f %p'\nrecovery_target_time = '$TARGET_TIME'\nrecovery_target_action = 'promote'\nEOF\n\n# Start PostgreSQL\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml start postgres\n\necho \"Point-in-time recovery initiated to: $TARGET_TIME\"\n</code></pre>"},{"location":"operations/backup/#system-recovery","title":"System Recovery","text":"<pre><code>#!/bin/bash\n# File system restore procedure\n\nBACKUP_DATE=\"$1\"\nif [ -z \"$BACKUP_DATE\" ]; then\n    echo \"Usage: $0 &lt;YYYYMMDD&gt;\"\n    exit 1\nfi\n\nBACKUP_DIR=\"/mnt/hdd/file-backups/$BACKUP_DATE\"\nif [ ! -d \"$BACKUP_DIR\" ]; then\n    echo \"Backup directory not found: $BACKUP_DIR\"\n    exit 1\nfi\n\n# Stop services\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml stop backend frontend\n\n# Restore files\nrsync -av \"$BACKUP_DIR/\" /mnt/ssd/uploads/\n\n# Start services\ndocker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml start backend frontend\n\necho \"File system restore completed from: $BACKUP_DATE\"\n</code></pre>"},{"location":"operations/backup/#backup-monitoring-and-verification","title":"Backup Monitoring and Verification","text":""},{"location":"operations/backup/#verification-script","title":"Verification Script","text":"<pre><code>#!/bin/bash\n# Daily backup verification script\n\nLOG_FILE=\"/var/log/prs-backup-verification.log\"\nDATE=$(date +%Y-%m-%d)\n\necho \"=== Backup Verification Report - $DATE ===\" &gt;&gt; \"$LOG_FILE\"\n\n# Check database backup\nLATEST_DB_BACKUP=$(ls -t /mnt/hdd/postgres-backups/daily/*.sql 2&gt;/dev/null | head -1)\nif [ -n \"$LATEST_DB_BACKUP\" ]; then\n    if [ -f \"${LATEST_DB_BACKUP}.sha256\" ]; then\n        cd \"$(dirname \"$LATEST_DB_BACKUP\")\"\n        if sha256sum -c \"$(basename \"${LATEST_DB_BACKUP}.sha256\")\" &gt;/dev/null 2&gt;&amp;1; then\n            echo \"\u2713 Database backup verified: $(basename \"$LATEST_DB_BACKUP\")\" &gt;&gt; \"$LOG_FILE\"\n        else\n            echo \"\u2717 Database backup checksum failed: $(basename \"$LATEST_DB_BACKUP\")\" &gt;&gt; \"$LOG_FILE\"\n        fi\n    else\n        echo \"\u2717 Database backup checksum missing: $(basename \"$LATEST_DB_BACKUP\")\" &gt;&gt; \"$LOG_FILE\"\n    fi\nelse\n    echo \"\u2717 No database backup found\" &gt;&gt; \"$LOG_FILE\"\nfi\n\n# Check application data backup\nLATEST_APP_BACKUP=$(ls -t /mnt/hdd/app-backups/*.tar.gz 2&gt;/dev/null | head -1)\nif [ -n \"$LATEST_APP_BACKUP\" ]; then\n    if tar -tzf \"$LATEST_APP_BACKUP\" &gt;/dev/null 2&gt;&amp;1; then\n        echo \"\u2713 Application backup verified: $(basename \"$LATEST_APP_BACKUP\")\" &gt;&gt; \"$LOG_FILE\"\n    else\n        echo \"\u2717 Application backup corrupted: $(basename \"$LATEST_APP_BACKUP\")\" &gt;&gt; \"$LOG_FILE\"\n    fi\nelse\n    echo \"\u2717 No application backup found\" &gt;&gt; \"$LOG_FILE\"\nfi\n\n# Check file backup\nif [ -d \"/mnt/hdd/file-backups/latest\" ]; then\n    FILE_COUNT=$(find /mnt/hdd/file-backups/latest -type f | wc -l)\n    echo \"\u2713 File backup verified: $FILE_COUNT files\" &gt;&gt; \"$LOG_FILE\"\nelse\n    echo \"\u2717 No file backup found\" &gt;&gt; \"$LOG_FILE\"\nfi\n\necho \"\" &gt;&gt; \"$LOG_FILE\"\n</code></pre>"},{"location":"operations/backup/#storage-monitoring","title":"Storage Monitoring","text":"<pre><code># Monitor backup storage usage\ndf -h /mnt/hdd | grep -E \"(Filesystem|/mnt/hdd)\"\n\n# Check backup directory sizes\ndu -sh /mnt/hdd/postgres-backups/\ndu -sh /mnt/hdd/app-backups/\n\n# Monitor backup growth trends\nfind /mnt/hdd/postgres-backups/daily -name \"*.sql\" -mtime -7 -exec ls -lh {} \\; | awk '{print $5, $9}'\n</code></pre>"},{"location":"operations/backup/#zero-deletion-policy-management","title":"Zero Deletion Policy Management","text":""},{"location":"operations/backup/#three-tier-storage-monitoring","title":"Three-Tier Storage Monitoring","text":"<p>Monitor all storage tiers with zero deletion policy:</p> <pre><code># Monitor all storage tiers\necho \"=== SSD (Hot Data) ===\"\ndf -h /mnt/ssd | grep -E \"(Filesystem|/mnt/ssd)\"\ndu -sh /mnt/ssd/postgresql-data/\ndu -sh /mnt/ssd/uploads/\n\necho \"=== HDD (Cold Data/Backups) ===\"\ndf -h /mnt/hdd | grep -E \"(Filesystem|/mnt/hdd)\"\ndu -sh /mnt/hdd/postgres-backups/\ndu -sh /mnt/hdd/app-backups/\ndu -sh /mnt/hdd/archive/\n\necho \"=== NAS (Backup/DR) ===\"\nif mountpoint -q /mnt/nas; then\n    df -h /mnt/nas | grep -E \"(Filesystem|/mnt/nas)\"\n    du -sh /mnt/nas/postgres-backups/\n    du -sh /mnt/nas/app-backups/\n    du -sh /mnt/nas/disaster-recovery/\nelse\n    echo \"NAS not mounted\"\nfi\n\n# Count backups across tiers\necho \"=== Backup Counts ===\"\necho \"HDD Database backups: $(find /mnt/hdd/postgres-backups -name \"*.sql*\" | wc -l)\"\necho \"HDD Application backups: $(find /mnt/hdd/app-backups -type d -name \"20*\" | wc -l)\"\necho \"Archive Database backups: $(find /mnt/hdd/archive -name \"*.sql*\" 2&gt;/dev/null | wc -l)\"\necho \"Archive Application backups: $(find /mnt/hdd/archive -type d -name \"20*\" 2&gt;/dev/null | wc -l)\"\n</code></pre>"},{"location":"operations/backup/#three-tier-data-lifecycle-management","title":"Three-Tier Data Lifecycle Management","text":"<p>Data Flow: SSD (Hot) \u2192 HDD (Cold) \u2192 NAS (Backup/DR)</p>"},{"location":"operations/backup/#when-hdd-storage-approaches-capacity-80","title":"When HDD Storage Approaches Capacity (&gt;80%)","text":"<pre><code># 1. Create archive structure on HDD\nmkdir -p /mnt/hdd/archive/{postgres-backups,app-backups}\n\n# 2. Move old backups to HDD archive (90+ days)\nfind /mnt/hdd/postgres-backups/daily -name \"*.sql*\" -mtime +90 -exec mv {} /mnt/hdd/archive/postgres-backups/ \\;\nfind /mnt/hdd/app-backups -type d -name \"20*\" -mtime +90 -exec mv {} /mnt/hdd/archive/app-backups/ \\;\n\n# 3. Compress archived backups to save HDD space\nfind /mnt/hdd/archive/postgres-backups -name \"*.sql\" ! -name \"*.gz\" -exec gzip {} \\;\n\n# 4. Sync archives to NAS for disaster recovery\nrsync -av /mnt/hdd/archive/ /mnt/nas/disaster-recovery/archive/\n\n# 5. Verify NAS sync completed successfully\necho \"Archive sync completed: $(date)\" &gt;&gt; /var/log/prs-archive.log\n</code></pre>"},{"location":"operations/backup/#when-hdd-archive-approaches-capacity-90","title":"When HDD Archive Approaches Capacity (&gt;90%)","text":"<pre><code># Move very old archives (1+ year) to NAS-only storage\nfind /mnt/hdd/archive -name \"*.sql.gz\" -mtime +365 -exec rsync {} /mnt/nas/disaster-recovery/long-term/ \\;\nfind /mnt/hdd/archive -name \"*.sql.gz\" -mtime +365 -delete  # Only after NAS verification\n\n# Note: This is the ONLY deletion allowed under zero deletion policy -\n# and only after confirmed NAS backup\n</code></pre>"},{"location":"operations/backup/#storage-alerts","title":"Storage Alerts","text":"<p>Set up alerts when storage reaches thresholds:</p> <pre><code># Add to crontab for storage monitoring\n0 8 * * * /opt/prs-deployment/scripts/check-storage-capacity.sh\n</code></pre>"},{"location":"operations/backup/#backup-troubleshooting","title":"Backup Troubleshooting","text":""},{"location":"operations/backup/#backup-issues","title":"Backup Issues","text":""},{"location":"operations/backup/#script-failures","title":"Script Failures","text":"<pre><code># Check backup logs\ntail -50 /var/log/prs-backup.log\n\n# Check disk space\ndf -h /mnt/hdd\n\n# Check permissions\nls -la /mnt/hdd/postgres-backups/\n\n# Test database connectivity\ndocker exec prs-onprem-postgres-timescale pg_isready -U prs_admin\n</code></pre>"},{"location":"operations/backup/#space-issues","title":"Space Issues","text":"<pre><code># Three-tier storage management - NEVER delete, only move between tiers\n# SSD \u2192 HDD \u2192 NAS data flow\n\n# Check storage capacity across all tiers\ndf -h /mnt/ssd /mnt/hdd /mnt/nas\n\n# Move data between tiers based on capacity\nif [ $(df /mnt/ssd | awk 'NR==2{print $5}' | sed 's/%//') -gt 80 ]; then\n    echo \"SSD &gt;80% - Consider moving old data to HDD\"\nfi\n\nif [ $(df /mnt/hdd | awk 'NR==2{print $5}' | sed 's/%//') -gt 80 ]; then\n    echo \"HDD &gt;80% - Archive old backups\"\n    mkdir -p /mnt/hdd/archive/{postgres-backups,app-backups}\n    find /mnt/hdd/postgres-backups/daily -name \"*.sql*\" -mtime +90 -exec mv {} /mnt/hdd/archive/postgres-backups/ \\;\n    find /mnt/hdd/app-backups -type d -name \"20*\" -mtime +90 -exec mv {} /mnt/hdd/archive/app-backups/ \\;\nfi\n\n# Compress old backups\nfind /mnt/hdd/postgres-backups/daily -name \"*.sql\" -mtime +7 ! -name \"*.gz\" -exec gzip {} \\;\n</code></pre>"},{"location":"operations/backup/#corruption","title":"Corruption","text":"<pre><code># Verify backup integrity\nsha256sum -c /mnt/hdd/postgres-backups/daily/*.sha256\n\n# Test restore on backup database\ndocker run --rm -v /mnt/hdd/postgres-backups:/backups postgres:15 \\\n  pg_restore --list /backups/daily/latest_backup.sql\n</code></pre> <p>Backup Strategy</p> <p>The comprehensive backup strategy ensures zero data loss with multiple recovery options including full restore, point-in-time recovery, and incremental restoration.</p> <p>Regular Testing</p> <p>Backup procedures should be tested monthly to ensure recovery capabilities are working correctly.</p>"},{"location":"operations/daily/","title":"Daily Operations","text":""},{"location":"operations/daily/#overview","title":"Overview","text":"<p>This guide covers the daily operational tasks required to maintain the PRS on-premises deployment in optimal condition.</p>"},{"location":"operations/daily/#daily-operations-schedule","title":"Daily Operations Schedule","text":""},{"location":"operations/daily/#routine-800-am","title":"Routine (8:00 AM)","text":""},{"location":"operations/daily/#health-check","title":"Health Check","text":"<pre><code># Run automated health check\ncd /opt/prs-deployment/scripts\n./system-health-check.sh\n\n# Check service status\ndocker-compose -f ../02-docker-configuration/docker-compose.onprem.yml ps\n\n# Verify all services are healthy\ndocker-compose -f ../02-docker-configuration/docker-compose.onprem.yml ps --filter \"health=healthy\"\n</code></pre>"},{"location":"operations/daily/#monitoring","title":"Monitoring","text":"<pre><code># Check storage usage\ndf -h /mnt/ssd /mnt/hdd\n\n# Check RAID status\ncat /proc/mdstat\n\n# Monitor storage alerts\ngrep -i \"storage\\|disk\\|raid\" /var/log/syslog | tail -20\n</code></pre>"},{"location":"operations/daily/#health-check_1","title":"Health Check","text":"<pre><code>-- Connect to database\ndocker exec -it prs-onprem-postgres-timescale psql -U prs_admin -d prs_production\n\n-- Check database status\nSELECT version();\nSELECT * FROM timescaledb_information.license;\n\n-- Check active connections\nSELECT count(*) as active_connections FROM pg_stat_activity WHERE state = 'active';\n\n-- Check database size\nSELECT pg_size_pretty(pg_database_size('prs_production')) as database_size;\n\n-- Check recent activity\nSELECT COUNT(*) as recent_records FROM notifications WHERE created_at &gt;= NOW() - INTERVAL '24 hours';\n\n-- Check TimescaleDB compression status\nSELECT * FROM timescaledb_status ORDER BY total_size_mb DESC LIMIT 10;\n\n-- Check chunk compression effectiveness\nSELECT\n    hypertable_name,\n    total_chunks,\n    compressed_chunks,\n    ROUND((compressed_chunks::numeric / total_chunks * 100), 1) as compression_percentage\nFROM timescaledb_status\nWHERE total_chunks &gt; 0\nORDER BY compression_percentage ASC;\n</code></pre>"},{"location":"operations/daily/#check-1200-pm","title":"Check (12:00 PM)","text":""},{"location":"operations/daily/#monitoring_1","title":"Monitoring","text":"<pre><code># Check system resources\nhtop\n\n# Monitor container resources\ndocker stats --no-stream\n\n# Check network usage\niftop -t -s 10\n\n# Review application logs\ndocker logs prs-onprem-backend --tail 50 | grep -E \"(ERROR|WARN)\"\n</code></pre>"},{"location":"operations/daily/#health","title":"Health","text":"<pre><code># Test API endpoints\ncurl -s https://your-domain.com/api/health | jq '.'\n\n# Check response times\ntime curl -s https://your-domain.com/api/health\n\n# Test database connectivity\ndocker exec prs-onprem-backend npm run db:test\n\n# Check Redis connectivity\ndocker exec prs-onprem-redis redis-cli -a $REDIS_PASSWORD ping\n</code></pre>"},{"location":"operations/daily/#review-600-pm","title":"Review (6:00 PM)","text":""},{"location":"operations/daily/#analysis","title":"Analysis","text":"<pre><code># Review system logs\nsudo journalctl --since \"today\" --priority=err\n\n# Check Docker logs\ndocker-compose -f ../02-docker-configuration/docker-compose.onprem.yml logs --since=\"24h\" | grep -E \"(ERROR|FATAL|CRITICAL)\"\n\n# Review nginx access logs\ndocker logs prs-onprem-nginx --tail 100 | grep -v \"200\\|304\"\n\n# Check database logs\ndocker logs prs-onprem-postgres-timescale --tail 50 | grep -E \"(ERROR|FATAL|WARNING)\"\n</code></pre>"},{"location":"operations/daily/#verification","title":"Verification","text":"<pre><code># Check backup status\nls -la /mnt/hdd/postgres-backups/ | head -10\n\n# Verify latest backup\nLATEST_BACKUP=$(ls -t /mnt/hdd/postgres-backups/*.sql | head -1)\necho \"Latest backup: $LATEST_BACKUP\"\nls -lh \"$LATEST_BACKUP\"\n\n# Check backup logs\ntail -20 /var/log/prs-backup.log\n</code></pre>"},{"location":"operations/daily/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":""},{"location":"operations/daily/#metrics","title":"Metrics","text":"Metric Target Warning Critical Action CPU Usage &lt;60% &gt;70% &gt;85% Investigate high CPU processes Memory Usage &lt;75% &gt;80% &gt;90% Check for memory leaks SSD Usage &lt;80% &gt;85% &gt;90% Archive old data HDD Usage &lt;70% &gt;80% &gt;90% Expand storage Network Usage &lt;50% &gt;70% &gt;90% Check network traffic"},{"location":"operations/daily/#metrics_1","title":"Metrics","text":"Metric Target Warning Critical Action Response Time &lt;200ms &gt;500ms &gt;1000ms Performance tuning Error Rate &lt;0.1% &gt;1% &gt;5% Investigate errors Active Users Variable &gt;80 concurrent &gt;100 concurrent Monitor capacity Database Connections &lt;100 &gt;120 &gt;140 Check connection pooling"},{"location":"operations/daily/#metrics_2","title":"Metrics","text":"<pre><code>-- Check slow queries\nSELECT query, calls, total_time, mean_time, rows\nFROM pg_stat_statements\nORDER BY total_time DESC\nLIMIT 10;\n\n-- Check table sizes\nSELECT\n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\nFROM pg_tables\nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\nLIMIT 10;\n\n-- Check TimescaleDB chunk status\nSELECT\n    hypertable_name,\n    COUNT(*) as total_chunks,\n    COUNT(*) FILTER (WHERE is_compressed) as compressed_chunks,\n    COUNT(*) FILTER (WHERE tablespace_name = 'ssd_hot') as ssd_chunks,\n    COUNT(*) FILTER (WHERE tablespace_name = 'hdd_cold') as hdd_chunks\nFROM timescaledb_information.chunks\nGROUP BY hypertable_name;\n</code></pre>"},{"location":"operations/daily/#daily-maintenance-tasks","title":"Daily Maintenance Tasks","text":""},{"location":"operations/daily/#tasks-via-cron","title":"Tasks (via cron)","text":"<pre><code># View current cron jobs\ncrontab -l\n\n# Expected daily tasks:\n# 0 2 * * * /opt/prs-deployment/scripts/backup-maintenance.sh\n# 0 3 * * * /opt/prs-deployment/scripts/log-rotation.sh\n# 0 4 * * * /opt/prs-deployment/scripts/cleanup-temp-files.sh\n# 0 5 * * * /opt/prs-deployment/scripts/update-statistics.sh\n</code></pre>"},{"location":"operations/daily/#tasks","title":"Tasks","text":""},{"location":"operations/daily/#rotation","title":"Rotation","text":"<pre><code># Rotate application logs\ndocker exec prs-onprem-backend logrotate /etc/logrotate.conf\n\n# Clean old Docker logs\ndocker system prune -f --filter \"until=24h\"\n\n# Archive old logs to HDD\nfind /mnt/ssd/logs -name \"*.log\" -mtime +7 -exec mv {} /mnt/hdd/app-logs-archive/ \\;\n</code></pre>"},{"location":"operations/daily/#maintenance","title":"Maintenance","text":"<pre><code>-- Update table statistics\nANALYZE notifications;\nANALYZE audit_logs;\nANALYZE requisitions;\n\n-- Check for bloated tables\nSELECT\n    schemaname,\n    tablename,\n    n_tup_ins,\n    n_tup_upd,\n    n_tup_del,\n    n_dead_tup\nFROM pg_stat_user_tables\nWHERE n_dead_tup &gt; 1000\nORDER BY n_dead_tup DESC;\n\n-- Vacuum if needed\nVACUUM ANALYZE notifications;\n</code></pre>"},{"location":"operations/daily/#maintenance_1","title":"Maintenance","text":"<pre><code># Check Redis memory usage\ndocker exec prs-onprem-redis redis-cli -a $REDIS_PASSWORD info memory\n\n# Clear expired keys\ndocker exec prs-onprem-redis redis-cli -a $REDIS_PASSWORD eval \"return #redis.call('keys', ARGV[1])\" 0 \"*expired*\"\n\n# Check cache hit ratio\ndocker exec prs-onprem-redis redis-cli -a $REDIS_PASSWORD info stats | grep keyspace\n</code></pre>"},{"location":"operations/daily/#alert-response-procedures","title":"Alert Response Procedures","text":""},{"location":"operations/daily/#cpu-usage","title":"CPU Usage","text":"<pre><code># Identify CPU-intensive processes\ntop -o %CPU\n\n# Check container CPU usage\ndocker stats --format \"table {{.Container}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\"\n\n# If backend is high CPU:\ndocker exec prs-onprem-backend pm2 list\ndocker exec prs-onprem-backend pm2 monit\n</code></pre>"},{"location":"operations/daily/#memory-usage","title":"Memory Usage","text":"<pre><code># Check memory usage by container\ndocker stats --format \"table {{.Container}}\\t{{.MemUsage}}\\t{{.MemPerc}}\"\n\n# Check for memory leaks in backend\ndocker exec prs-onprem-backend node --expose-gc -e \"global.gc(); console.log(process.memoryUsage());\"\n\n# Restart service if memory leak detected\ndocker-compose -f ../02-docker-configuration/docker-compose.onprem.yml restart backend\n</code></pre>"},{"location":"operations/daily/#alerts","title":"Alerts","text":"<pre><code># If SSD usage &gt;85%\n# 1. Check for large files\nfind /mnt/ssd -type f -size +100M -exec ls -lh {} \\;\n\n# 2. Compress old logs\nfind /mnt/ssd/logs -name \"*.log\" -mtime +1 -exec gzip {} \\;\n\n# 3. Move old data to HDD\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT move_chunk(chunk_name, 'hdd_cold')\nFROM timescaledb_information.chunks\nWHERE range_start &lt; NOW() - INTERVAL '14 days'\nAND tablespace_name = 'ssd_hot'\nLIMIT 10;\n\"\n</code></pre>"},{"location":"operations/daily/#connection-issues","title":"Connection Issues","text":"<pre><code># Check connection count\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT count(*) as connections, state\nFROM pg_stat_activity\nGROUP BY state;\n\"\n\n# Kill idle connections if needed\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT pg_terminate_backend(pid)\nFROM pg_stat_activity\nWHERE state = 'idle'\nAND query_start &lt; NOW() - INTERVAL '1 hour';\n\"\n\n# Restart backend if connection pool issues\ndocker-compose -f ../02-docker-configuration/docker-compose.onprem.yml restart backend\n</code></pre>"},{"location":"operations/daily/#daily-checklist","title":"Daily Checklist","text":""},{"location":"operations/daily/#800-am","title":"(8:00 AM)","text":"<ul> <li>[ ] Run system health check script</li> <li>[ ] Verify all Docker services are running</li> <li>[ ] Check storage usage (SSD &lt;85%, HDD &lt;80%)</li> <li>[ ] Verify database connectivity</li> <li>[ ] Check overnight backup completion</li> <li>[ ] Review system logs for errors</li> <li>[ ] Test application endpoints</li> </ul>"},{"location":"operations/daily/#1200-pm","title":"(12:00 PM)","text":"<ul> <li>[ ] Monitor system performance metrics</li> <li>[ ] Check application response times</li> <li>[ ] Review error logs</li> <li>[ ] Verify user activity levels</li> <li>[ ] Check Redis cache performance</li> <li>[ ] Monitor network usage</li> </ul>"},{"location":"operations/daily/#600-pm","title":"(6:00 PM)","text":"<ul> <li>[ ] Review daily log summaries</li> <li>[ ] Verify backup integrity</li> <li>[ ] Check TimescaleDB compression status</li> <li>[ ] Review performance metrics</li> <li>[ ] Plan any needed maintenance</li> <li>[ ] Update operational notes</li> </ul>"},{"location":"operations/daily/#daily-report-template","title":"Daily Report Template","text":"<pre><code># Generate daily report\ncat &gt; /tmp/daily-report-$(date +%Y%m%d).txt &lt;&lt; EOF\nPRS Daily Operations Report - $(date +%Y-%m-%d)\n================================================\n\nSystem Status:\n- CPU Usage: $(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | cut -d'%' -f1)%\n- Memory Usage: $(free | grep Mem | awk '{printf(\"%.1f%%\", $3/$2 * 100.0)}')\n- SSD Usage: $(df -h /mnt/ssd | awk 'NR==2{print $5}')\n- HDD Usage: $(df -h /mnt/hdd | awk 'NR==2{print $5}')\n\nService Status:\n$(docker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml ps --format table)\n\nDatabase Status:\n- Active Connections: $(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT count(*) FROM pg_stat_activity WHERE state = 'active';\" | xargs)\n- Database Size: $(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT pg_size_pretty(pg_database_size('prs_production'));\" | xargs)\n\nTimescaleDB Status:\n- Total Hypertables: $(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT count(*) FROM timescaledb_information.hypertables;\" | xargs)\n- Total Chunks: $(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT count(*) FROM timescaledb_information.chunks;\" | xargs)\n- Compressed Chunks: $(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT count(*) FROM timescaledb_information.chunks WHERE is_compressed;\" | xargs)\n- Compression Ratio: $(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT ROUND((SELECT count(*)::numeric FROM timescaledb_information.chunks WHERE is_compressed) / (SELECT count(*) FROM timescaledb_information.chunks) * 100, 1) || '%';\" | xargs)\n\nBackup Status:\n- Latest Backup: $(ls -t /mnt/hdd/postgres-backups/*.sql 2&gt;/dev/null | head -1 | xargs basename)\n- Backup Size: $(ls -lh /mnt/hdd/postgres-backups/*.sql 2&gt;/dev/null | head -1 | awk '{print $5}')\n\nIssues Found:\n$(grep -i \"error\\|critical\\|fatal\" /var/log/syslog | tail -5 || echo \"No critical issues found\")\n\nEOF\n\n# Email report (if configured)\n# mail -s \"PRS Daily Report - $(date +%Y-%m-%d)\" admin@your-domain.com &lt; /tmp/daily-report-$(date +%Y%m%d).txt\n</code></pre> <p>Automation</p> <p>Most daily tasks can be automated using cron jobs and monitoring scripts. Focus manual effort on reviewing metrics and investigating any anomalies.</p> <p>Escalation</p> <p>If any critical thresholds are exceeded or services are unresponsive, follow the escalation procedures in the troubleshooting guide.</p>"},{"location":"operations/health-checks/","title":"Health Checks","text":""},{"location":"operations/health-checks/#overview","title":"Overview","text":"<p>This guide covers comprehensive health check procedures for the PRS on-premises deployment, including automated monitoring, manual validation, and proactive system health management.</p>"},{"location":"operations/health-checks/#health-check-architecture","title":"Health Check Architecture","text":"<pre><code>graph TB\n    subgraph \"Health Check Layers\"\n        INFRA[Infrastructure Health&lt;br/&gt;CPU, Memory, Disk, Network]\n        SERVICES[Service Health&lt;br/&gt;Docker, Database, Cache]\n        APP[Application Health&lt;br/&gt;API, Frontend, Backend]\n        BUSINESS[Business Health&lt;br/&gt;Workflows, Data Integrity]\n    end\n\n    subgraph \"Monitoring Systems\"\n        INFRA --&gt; PROMETHEUS[Prometheus Metrics]\n        SERVICES --&gt; PROMETHEUS\n        APP --&gt; PROMETHEUS\n        BUSINESS --&gt; PROMETHEUS\n\n        PROMETHEUS --&gt; GRAFANA[Grafana Dashboards]\n        PROMETHEUS --&gt; ALERTS[Alert Manager]\n    end\n\n    subgraph \"Health Check Scripts\"\n        MANUAL[Manual Health Checks&lt;br/&gt;./system-health-check.sh]\n        AUTO[Automated Checks&lt;br/&gt;Cron Jobs]\n        CUSTOM[Custom Checks&lt;br/&gt;Business Logic]\n    end\n\n    subgraph \"Notification Channels\"\n        ALERTS --&gt; EMAIL[Email Alerts]\n        ALERTS --&gt; SLACK[Slack Notifications]\n        ALERTS --&gt; WEBHOOK[Webhook Endpoints]\n    end\n\n    style PROMETHEUS fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style GRAFANA fill:#e8f5e8,stroke:#4caf50,stroke-width:2px\n    style ALERTS fill:#fff3e0,stroke:#ff9800,stroke-width:2px</code></pre>"},{"location":"operations/health-checks/#automated-health-checks","title":"Automated Health Checks","text":""},{"location":"operations/health-checks/#system-health-check-script","title":"System Health Check Script","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/system-health-check.sh\n\nset -euo pipefail\n\n# Configuration\nHEALTH_LOG=\"/var/log/prs-health.log\"\nMETRICS_FILE=\"/tmp/prs-health-metrics.prom\"\nALERT_EMAIL=\"admin@your-domain.com\"\nCOMPONENT=\"$1\"\n\n# Colors for output\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nNC='\\033[0m' # No Color\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$HEALTH_LOG\"\n}\n\nprint_status() {\n    local status=\"$1\"\n    local message=\"$2\"\n\n    case \"$status\" in\n        \"OK\")\n            echo -e \"${GREEN}\u2713${NC} $message\"\n            ;;\n        \"WARNING\")\n            echo -e \"${YELLOW}\u26a0${NC} $message\"\n            ;;\n        \"ERROR\")\n            echo -e \"${RED}\u2717${NC} $message\"\n            ;;\n    esac\n}\n\n# Infrastructure Health Checks\ncheck_infrastructure() {\n    echo \"=== Infrastructure Health ===\"\n\n    # CPU Usage\n    CPU_USAGE=$(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | sed 's/%us,//')\n    if (( $(echo \"$CPU_USAGE &gt; 80\" | bc -l) )); then\n        print_status \"WARNING\" \"High CPU usage: ${CPU_USAGE}%\"\n    else\n        print_status \"OK\" \"CPU usage: ${CPU_USAGE}%\"\n    fi\n\n    # Memory Usage\n    MEMORY_USAGE=$(free | grep Mem | awk '{printf \"%.1f\", $3/$2 * 100.0}')\n    if (( $(echo \"$MEMORY_USAGE &gt; 85\" | bc -l) )); then\n        print_status \"WARNING\" \"High memory usage: ${MEMORY_USAGE}%\"\n    else\n        print_status \"OK\" \"Memory usage: ${MEMORY_USAGE}%\"\n    fi\n\n    # Disk Usage\n    SSD_USAGE=$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\n    HDD_USAGE=$(df /mnt/hdd | awk 'NR==2 {print $5}' | sed 's/%//')\n\n    if [ \"$SSD_USAGE\" -gt 90 ]; then\n        print_status \"ERROR\" \"SSD usage critical: ${SSD_USAGE}%\"\n    elif [ \"$SSD_USAGE\" -gt 85 ]; then\n        print_status \"WARNING\" \"SSD usage high: ${SSD_USAGE}%\"\n    else\n        print_status \"OK\" \"SSD usage: ${SSD_USAGE}%\"\n    fi\n\n    if [ \"$HDD_USAGE\" -gt 85 ]; then\n        print_status \"WARNING\" \"HDD usage high: ${HDD_USAGE}%\"\n    else\n        print_status \"OK\" \"HDD usage: ${HDD_USAGE}%\"\n    fi\n\n    # RAID Status\n    if [ -f /proc/mdstat ]; then\n        RAID_STATUS=$(cat /proc/mdstat | grep -E \"(md0|md1)\" | grep -c \"active\")\n        if [ \"$RAID_STATUS\" -eq 2 ]; then\n            print_status \"OK\" \"RAID arrays active\"\n        else\n            print_status \"ERROR\" \"RAID array issues detected\"\n        fi\n    fi\n\n    # Network Connectivity\n    if ping -c 1 8.8.8.8 &gt;/dev/null 2&gt;&amp;1; then\n        print_status \"OK\" \"External network connectivity\"\n    else\n        print_status \"ERROR\" \"No external network connectivity\"\n    fi\n}\n\n# Service Health Checks\ncheck_services() {\n    echo \"=== Service Health ===\"\n\n    # Docker Service\n    if systemctl is-active docker &gt;/dev/null 2&gt;&amp;1; then\n        print_status \"OK\" \"Docker service running\"\n    else\n        print_status \"ERROR\" \"Docker service not running\"\n        return 1\n    fi\n\n    # Container Status\n    local containers=(\n        \"prs-onprem-nginx\"\n        \"prs-onprem-frontend\"\n        \"prs-onprem-backend\"\n        \"prs-onprem-postgres-timescale\"\n        \"prs-onprem-redis\"\n    )\n\n    for container in \"${containers[@]}\"; do\n        if docker ps --filter \"name=$container\" --filter \"status=running\" | grep -q \"$container\"; then\n            print_status \"OK\" \"$container running\"\n        else\n            print_status \"ERROR\" \"$container not running\"\n        fi\n    done\n\n    # Database Connectivity\n    if docker exec prs-onprem-postgres-timescale pg_isready -U prs_admin &gt;/dev/null 2&gt;&amp;1; then\n        print_status \"OK\" \"Database connectivity\"\n\n        # Database Connection Count\n        CONNECTIONS=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT count(*) FROM pg_stat_activity;\" 2&gt;/dev/null | xargs)\n        if [ \"$CONNECTIONS\" -gt 120 ]; then\n            print_status \"WARNING\" \"High database connections: $CONNECTIONS\"\n        else\n            print_status \"OK\" \"Database connections: $CONNECTIONS\"\n        fi\n    else\n        print_status \"ERROR\" \"Database connectivity failed\"\n    fi\n\n    # Redis Connectivity\n    if docker exec prs-onprem-redis redis-cli ping &gt;/dev/null 2&gt;&amp;1; then\n        print_status \"OK\" \"Redis connectivity\"\n    else\n        print_status \"ERROR\" \"Redis connectivity failed\"\n    fi\n}\n\n# Application Health Checks\ncheck_application() {\n    echo \"=== Application Health ===\"\n\n    # API Health Endpoint\n    if curl -f -s https://localhost/api/health &gt;/dev/null 2&gt;&amp;1; then\n        print_status \"OK\" \"API health endpoint\"\n\n        # API Response Time\n        RESPONSE_TIME=$(curl -w \"%{time_total}\" -o /dev/null -s https://localhost/api/health)\n        if (( $(echo \"$RESPONSE_TIME &gt; 2.0\" | bc -l) )); then\n            print_status \"WARNING\" \"Slow API response: ${RESPONSE_TIME}s\"\n        else\n            print_status \"OK\" \"API response time: ${RESPONSE_TIME}s\"\n        fi\n    else\n        print_status \"ERROR\" \"API health endpoint failed\"\n    fi\n\n    # Frontend Accessibility\n    if curl -f -s https://localhost/ &gt;/dev/null 2&gt;&amp;1; then\n        print_status \"OK\" \"Frontend accessibility\"\n    else\n        print_status \"ERROR\" \"Frontend not accessible\"\n    fi\n\n    # SSL Certificate\n    CERT_EXPIRY=$(openssl x509 -in /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt -noout -enddate 2&gt;/dev/null | cut -d= -f2)\n    if [ -n \"$CERT_EXPIRY\" ]; then\n        EXPIRY_EPOCH=$(date -d \"$CERT_EXPIRY\" +%s)\n        CURRENT_EPOCH=$(date +%s)\n        DAYS_UNTIL_EXPIRY=$(( (EXPIRY_EPOCH - CURRENT_EPOCH) / 86400 ))\n\n        if [ \"$DAYS_UNTIL_EXPIRY\" -lt 7 ]; then\n            print_status \"ERROR\" \"SSL certificate expires in $DAYS_UNTIL_EXPIRY days\"\n        elif [ \"$DAYS_UNTIL_EXPIRY\" -lt 30 ]; then\n            print_status \"WARNING\" \"SSL certificate expires in $DAYS_UNTIL_EXPIRY days\"\n        else\n            print_status \"OK\" \"SSL certificate valid for $DAYS_UNTIL_EXPIRY days\"\n        fi\n    else\n        print_status \"ERROR\" \"SSL certificate check failed\"\n    fi\n}\n\n# Business Logic Health Checks\ncheck_business_logic() {\n    echo \"=== Business Logic Health ===\"\n\n    # Database Table Counts\n    USER_COUNT=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT count(*) FROM users;\" 2&gt;/dev/null | xargs)\n    if [ \"$USER_COUNT\" -gt 0 ]; then\n        print_status \"OK\" \"User data present: $USER_COUNT users\"\n    else\n        print_status \"WARNING\" \"No user data found\"\n    fi\n\n    # Recent Activity\n    RECENT_ACTIVITY=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT count(*) FROM audit_logs \n    WHERE created_at &gt;= NOW() - INTERVAL '24 hours';\" 2&gt;/dev/null | xargs)\n\n    if [ \"$RECENT_ACTIVITY\" -gt 0 ]; then\n        print_status \"OK\" \"Recent system activity: $RECENT_ACTIVITY events\"\n    else\n        print_status \"WARNING\" \"No recent system activity\"\n    fi\n\n    # Data Integrity Checks\n    ORPHANED_RECORDS=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT count(*) FROM requisitions r \n    LEFT JOIN users u ON r.user_id = u.id \n    WHERE u.id IS NULL;\" 2&gt;/dev/null | xargs)\n\n    if [ \"$ORPHANED_RECORDS\" -eq 0 ]; then\n        print_status \"OK\" \"Data integrity check passed\"\n    else\n        print_status \"WARNING\" \"Found $ORPHANED_RECORDS orphaned records\"\n    fi\n}\n\n# Performance Health Checks\ncheck_performance() {\n    echo \"=== Performance Health ===\"\n\n    # Database Cache Hit Ratio\n    CACHE_HIT_RATIO=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT round(100.0 * sum(blks_hit) / nullif(sum(blks_hit) + sum(blks_read), 0), 2)\n    FROM pg_stat_database WHERE datname = 'prs_production';\" 2&gt;/dev/null | xargs)\n\n    if (( $(echo \"$CACHE_HIT_RATIO &lt; 95\" | bc -l) )); then\n        print_status \"WARNING\" \"Low cache hit ratio: ${CACHE_HIT_RATIO}%\"\n    else\n        print_status \"OK\" \"Cache hit ratio: ${CACHE_HIT_RATIO}%\"\n    fi\n\n    # TimescaleDB Compression\n    COMPRESSION_RATIO=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT round(AVG((before_compression_total_bytes::numeric - after_compression_total_bytes::numeric) \n    / before_compression_total_bytes::numeric * 100), 2)\n    FROM timescaledb_information.compressed_hypertable_stats;\" 2&gt;/dev/null | xargs)\n\n    if [ -n \"$COMPRESSION_RATIO\" ] &amp;&amp; (( $(echo \"$COMPRESSION_RATIO &gt; 50\" | bc -l) )); then\n        print_status \"OK\" \"Compression ratio: ${COMPRESSION_RATIO}%\"\n    else\n        print_status \"WARNING\" \"Low compression ratio: ${COMPRESSION_RATIO}%\"\n    fi\n\n    # Load Average\n    LOAD_AVERAGE=$(uptime | awk -F'load average:' '{print $2}' | awk '{print $1}' | sed 's/,//')\n    CPU_CORES=$(nproc)\n\n    if (( $(echo \"$LOAD_AVERAGE &gt; $CPU_CORES\" | bc -l) )); then\n        print_status \"WARNING\" \"High load average: $LOAD_AVERAGE (cores: $CPU_CORES)\"\n    else\n        print_status \"OK\" \"Load average: $LOAD_AVERAGE (cores: $CPU_CORES)\"\n    fi\n}\n\n# Generate Health Metrics\ngenerate_metrics() {\n    &gt; \"$METRICS_FILE\"\n\n    # System metrics\n    echo \"prs_cpu_usage_percent $(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | sed 's/%us,//')\" &gt;&gt; \"$METRICS_FILE\"\n    echo \"prs_memory_usage_percent $(free | grep Mem | awk '{printf \"%.1f\", $3/$2 * 100.0}')\" &gt;&gt; \"$METRICS_FILE\"\n    echo \"prs_ssd_usage_percent $(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\" &gt;&gt; \"$METRICS_FILE\"\n    echo \"prs_hdd_usage_percent $(df /mnt/hdd | awk 'NR==2 {print $5}' | sed 's/%//')\" &gt;&gt; \"$METRICS_FILE\"\n\n    # Service metrics\n    for container in prs-onprem-nginx prs-onprem-frontend prs-onprem-backend prs-onprem-postgres-timescale prs-onprem-redis; do\n        if docker ps --filter \"name=$container\" --filter \"status=running\" | grep -q \"$container\"; then\n            echo \"prs_service_up{service=\\\"$container\\\"} 1\" &gt;&gt; \"$METRICS_FILE\"\n        else\n            echo \"prs_service_up{service=\\\"$container\\\"} 0\" &gt;&gt; \"$METRICS_FILE\"\n        fi\n    done\n\n    # Application metrics\n    if curl -f -s https://localhost/api/health &gt;/dev/null 2&gt;&amp;1; then\n        echo \"prs_api_healthy 1\" &gt;&gt; \"$METRICS_FILE\"\n        RESPONSE_TIME=$(curl -w \"%{time_total}\" -o /dev/null -s https://localhost/api/health)\n        echo \"prs_api_response_time_seconds $RESPONSE_TIME\" &gt;&gt; \"$METRICS_FILE\"\n    else\n        echo \"prs_api_healthy 0\" &gt;&gt; \"$METRICS_FILE\"\n    fi\n\n    # Copy metrics for Prometheus\n    if [ -d \"/var/lib/node_exporter/textfile_collector\" ]; then\n        cp \"$METRICS_FILE\" /var/lib/node_exporter/textfile_collector/prs-health.prom\n    fi\n}\n\n# Main execution\nmain() {\n    log_message \"Starting health check (component: ${COMPONENT:-all})\"\n\n    case \"${COMPONENT:-all}\" in\n        \"infrastructure\"|\"infra\")\n            check_infrastructure\n            ;;\n        \"services\"|\"service\")\n            check_services\n            ;;\n        \"application\"|\"app\")\n            check_application\n            ;;\n        \"business\")\n            check_business_logic\n            ;;\n        \"performance\"|\"perf\")\n            check_performance\n            ;;\n        \"all\"|\"\")\n            check_infrastructure\n            echo \"\"\n            check_services\n            echo \"\"\n            check_application\n            echo \"\"\n            check_business_logic\n            echo \"\"\n            check_performance\n            ;;\n        *)\n            echo \"Usage: $0 [infrastructure|services|application|business|performance|all]\"\n            exit 1\n            ;;\n    esac\n\n    echo \"\"\n    generate_metrics\n    log_message \"Health check completed\"\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"operations/health-checks/#manual-health-check-procedures","title":"Manual Health Check Procedures","text":""},{"location":"operations/health-checks/#daily-health-check-routine","title":"Daily Health Check Routine","text":"<pre><code>#!/bin/bash\n# Daily health check routine\n\necho \"=== Daily PRS Health Check - $(date) ===\"\n\n# 1. Quick system overview\necho \"System Overview:\"\necho \"- Uptime: $(uptime -p)\"\necho \"- Load: $(uptime | awk -F'load average:' '{print $2}')\"\necho \"- Memory: $(free -h | grep Mem | awk '{print $3 \"/\" $2}')\"\necho \"- SSD: $(df -h /mnt/ssd | awk 'NR==2 {print $5}')\"\necho \"- HDD: $(df -h /mnt/hdd | awk 'NR==2 {print $5}')\"\n\n# 2. Service status\necho -e \"\\nService Status:\"\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml ps\n\n# 3. Application accessibility\necho -e \"\\nApplication Tests:\"\ncurl -s -o /dev/null -w \"API Health: %{http_code} (%{time_total}s)\\n\" https://localhost/api/health\ncurl -s -o /dev/null -w \"Frontend: %{http_code} (%{time_total}s)\\n\" https://localhost/\n\n# 4. Database health\necho -e \"\\nDatabase Health:\"\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT \n    'Connections: ' || count(*) as metric\nFROM pg_stat_activity\nUNION ALL\nSELECT \n    'Cache Hit Ratio: ' || round(100.0 * sum(blks_hit) / nullif(sum(blks_hit) + sum(blks_read), 0), 2) || '%'\nFROM pg_stat_database \nWHERE datname = 'prs_production';\n\"\n\n# 5. Recent errors\necho -e \"\\nRecent Errors:\"\nERROR_COUNT=$(docker logs prs-onprem-backend --since 24h 2&gt;&amp;1 | grep -i error | wc -l)\necho \"Backend errors (24h): $ERROR_COUNT\"\n\nif [ \"$ERROR_COUNT\" -gt 10 ]; then\n    echo \"WARNING: High error count detected\"\n    docker logs prs-onprem-backend --since 24h 2&gt;&amp;1 | grep -i error | tail -5\nfi\n</code></pre>"},{"location":"operations/health-checks/#weekly-health-assessment","title":"Weekly Health Assessment","text":"<pre><code>#!/bin/bash\n# Weekly comprehensive health assessment\n\nREPORT_FILE=\"/tmp/weekly-health-report-$(date +%Y%m%d).txt\"\n\ncat &gt; \"$REPORT_FILE\" &lt;&lt; EOF\nPRS Weekly Health Report\nGenerated: $(date)\n========================\n\nSystem Information:\n- Hostname: $(hostname)\n- OS: $(lsb_release -d | cut -f2)\n- Kernel: $(uname -r)\n- Uptime: $(uptime -p)\n\nHardware Status:\n- CPU Cores: $(nproc)\n- Total Memory: $(free -h | grep Mem | awk '{print $2}')\n- SSD Capacity: $(df -h /mnt/ssd | awk 'NR==2 {print $2}')\n- HDD Capacity: $(df -h /mnt/hdd | awk 'NR==2 {print $2}')\n\nPerformance Metrics (7-day average):\nEOF\n\n# Add performance data\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\nSELECT \n    'Database Size: ' || pg_size_pretty(pg_database_size('prs_production'))\nUNION ALL\nSELECT \n    'Active Users (7d): ' || count(DISTINCT user_id)::text\nFROM audit_logs \nWHERE created_at &gt;= NOW() - INTERVAL '7 days'\nUNION ALL\nSELECT \n    'Total Requisitions: ' || count(*)::text\nFROM requisitions\nUNION ALL\nSELECT \n    'Avg Response Time: ' || round(avg(total_time), 2)::text || 'ms'\nFROM pg_stat_statements \nWHERE calls &gt; 100;\n\" &gt;&gt; \"$REPORT_FILE\"\n\necho \"Weekly health report generated: $REPORT_FILE\"\n\n# Email report if mail is configured\nif command -v mail &gt;/dev/null 2&gt;&amp;1; then\n    mail -s \"PRS Weekly Health Report\" admin@your-domain.com &lt; \"$REPORT_FILE\"\nfi\n</code></pre>"},{"location":"operations/health-checks/#health-check-automation","title":"Health Check Automation","text":""},{"location":"operations/health-checks/#cron-schedule","title":"Cron Schedule","text":"<pre><code># Setup health check automation\n(crontab -l 2&gt;/dev/null; cat &lt;&lt; 'EOF'\n# PRS Health Check Schedule\n\n# Comprehensive health check every 5 minutes\n*/5 * * * * /opt/prs-deployment/scripts/system-health-check.sh &gt;/dev/null 2&gt;&amp;1\n\n# Daily health summary at 8:00 AM\n0 8 * * * /opt/prs-deployment/scripts/daily-health-check.sh\n\n# Weekly health report on Monday at 9:00 AM\n0 9 * * 1 /opt/prs-deployment/scripts/weekly-health-assessment.sh\n\n# Performance check every hour\n0 * * * * /opt/prs-deployment/scripts/system-health-check.sh performance &gt;/dev/null 2&gt;&amp;1\nEOF\n) | crontab -\n</code></pre>"},{"location":"operations/health-checks/#health-check-alerts","title":"Health Check Alerts","text":"<pre><code>#!/bin/bash\n# Health check alerting system\n\nALERT_THRESHOLD_FILE=\"/tmp/prs-alert-thresholds\"\nLAST_ALERT_FILE=\"/tmp/prs-last-alert\"\n\n# Alert thresholds\ncat &gt; \"$ALERT_THRESHOLD_FILE\" &lt;&lt; EOF\nCPU_THRESHOLD=80\nMEMORY_THRESHOLD=85\nSSD_THRESHOLD=90\nHDD_THRESHOLD=85\nRESPONSE_TIME_THRESHOLD=2.0\nCONNECTION_THRESHOLD=120\nEOF\n\nsource \"$ALERT_THRESHOLD_FILE\"\n\nsend_alert() {\n    local severity=\"$1\"\n    local message=\"$2\"\n    local current_time=$(date +%s)\n    local last_alert_time=0\n\n    # Rate limiting: don't send same alert within 1 hour\n    if [ -f \"$LAST_ALERT_FILE\" ]; then\n        last_alert_time=$(cat \"$LAST_ALERT_FILE\")\n    fi\n\n    if [ $((current_time - last_alert_time)) -gt 3600 ]; then\n        echo \"$current_time\" &gt; \"$LAST_ALERT_FILE\"\n\n        # Send email alert\n        echo \"$message\" | mail -s \"PRS Health Alert: $severity\" admin@your-domain.com\n\n        # Log alert\n        echo \"$(date): ALERT [$severity] $message\" &gt;&gt; /var/log/prs-health.log\n\n        # Send to monitoring system (if configured)\n        if command -v curl &gt;/dev/null 2&gt;&amp;1; then\n            curl -X POST http://monitoring-system/alerts \\\n                -H \"Content-Type: application/json\" \\\n                -d \"{\\\"severity\\\":\\\"$severity\\\",\\\"message\\\":\\\"$message\\\",\\\"timestamp\\\":\\\"$(date -Iseconds)\\\"}\"\n        fi\n    fi\n}\n\n# Check thresholds and send alerts\ncheck_and_alert() {\n    # CPU check\n    CPU_USAGE=$(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | sed 's/%us,//')\n    if (( $(echo \"$CPU_USAGE &gt; $CPU_THRESHOLD\" | bc -l) )); then\n        send_alert \"WARNING\" \"High CPU usage: ${CPU_USAGE}% (threshold: ${CPU_THRESHOLD}%)\"\n    fi\n\n    # Memory check\n    MEMORY_USAGE=$(free | grep Mem | awk '{printf \"%.1f\", $3/$2 * 100.0}')\n    if (( $(echo \"$MEMORY_USAGE &gt; $MEMORY_THRESHOLD\" | bc -l) )); then\n        send_alert \"WARNING\" \"High memory usage: ${MEMORY_USAGE}% (threshold: ${MEMORY_THRESHOLD}%)\"\n    fi\n\n    # Storage checks\n    SSD_USAGE=$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\n    if [ \"$SSD_USAGE\" -gt \"$SSD_THRESHOLD\" ]; then\n        send_alert \"CRITICAL\" \"SSD storage critical: ${SSD_USAGE}% (threshold: ${SSD_THRESHOLD}%)\"\n    fi\n\n    # Service checks\n    if ! docker ps --filter \"name=prs-onprem-backend\" --filter \"status=running\" | grep -q prs-onprem-backend; then\n        send_alert \"CRITICAL\" \"Backend service is down\"\n    fi\n\n    if ! docker exec prs-onprem-postgres-timescale pg_isready -U prs_admin &gt;/dev/null 2&gt;&amp;1; then\n        send_alert \"CRITICAL\" \"Database is not accessible\"\n    fi\n}\n\ncheck_and_alert\n</code></pre> <p>Health Monitoring Active</p> <p>Your PRS deployment now has comprehensive health monitoring with automated checks, alerting, and reporting capabilities.</p> <p>Proactive Monitoring</p> <p>Regular health checks help identify issues before they impact users. Review health reports weekly and adjust thresholds based on your environment.</p> <p>Alert Fatigue</p> <p>Tune alert thresholds to reduce false positives while maintaining effective monitoring coverage. Too many alerts can lead to important issues being ignored.</p>"},{"location":"operations/monitoring/","title":"Monitoring","text":""},{"location":"operations/monitoring/#overview","title":"Overview","text":"<p>This guide covers comprehensive monitoring setup for the PRS on-premises deployment using Prometheus, Grafana, and custom monitoring solutions.</p>"},{"location":"operations/monitoring/#monitoring-architecture","title":"Monitoring Architecture","text":"<pre><code>graph TB\n    subgraph \"Data Sources\"\n        APP[PRS Application&lt;br/&gt;Metrics Endpoint]\n        DB[PostgreSQL&lt;br/&gt;pg_stat_*]\n        REDIS[Redis&lt;br/&gt;INFO command]\n        SYSTEM[System Metrics&lt;br/&gt;node_exporter]\n        DOCKER[Docker Stats&lt;br/&gt;cAdvisor]\n    end\n\n    subgraph \"Collection Layer\"\n        PROM[Prometheus&lt;br/&gt;Metrics Collection]\n        APP --&gt; PROM\n        DB --&gt; PROM\n        REDIS --&gt; PROM\n        SYSTEM --&gt; PROM\n        DOCKER --&gt; PROM\n    end\n\n    subgraph \"Storage &amp; Processing\"\n        TSDB[Time Series Database&lt;br/&gt;Prometheus TSDB]\n        PROM --&gt; TSDB\n    end\n\n    subgraph \"Visualization &amp; Alerting\"\n        GRAFANA[Grafana Dashboards]\n        ALERTS[Alert Manager]\n        TSDB --&gt; GRAFANA\n        TSDB --&gt; ALERTS\n    end\n\n    subgraph \"Notification Channels\"\n        EMAIL[Email Notifications]\n        SLACK[Slack Alerts]\n        WEBHOOK[Webhook Endpoints]\n        ALERTS --&gt; EMAIL\n        ALERTS --&gt; SLACK\n        ALERTS --&gt; WEBHOOK\n    end\n\n    style PROM fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n    style GRAFANA fill:#e8f5e8,stroke:#4caf50,stroke-width:2px\n    style ALERTS fill:#fff3e0,stroke:#ff9800,stroke-width:2px</code></pre>"},{"location":"operations/monitoring/#prometheus-configuration","title":"Prometheus Configuration","text":""},{"location":"operations/monitoring/#main-configuration","title":"Main Configuration","text":"<pre><code># config/prometheus/prometheus.yml\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n  external_labels:\n    cluster: 'prs-onprem'\n    environment: 'production'\n\nrule_files:\n  - \"alerts.yml\"\n  - \"recording_rules.yml\"\n\nscrape_configs:\n  # Prometheus self-monitoring\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n\n  # System metrics\n  - job_name: 'node-exporter'\n    static_configs:\n      - targets: ['node-exporter:9100']\n    scrape_interval: 30s\n\n  # PostgreSQL metrics\n  - job_name: 'postgres-exporter'\n    static_configs:\n      - targets: ['postgres-exporter:9187']\n    scrape_interval: 30s\n\n  # Redis metrics\n  - job_name: 'redis-exporter'\n    static_configs:\n      - targets: ['redis-exporter:9121']\n    scrape_interval: 30s\n\n  # Nginx metrics\n  - job_name: 'nginx-exporter'\n    static_configs:\n      - targets: ['nginx-exporter:9113']\n    scrape_interval: 30s\n\n  # PRS Backend application\n  - job_name: 'prs-backend'\n    static_configs:\n      - targets: ['backend:4000']\n    metrics_path: '/metrics'\n    scrape_interval: 30s\n\n  # Docker container metrics\n  - job_name: 'cadvisor'\n    static_configs:\n      - targets: ['cadvisor:8080']\n    scrape_interval: 30s\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n          - alertmanager:9093\n</code></pre>"},{"location":"operations/monitoring/#recording-rules","title":"Recording Rules","text":"<pre><code># config/prometheus/recording_rules.yml\ngroups:\n  - name: prs_application_rules\n    interval: 30s\n    rules:\n      # Application performance metrics\n      - record: prs:request_duration_seconds:rate5m\n        expr: rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m])\n\n      - record: prs:request_rate:rate5m\n        expr: rate(http_requests_total[5m])\n\n      - record: prs:error_rate:rate5m\n        expr: rate(http_requests_total{status=~\"5..\"}[5m]) / rate(http_requests_total[5m])\n\n  - name: prs_database_rules\n    interval: 30s\n    rules:\n      # Database performance metrics\n      - record: prs:db_connections:active\n        expr: pg_stat_database_numbackends{datname=\"prs_production\"}\n\n      - record: prs:db_cache_hit_ratio\n        expr: pg_stat_database_blks_hit{datname=\"prs_production\"} / (pg_stat_database_blks_hit{datname=\"prs_production\"} + pg_stat_database_blks_read{datname=\"prs_production\"})\n\n      - record: prs:db_transaction_rate:rate5m\n        expr: rate(pg_stat_database_xact_commit{datname=\"prs_production\"}[5m])\n\n  - name: prs_system_rules\n    interval: 30s\n    rules:\n      # System resource metrics\n      - record: prs:cpu_usage:rate5m\n        expr: 1 - rate(node_cpu_seconds_total{mode=\"idle\"}[5m])\n\n      - record: prs:memory_usage:ratio\n        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes\n\n      - record: prs:disk_usage:ratio\n        expr: (node_filesystem_size_bytes - node_filesystem_avail_bytes) / node_filesystem_size_bytes\n</code></pre>"},{"location":"operations/monitoring/#alert-configuration","title":"Alert Configuration","text":""},{"location":"operations/monitoring/#alert-rules","title":"Alert Rules","text":"<pre><code># config/prometheus/alerts.yml\ngroups:\n  - name: prs_critical_alerts\n    rules:\n      # Service availability\n      - alert: ServiceDown\n        expr: up == 0\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Service {{ $labels.job }} is down\"\n          description: \"Service {{ $labels.job }} has been down for more than 1 minute\"\n\n      # High error rate\n      - alert: HighErrorRate\n        expr: prs:error_rate:rate5m &gt; 0.05\n        for: 5m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"High error rate detected\"\n          description: \"Error rate is {{ $value | humanizePercentage }} for the last 5 minutes\"\n\n      # Database connection issues\n      - alert: DatabaseConnectionHigh\n        expr: prs:db_connections:active &gt; 120\n        for: 2m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"High database connections\"\n          description: \"Database has {{ $value }} active connections\"\n\n      # Storage space alerts\n      - alert: DiskSpaceHigh\n        expr: prs:disk_usage:ratio{mountpoint=\"/mnt/ssd\"} &gt; 0.85\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"SSD disk space high\"\n          description: \"SSD usage is {{ $value | humanizePercentage }}\"\n\n      - alert: DiskSpaceCritical\n        expr: prs:disk_usage:ratio{mountpoint=\"/mnt/ssd\"} &gt; 0.90\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"SSD disk space critical\"\n          description: \"SSD usage is {{ $value | humanizePercentage }}\"\n\n  - name: prs_performance_alerts\n    rules:\n      # High CPU usage\n      - alert: HighCPUUsage\n        expr: prs:cpu_usage:rate5m &gt; 0.80\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"High CPU usage\"\n          description: \"CPU usage is {{ $value | humanizePercentage }} for the last 5 minutes\"\n\n      # High memory usage\n      - alert: HighMemoryUsage\n        expr: prs:memory_usage:ratio &gt; 0.85\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"High memory usage\"\n          description: \"Memory usage is {{ $value | humanizePercentage }}\"\n\n      # Slow response times\n      - alert: SlowResponseTime\n        expr: prs:request_duration_seconds:rate5m &gt; 1.0\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Slow response times\"\n          description: \"Average response time is {{ $value }}s for the last 5 minutes\"\n\n  - name: prs_database_alerts\n    rules:\n      # Low cache hit ratio\n      - alert: LowDatabaseCacheHitRatio\n        expr: prs:db_cache_hit_ratio &lt; 0.95\n        for: 5m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"Low database cache hit ratio\"\n          description: \"Cache hit ratio is {{ $value | humanizePercentage }}\"\n\n      # TimescaleDB chunk issues\n      - alert: TimescaleDBCompressionFailed\n        expr: increase(timescaledb_background_job_failures_total[1h]) &gt; 0\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"TimescaleDB background job failed\"\n          description: \"TimescaleDB background job has failed {{ $value }} times in the last hour\"\n</code></pre>"},{"location":"operations/monitoring/#grafana-dashboards","title":"Grafana Dashboards","text":""},{"location":"operations/monitoring/#system-overview-dashboard","title":"System Overview Dashboard","text":"<pre><code>{\n  \"dashboard\": {\n    \"title\": \"PRS System Overview\",\n    \"panels\": [\n      {\n        \"title\": \"System Health\",\n        \"type\": \"stat\",\n        \"targets\": [\n          {\n            \"expr\": \"up\",\n            \"legendFormat\": \"{{ job }}\"\n          }\n        ]\n      },\n      {\n        \"title\": \"CPU Usage\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"prs:cpu_usage:rate5m * 100\",\n            \"legendFormat\": \"CPU Usage %\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Memory Usage\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"prs:memory_usage:ratio * 100\",\n            \"legendFormat\": \"Memory Usage %\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Disk Usage\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"prs:disk_usage:ratio{mountpoint=\\\"/mnt/ssd\\\"} * 100\",\n            \"legendFormat\": \"SSD Usage %\"\n          },\n          {\n            \"expr\": \"prs:disk_usage:ratio{mountpoint=\\\"/mnt/hdd\\\"} * 100\",\n            \"legendFormat\": \"HDD Usage %\"\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"operations/monitoring/#application-performance-dashboard","title":"Application Performance Dashboard","text":"<pre><code>{\n  \"dashboard\": {\n    \"title\": \"PRS Application Performance\",\n    \"panels\": [\n      {\n        \"title\": \"Request Rate\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"prs:request_rate:rate5m\",\n            \"legendFormat\": \"Requests/sec\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Response Time\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"prs:request_duration_seconds:rate5m\",\n            \"legendFormat\": \"Avg Response Time\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Error Rate\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"prs:error_rate:rate5m * 100\",\n            \"legendFormat\": \"Error Rate %\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Active Users\",\n        \"type\": \"stat\",\n        \"targets\": [\n          {\n            \"expr\": \"prs_active_sessions_total\",\n            \"legendFormat\": \"Active Sessions\"\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"operations/monitoring/#database-performance-dashboard","title":"Database Performance Dashboard","text":"<pre><code>{\n  \"dashboard\": {\n    \"title\": \"PRS Database Performance\",\n    \"panels\": [\n      {\n        \"title\": \"Database Connections\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"prs:db_connections:active\",\n            \"legendFormat\": \"Active Connections\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Cache Hit Ratio\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"prs:db_cache_hit_ratio * 100\",\n            \"legendFormat\": \"Cache Hit Ratio %\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Transaction Rate\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"prs:db_transaction_rate:rate5m\",\n            \"legendFormat\": \"Transactions/sec\"\n          }\n        ]\n      },\n      {\n        \"title\": \"TimescaleDB Chunks\",\n        \"type\": \"table\",\n        \"targets\": [\n          {\n            \"expr\": \"timescaledb_hypertable_chunks_total\",\n            \"legendFormat\": \"{{ hypertable_name }}\"\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"operations/monitoring/#custom-monitoring-scripts","title":"Custom Monitoring Scripts","text":""},{"location":"operations/monitoring/#health-check-script","title":"Health Check Script","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/health-monitor.sh\n\nMETRICS_FILE=\"/tmp/prs-health-metrics.prom\"\nLOG_FILE=\"/var/log/prs-health.log\"\n\n# Function to log with timestamp\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" &gt;&gt; \"$LOG_FILE\"\n}\n\n# Check service health\ncheck_services() {\n    local healthy=0\n    local total=0\n\n    services=(\n        \"prs-onprem-nginx\"\n        \"prs-onprem-frontend\"\n        \"prs-onprem-backend\"\n        \"prs-onprem-postgres-timescale\"\n        \"prs-onprem-redis\"\n    )\n\n    for service in \"${services[@]}\"; do\n        total=$((total + 1))\n        if docker ps --filter \"name=$service\" --filter \"status=running\" | grep -q \"$service\"; then\n            healthy=$((healthy + 1))\n            echo \"prs_service_up{service=\\\"$service\\\"} 1\" &gt;&gt; \"$METRICS_FILE\"\n        else\n            echo \"prs_service_up{service=\\\"$service\\\"} 0\" &gt;&gt; \"$METRICS_FILE\"\n            log_message \"WARNING: Service $service is not running\"\n        fi\n    done\n\n    echo \"prs_services_healthy_ratio $(echo \"scale=2; $healthy / $total\" | bc)\" &gt;&gt; \"$METRICS_FILE\"\n}\n\n# Check API endpoints\ncheck_api_health() {\n    local response_time\n    local status_code\n\n    # Test main API health endpoint\n    response_time=$(curl -w \"%{time_total}\" -s -o /dev/null https://localhost/api/health)\n    status_code=$(curl -w \"%{http_code}\" -s -o /dev/null https://localhost/api/health)\n\n    echo \"prs_api_response_time_seconds $response_time\" &gt;&gt; \"$METRICS_FILE\"\n    echo \"prs_api_status_code $status_code\" &gt;&gt; \"$METRICS_FILE\"\n\n    if [ \"$status_code\" = \"200\" ]; then\n        echo \"prs_api_healthy 1\" &gt;&gt; \"$METRICS_FILE\"\n    else\n        echo \"prs_api_healthy 0\" &gt;&gt; \"$METRICS_FILE\"\n        log_message \"ERROR: API health check failed with status $status_code\"\n    fi\n}\n\n# Check database connectivity\ncheck_database() {\n    if docker exec prs-onprem-postgres-timescale pg_isready -U prs_admin &gt;/dev/null 2&gt;&amp;1; then\n        echo \"prs_database_healthy 1\" &gt;&gt; \"$METRICS_FILE\"\n\n        # Get connection count\n        local connections\n        connections=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT count(*) FROM pg_stat_activity;\" 2&gt;/dev/null | xargs)\n        echo \"prs_database_connections $connections\" &gt;&gt; \"$METRICS_FILE\"\n    else\n        echo \"prs_database_healthy 0\" &gt;&gt; \"$METRICS_FILE\"\n        log_message \"ERROR: Database health check failed\"\n    fi\n}\n\n# Check Redis connectivity\ncheck_redis() {\n    if docker exec prs-onprem-redis redis-cli -a \"$REDIS_PASSWORD\" ping &gt;/dev/null 2&gt;&amp;1; then\n        echo \"prs_redis_healthy 1\" &gt;&gt; \"$METRICS_FILE\"\n\n        # Get memory usage\n        local memory_used\n        memory_used=$(docker exec prs-onprem-redis redis-cli -a \"$REDIS_PASSWORD\" info memory | grep used_memory: | cut -d: -f2 | tr -d '\\r')\n        echo \"prs_redis_memory_used_bytes $memory_used\" &gt;&gt; \"$METRICS_FILE\"\n    else\n        echo \"prs_redis_healthy 0\" &gt;&gt; \"$METRICS_FILE\"\n        log_message \"ERROR: Redis health check failed\"\n    fi\n}\n\n# Main execution\nmain() {\n    # Clear previous metrics\n    &gt; \"$METRICS_FILE\"\n\n    # Run health checks\n    check_services\n    check_api_health\n    check_database\n    check_redis\n\n    # Expose metrics for Prometheus\n    cp \"$METRICS_FILE\" /var/lib/node_exporter/textfile_collector/prs-health.prom\n\n    log_message \"Health check completed\"\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"operations/monitoring/#performance-monitor","title":"Performance Monitor","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/performance-monitor.sh\n\nMETRICS_FILE=\"/tmp/prs-performance-metrics.prom\"\n\n# Monitor application performance\nmonitor_application() {\n    # Get container stats\n    local cpu_usage\n    local memory_usage\n\n    cpu_usage=$(docker stats prs-onprem-backend --no-stream --format \"{{.CPUPerc}}\" | sed 's/%//')\n    memory_usage=$(docker stats prs-onprem-backend --no-stream --format \"{{.MemUsage}}\" | cut -d'/' -f1 | sed 's/[^0-9.]//g')\n\n    echo \"prs_backend_cpu_usage_percent $cpu_usage\" &gt;&gt; \"$METRICS_FILE\"\n    echo \"prs_backend_memory_usage_bytes ${memory_usage}000000\" &gt;&gt; \"$METRICS_FILE\"\n}\n\n# Monitor storage performance\nmonitor_storage() {\n    # SSD usage\n    local ssd_usage\n    ssd_usage=$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\n    echo \"prs_ssd_usage_percent $ssd_usage\" &gt;&gt; \"$METRICS_FILE\"\n\n    # HDD usage\n    local hdd_usage\n    hdd_usage=$(df /mnt/hdd | awk 'NR==2 {print $5}' | sed 's/%//')\n    echo \"prs_hdd_usage_percent $hdd_usage\" &gt;&gt; \"$METRICS_FILE\"\n\n    # I/O statistics\n    local ssd_reads\n    local ssd_writes\n    ssd_reads=$(iostat -d 1 2 | grep sda | tail -1 | awk '{print $4}')\n    ssd_writes=$(iostat -d 1 2 | grep sda | tail -1 | awk '{print $5}')\n\n    echo \"prs_ssd_reads_per_sec $ssd_reads\" &gt;&gt; \"$METRICS_FILE\"\n    echo \"prs_ssd_writes_per_sec $ssd_writes\" &gt;&gt; \"$METRICS_FILE\"\n}\n\n# Main execution\nmain() {\n    &gt; \"$METRICS_FILE\"\n\n    monitor_application\n    monitor_storage\n\n    # Expose metrics\n    cp \"$METRICS_FILE\" /var/lib/node_exporter/textfile_collector/prs-performance.prom\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"operations/monitoring/#alert-manager-configuration","title":"Alert Manager Configuration","text":""},{"location":"operations/monitoring/#alert-manager-setup","title":"Alert Manager Setup","text":"<pre><code># config/alertmanager/alertmanager.yml\nglobal:\n  smtp_smarthost: 'smtp.your-domain.com:587'\n  smtp_from: 'alerts@your-domain.com'\n  smtp_auth_username: 'alerts@your-domain.com'\n  smtp_auth_password: 'smtp_password'\n\nroute:\n  group_by: ['alertname']\n  group_wait: 10s\n  group_interval: 10s\n  repeat_interval: 1h\n  receiver: 'web.hook'\n  routes:\n    - match:\n        severity: critical\n      receiver: 'critical-alerts'\n    - match:\n        severity: warning\n      receiver: 'warning-alerts'\n\nreceivers:\n  - name: 'web.hook'\n    webhook_configs:\n      - url: 'http://localhost:5001/'\n\n  - name: 'critical-alerts'\n    email_configs:\n      - to: 'admin@your-domain.com'\n        subject: 'CRITICAL: PRS Alert - {{ .GroupLabels.alertname }}'\n        body: |\n          {{ range .Alerts }}\n          Alert: {{ .Annotations.summary }}\n          Description: {{ .Annotations.description }}\n          {{ end }}\n    slack_configs:\n      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'\n        channel: '#prs-alerts'\n        title: 'CRITICAL: PRS Alert'\n        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'\n\n  - name: 'warning-alerts'\n    email_configs:\n      - to: 'team@your-domain.com'\n        subject: 'WARNING: PRS Alert - {{ .GroupLabels.alertname }}'\n        body: |\n          {{ range .Alerts }}\n          Alert: {{ .Annotations.summary }}\n          Description: {{ .Annotations.description }}\n          {{ end }}\n\ninhibit_rules:\n  - source_match:\n      severity: 'critical'\n    target_match:\n      severity: 'warning'\n    equal: ['alertname', 'dev', 'instance']\n</code></pre>"},{"location":"operations/monitoring/#monitoring-automation","title":"Monitoring Automation","text":""},{"location":"operations/monitoring/#automated-monitoring-setup","title":"Automated Monitoring Setup","text":"<pre><code>#!/bin/bash\n# Setup monitoring automation\n\n# Create monitoring cron jobs\n(crontab -l 2&gt;/dev/null; echo \"*/5 * * * * /opt/prs-deployment/scripts/health-monitor.sh\") | crontab -\n(crontab -l 2&gt;/dev/null; echo \"*/1 * * * * /opt/prs-deployment/scripts/performance-monitor.sh\") | crontab -\n\n# Create log rotation for monitoring logs\ncat &gt; /etc/logrotate.d/prs-monitoring &lt;&lt; 'EOF'\n/var/log/prs-health.log {\n    daily\n    rotate 30\n    compress\n    delaycompress\n    missingok\n    notifempty\n    create 644 root root\n}\nEOF\n\necho \"Monitoring automation setup completed\"\n</code></pre> <p>Monitoring Active</p> <p>Your PRS deployment now has comprehensive monitoring with real-time dashboards, automated alerts, and performance tracking.</p> <p>Dashboard Customization</p> <p>Customize Grafana dashboards based on your specific monitoring needs and add additional metrics as required.</p> <p>Alert Tuning</p> <p>Monitor alert frequency and adjust thresholds to reduce false positives while maintaining effective monitoring coverage.</p>"},{"location":"reference/api/","title":"API Reference","text":""},{"location":"reference/api/#overview","title":"Overview","text":"<p>This reference guide provides comprehensive documentation for the PRS REST API, including authentication, endpoints, request/response formats, and integration examples.</p>"},{"location":"reference/api/#api-base-information","title":"API Base Information","text":""},{"location":"reference/api/#base-url","title":"Base URL","text":"<pre><code>https://your-domain.com/api\n</code></pre>"},{"location":"reference/api/#api-version","title":"API Version","text":"<pre><code>Current Version: v1\nVersioning: URL path (/api/v1/)\n</code></pre>"},{"location":"reference/api/#content-type","title":"Content Type","text":"<pre><code>Content-Type: application/json\nAccept: application/json\n</code></pre>"},{"location":"reference/api/#authentication","title":"Authentication","text":""},{"location":"reference/api/#jwt-authentication","title":"JWT Authentication","text":"<p>The PRS API uses JWT (JSON Web Token) for authentication. Include the token in the Authorization header:</p> <pre><code>Authorization: Bearer &lt;jwt_token&gt;\n</code></pre>"},{"location":"reference/api/#login-endpoint","title":"Login Endpoint","text":"<pre><code>POST /api/auth/login\nContent-Type: application/json\n\n{\n  \"username\": \"user@example.com\",\n  \"password\": \"password123\"\n}\n</code></pre> <p>Response: <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\",\n    \"user\": {\n      \"id\": 1,\n      \"username\": \"user@example.com\",\n      \"name\": \"John Doe\",\n      \"role\": \"user\",\n      \"department_id\": 5\n    },\n    \"expires_in\": 86400\n  }\n}\n</code></pre></p>"},{"location":"reference/api/#token-refresh","title":"Token Refresh","text":"<pre><code>POST /api/auth/refresh\nAuthorization: Bearer &lt;current_token&gt;\n</code></pre>"},{"location":"reference/api/#logout","title":"Logout","text":"<pre><code>POST /api/auth/logout\nAuthorization: Bearer &lt;jwt_token&gt;\n</code></pre>"},{"location":"reference/api/#core-endpoints","title":"Core Endpoints","text":""},{"location":"reference/api/#health-check","title":"Health Check","text":""},{"location":"reference/api/#system-health","title":"System Health","text":"<pre><code>GET /api/health\n</code></pre> <p>Response: <pre><code>{\n  \"status\": \"healthy\",\n  \"timestamp\": \"2024-08-22T10:30:00Z\",\n  \"version\": \"2.1.0\",\n  \"database\": \"connected\",\n  \"redis\": \"connected\",\n  \"uptime\": 86400\n}\n</code></pre></p>"},{"location":"reference/api/#detailed-health","title":"Detailed Health","text":"<pre><code>GET /api/health/detailed\nAuthorization: Bearer &lt;jwt_token&gt;\n</code></pre> <p>Response: <pre><code>{\n  \"status\": \"healthy\",\n  \"components\": {\n    \"database\": {\n      \"status\": \"healthy\",\n      \"connections\": 15,\n      \"response_time_ms\": 2.5\n    },\n    \"redis\": {\n      \"status\": \"healthy\",\n      \"memory_usage\": \"45MB\",\n      \"connected_clients\": 8\n    },\n    \"storage\": {\n      \"ssd_usage\": \"65%\",\n      \"hdd_usage\": \"40%\"\n    }\n  }\n}\n</code></pre></p>"},{"location":"reference/api/#user-management","title":"User Management","text":""},{"location":"reference/api/#get-current-user","title":"Get Current User","text":"<pre><code>GET /api/users/me\nAuthorization: Bearer &lt;jwt_token&gt;\n</code></pre>"},{"location":"reference/api/#list-users","title":"List Users","text":"<pre><code>GET /api/users\nAuthorization: Bearer &lt;jwt_token&gt;\n</code></pre> <p>Query Parameters: - <code>page</code> (integer): Page number (default: 1) - <code>limit</code> (integer): Items per page (default: 20, max: 100) - <code>search</code> (string): Search term for name/email - <code>role</code> (string): Filter by role - <code>department_id</code> (integer): Filter by department</p> <p>Response: <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"users\": [\n      {\n        \"id\": 1,\n        \"username\": \"john.doe@company.com\",\n        \"name\": \"John Doe\",\n        \"email\": \"john.doe@company.com\",\n        \"role\": \"user\",\n        \"department_id\": 5,\n        \"department_name\": \"IT Department\",\n        \"active\": true,\n        \"last_login_at\": \"2024-08-22T09:15:00Z\",\n        \"created_at\": \"2024-01-15T10:00:00Z\"\n      }\n    ],\n    \"pagination\": {\n      \"current_page\": 1,\n      \"total_pages\": 5,\n      \"total_items\": 95,\n      \"items_per_page\": 20\n    }\n  }\n}\n</code></pre></p>"},{"location":"reference/api/#create-user","title":"Create User","text":"<pre><code>POST /api/users\nAuthorization: Bearer &lt;jwt_token&gt;\nContent-Type: application/json\n\n{\n  \"username\": \"new.user@company.com\",\n  \"name\": \"New User\",\n  \"email\": \"new.user@company.com\",\n  \"password\": \"SecurePassword123!\",\n  \"role\": \"user\",\n  \"department_id\": 3\n}\n</code></pre>"},{"location":"reference/api/#update-user","title":"Update User","text":"<pre><code>PUT /api/users/{id}\nAuthorization: Bearer &lt;jwt_token&gt;\nContent-Type: application/json\n\n{\n  \"name\": \"Updated Name\",\n  \"role\": \"admin\",\n  \"active\": true\n}\n</code></pre>"},{"location":"reference/api/#delete-user","title":"Delete User","text":"<pre><code>DELETE /api/users/{id}\nAuthorization: Bearer &lt;jwt_token&gt;\n</code></pre>"},{"location":"reference/api/#requisitions","title":"Requisitions","text":""},{"location":"reference/api/#list-requisitions","title":"List Requisitions","text":"<pre><code>GET /api/requisitions\nAuthorization: Bearer &lt;jwt_token&gt;\n</code></pre> <p>Query Parameters: - <code>page</code> (integer): Page number - <code>limit</code> (integer): Items per page - <code>status</code> (string): Filter by status (pending, approved, rejected, processing, completed) - <code>department_id</code> (integer): Filter by department - <code>user_id</code> (integer): Filter by user - <code>date_from</code> (string): Start date (YYYY-MM-DD) - <code>date_to</code> (string): End date (YYYY-MM-DD) - <code>search</code> (string): Search in description/notes</p> <p>Response: <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"requisitions\": [\n      {\n        \"id\": 123,\n        \"requisition_number\": \"REQ-2024-000123\",\n        \"description\": \"Office supplies for Q3\",\n        \"status\": \"pending\",\n        \"total_amount\": 1250.00,\n        \"currency\": \"USD\",\n        \"user_id\": 15,\n        \"user_name\": \"Jane Smith\",\n        \"department_id\": 3,\n        \"department_name\": \"Marketing\",\n        \"created_at\": \"2024-08-22T08:30:00Z\",\n        \"updated_at\": \"2024-08-22T08:30:00Z\",\n        \"items_count\": 5\n      }\n    ],\n    \"pagination\": {\n      \"current_page\": 1,\n      \"total_pages\": 12,\n      \"total_items\": 234,\n      \"items_per_page\": 20\n    }\n  }\n}\n</code></pre></p>"},{"location":"reference/api/#get-requisition-details","title":"Get Requisition Details","text":"<pre><code>GET /api/requisitions/{id}\nAuthorization: Bearer &lt;jwt_token&gt;\n</code></pre> <p>Response: <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"id\": 123,\n    \"requisition_number\": \"REQ-2024-000123\",\n    \"description\": \"Office supplies for Q3\",\n    \"notes\": \"Urgent requirement for new office setup\",\n    \"status\": \"pending\",\n    \"total_amount\": 1250.00,\n    \"currency\": \"USD\",\n    \"user_id\": 15,\n    \"user_name\": \"Jane Smith\",\n    \"department_id\": 3,\n    \"department_name\": \"Marketing\",\n    \"created_at\": \"2024-08-22T08:30:00Z\",\n    \"updated_at\": \"2024-08-22T08:30:00Z\",\n    \"items\": [\n      {\n        \"id\": 456,\n        \"description\": \"Laptop computers\",\n        \"quantity\": 5,\n        \"unit_price\": 1200.00,\n        \"total_price\": 6000.00,\n        \"specifications\": \"Dell Latitude 5520, 16GB RAM, 512GB SSD\"\n      }\n    ],\n    \"approvals\": [\n      {\n        \"id\": 789,\n        \"approver_id\": 8,\n        \"approver_name\": \"Mike Johnson\",\n        \"status\": \"pending\",\n        \"level\": 1,\n        \"created_at\": \"2024-08-22T08:35:00Z\"\n      }\n    ],\n    \"attachments\": [\n      {\n        \"id\": 101,\n        \"filename\": \"quote.pdf\",\n        \"size\": 245760,\n        \"uploaded_at\": \"2024-08-22T08:32:00Z\"\n      }\n    ]\n  }\n}\n</code></pre></p>"},{"location":"reference/api/#create-requisition","title":"Create Requisition","text":"<pre><code>POST /api/requisitions\nAuthorization: Bearer &lt;jwt_token&gt;\nContent-Type: application/json\n\n{\n  \"description\": \"New office equipment\",\n  \"notes\": \"Required for new team members\",\n  \"department_id\": 3,\n  \"items\": [\n    {\n      \"description\": \"Laptop computers\",\n      \"quantity\": 3,\n      \"unit_price\": 1200.00,\n      \"specifications\": \"Dell Latitude 5520\"\n    },\n    {\n      \"description\": \"Office chairs\",\n      \"quantity\": 3,\n      \"unit_price\": 250.00,\n      \"specifications\": \"Ergonomic with lumbar support\"\n    }\n  ]\n}\n</code></pre>"},{"location":"reference/api/#update-requisition","title":"Update Requisition","text":"<pre><code>PUT /api/requisitions/{id}\nAuthorization: Bearer &lt;jwt_token&gt;\nContent-Type: application/json\n\n{\n  \"description\": \"Updated description\",\n  \"notes\": \"Additional notes\",\n  \"items\": [\n    {\n      \"id\": 456,\n      \"quantity\": 4,\n      \"unit_price\": 1150.00\n    }\n  ]\n}\n</code></pre>"},{"location":"reference/api/#approvereject-requisition","title":"Approve/Reject Requisition","text":"<pre><code>POST /api/requisitions/{id}/approve\nAuthorization: Bearer &lt;jwt_token&gt;\nContent-Type: application/json\n\n{\n  \"action\": \"approve\",\n  \"comments\": \"Approved with budget allocation\"\n}\n</code></pre> <pre><code>POST /api/requisitions/{id}/reject\nAuthorization: Bearer &lt;jwt_token&gt;\nContent-Type: application/json\n\n{\n  \"action\": \"reject\",\n  \"comments\": \"Insufficient budget for this quarter\"\n}\n</code></pre>"},{"location":"reference/api/#departments","title":"Departments","text":""},{"location":"reference/api/#list-departments","title":"List Departments","text":"<pre><code>GET /api/departments\nAuthorization: Bearer &lt;jwt_token&gt;\n</code></pre>"},{"location":"reference/api/#get-department-details","title":"Get Department Details","text":"<pre><code>GET /api/departments/{id}\nAuthorization: Bearer &lt;jwt_token&gt;\n</code></pre>"},{"location":"reference/api/#create-department","title":"Create Department","text":"<pre><code>POST /api/departments\nAuthorization: Bearer &lt;jwt_token&gt;\nContent-Type: application/json\n\n{\n  \"name\": \"New Department\",\n  \"description\": \"Department description\",\n  \"budget\": 50000.00,\n  \"manager_id\": 15\n}\n</code></pre>"},{"location":"reference/api/#reports","title":"Reports","text":""},{"location":"reference/api/#requisition-summary-report","title":"Requisition Summary Report","text":"<pre><code>GET /api/reports/requisitions/summary\nAuthorization: Bearer &lt;jwt_token&gt;\n</code></pre> <p>Query Parameters: - <code>date_from</code> (string): Start date (YYYY-MM-DD) - <code>date_to</code> (string): End date (YYYY-MM-DD) - <code>department_id</code> (integer): Filter by department - <code>status</code> (string): Filter by status</p>"},{"location":"reference/api/#export-report","title":"Export Report","text":"<pre><code>GET /api/reports/requisitions/export\nAuthorization: Bearer &lt;jwt_token&gt;\n</code></pre> <p>Query Parameters: - <code>format</code> (string): Export format (csv, xlsx, pdf) - <code>date_from</code> (string): Start date - <code>date_to</code> (string): End date</p>"},{"location":"reference/api/#file-management","title":"File Management","text":""},{"location":"reference/api/#upload-file","title":"Upload File","text":"<pre><code>POST /api/files/upload\nAuthorization: Bearer &lt;jwt_token&gt;\nContent-Type: multipart/form-data\n\nfile: &lt;binary_data&gt;\nentity_type: requisition\nentity_id: 123\n</code></pre> <p>Response: <pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"id\": 456,\n    \"filename\": \"document.pdf\",\n    \"original_name\": \"Purchase Quote.pdf\",\n    \"size\": 245760,\n    \"mime_type\": \"application/pdf\",\n    \"url\": \"/api/files/456/download\"\n  }\n}\n</code></pre></p>"},{"location":"reference/api/#download-file","title":"Download File","text":"<pre><code>GET /api/files/{id}/download\nAuthorization: Bearer &lt;jwt_token&gt;\n</code></pre>"},{"location":"reference/api/#error-handling","title":"Error Handling","text":""},{"location":"reference/api/#error-response-format","title":"Error Response Format","text":"<pre><code>{\n  \"success\": false,\n  \"error\": {\n    \"code\": \"VALIDATION_ERROR\",\n    \"message\": \"Validation failed\",\n    \"details\": [\n      {\n        \"field\": \"email\",\n        \"message\": \"Email is required\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"reference/api/#http-status-codes","title":"HTTP Status Codes","text":"Code Description 200 Success 201 Created 400 Bad Request 401 Unauthorized 403 Forbidden 404 Not Found 422 Validation Error 429 Rate Limit Exceeded 500 Internal Server Error"},{"location":"reference/api/#common-error-codes","title":"Common Error Codes","text":"Code Description <code>AUTHENTICATION_REQUIRED</code> Valid authentication token required <code>INVALID_CREDENTIALS</code> Username or password incorrect <code>TOKEN_EXPIRED</code> JWT token has expired <code>INSUFFICIENT_PERMISSIONS</code> User lacks required permissions <code>VALIDATION_ERROR</code> Request validation failed <code>RESOURCE_NOT_FOUND</code> Requested resource not found <code>DUPLICATE_ENTRY</code> Resource already exists <code>RATE_LIMIT_EXCEEDED</code> Too many requests"},{"location":"reference/api/#rate-limiting","title":"Rate Limiting","text":""},{"location":"reference/api/#limits","title":"Limits","text":"<ul> <li>Authenticated requests: 1000 requests per hour</li> <li>Authentication endpoints: 10 requests per minute</li> <li>File uploads: 50 requests per hour</li> </ul>"},{"location":"reference/api/#headers","title":"Headers","text":"<pre><code>X-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 999\nX-RateLimit-Reset: 1692705600\n</code></pre>"},{"location":"reference/api/#webhooks","title":"Webhooks","text":""},{"location":"reference/api/#webhook-events","title":"Webhook Events","text":"Event Description <code>requisition.created</code> New requisition created <code>requisition.approved</code> Requisition approved <code>requisition.rejected</code> Requisition rejected <code>user.created</code> New user created <code>user.updated</code> User information updated"},{"location":"reference/api/#webhook-payload","title":"Webhook Payload","text":"<pre><code>{\n  \"event\": \"requisition.approved\",\n  \"timestamp\": \"2024-08-22T10:30:00Z\",\n  \"data\": {\n    \"id\": 123,\n    \"requisition_number\": \"REQ-2024-000123\",\n    \"status\": \"approved\",\n    \"approver\": {\n      \"id\": 8,\n      \"name\": \"Mike Johnson\"\n    }\n  }\n}\n</code></pre>"},{"location":"reference/api/#sdk-examples","title":"SDK Examples","text":""},{"location":"reference/api/#javascriptnodejs","title":"JavaScript/Node.js","text":"<pre><code>const axios = require('axios');\n\nclass PRSClient {\n  constructor(baseURL, token) {\n    this.client = axios.create({\n      baseURL,\n      headers: {\n        'Authorization': `Bearer ${token}`,\n        'Content-Type': 'application/json'\n      }\n    });\n  }\n\n  async getRequisitions(params = {}) {\n    const response = await this.client.get('/requisitions', { params });\n    return response.data;\n  }\n\n  async createRequisition(data) {\n    const response = await this.client.post('/requisitions', data);\n    return response.data;\n  }\n\n  async approveRequisition(id, comments) {\n    const response = await this.client.post(`/requisitions/${id}/approve`, {\n      action: 'approve',\n      comments\n    });\n    return response.data;\n  }\n}\n\n// Usage\nconst prs = new PRSClient('https://your-domain.com/api', 'your-jwt-token');\n\n// Get requisitions\nconst requisitions = await prs.getRequisitions({\n  status: 'pending',\n  page: 1,\n  limit: 20\n});\n\n// Create requisition\nconst newRequisition = await prs.createRequisition({\n  description: 'Office supplies',\n  department_id: 3,\n  items: [\n    {\n      description: 'Laptops',\n      quantity: 2,\n      unit_price: 1200.00\n    }\n  ]\n});\n</code></pre>"},{"location":"reference/api/#python","title":"Python","text":"<pre><code>import requests\nfrom typing import Dict, List, Optional\n\nclass PRSClient:\n    def __init__(self, base_url: str, token: str):\n        self.base_url = base_url\n        self.headers = {\n            'Authorization': f'Bearer {token}',\n            'Content-Type': 'application/json'\n        }\n\n    def get_requisitions(self, **params) -&gt; Dict:\n        response = requests.get(\n            f'{self.base_url}/requisitions',\n            headers=self.headers,\n            params=params\n        )\n        response.raise_for_status()\n        return response.json()\n\n    def create_requisition(self, data: Dict) -&gt; Dict:\n        response = requests.post(\n            f'{self.base_url}/requisitions',\n            headers=self.headers,\n            json=data\n        )\n        response.raise_for_status()\n        return response.json()\n\n    def approve_requisition(self, req_id: int, comments: str) -&gt; Dict:\n        response = requests.post(\n            f'{self.base_url}/requisitions/{req_id}/approve',\n            headers=self.headers,\n            json={'action': 'approve', 'comments': comments}\n        )\n        response.raise_for_status()\n        return response.json()\n\n# Usage\nprs = PRSClient('https://your-domain.com/api', 'your-jwt-token')\n\n# Get requisitions\nrequisitions = prs.get_requisitions(status='pending', page=1, limit=20)\n\n# Create requisition\nnew_req = prs.create_requisition({\n    'description': 'Office supplies',\n    'department_id': 3,\n    'items': [\n        {\n            'description': 'Laptops',\n            'quantity': 2,\n            'unit_price': 1200.00\n        }\n    ]\n})\n</code></pre> <p>API Reference Complete</p> <p>This comprehensive API reference covers all major endpoints, authentication, error handling, and integration examples for the PRS system.</p> <p>API Testing</p> <p>Use tools like Postman or curl to test API endpoints during development and integration.</p> <p>Security</p> <p>Always use HTTPS in production and keep JWT tokens secure. Never expose tokens in client-side code or logs.</p>"},{"location":"reference/commands/","title":"Command Reference","text":""},{"location":"reference/commands/#overview","title":"Overview","text":"<p>This reference guide provides quick access to commonly used commands for managing the PRS on-premises deployment.</p>"},{"location":"reference/commands/#docker-commands","title":"Docker Commands","text":""},{"location":"reference/commands/#management","title":"Management","text":"<pre><code># Start all services\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml up -d\n\n# Stop all services\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml down\n\n# Restart all services\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml restart\n\n# Restart specific service\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml restart backend\n\n# View service status\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml ps\n\n# View service logs\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml logs -f backend\n\n# Scale service (if supported)\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml up -d --scale backend=2\n</code></pre>"},{"location":"reference/commands/#operations","title":"Operations","text":"<pre><code># Execute command in container\ndocker exec -it prs-onprem-backend bash\ndocker exec -it prs-onprem-postgres-timescale psql -U prs_admin -d prs_production\n\n# Copy files to/from container\ndocker cp file.txt prs-onprem-backend:/tmp/\ndocker cp prs-onprem-backend:/tmp/file.txt ./\n\n# View container resource usage\ndocker stats\ndocker stats prs-onprem-backend\n\n# Inspect container configuration\ndocker inspect prs-onprem-backend\n\n# View container logs\ndocker logs prs-onprem-backend --tail 100 -f\n</code></pre>"},{"location":"reference/commands/#management_1","title":"Management","text":"<pre><code># Build images\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml build\n\n# Pull latest images\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml pull\n\n# List images\ndocker images\n\n# Remove unused images\ndocker image prune -f\n\n# Remove specific image\ndocker rmi prs-backend:latest\n</code></pre>"},{"location":"reference/commands/#and-network-management","title":"and Network Management","text":"<pre><code># List volumes\ndocker volume ls\n\n# Inspect volume\ndocker volume inspect prs_onprem_database_data\n\n# List networks\ndocker network ls\n\n# Inspect network\ndocker network inspect prs_onprem_network\n\n# Clean up unused resources\ndocker system prune -f\n</code></pre>"},{"location":"reference/commands/#database-commands","title":"Database Commands","text":""},{"location":"reference/commands/#administration","title":"Administration","text":"<pre><code># Connect to database\ndocker exec -it prs-onprem-postgres-timescale psql -U prs_admin -d prs_production\n\n# Check database status\ndocker exec prs-onprem-postgres-timescale pg_isready -U prs_admin\n\n# Create database backup\ndocker exec prs-onprem-postgres-timescale pg_dump -U prs_admin -d prs_production &gt; backup.sql\n\n# Restore database backup\ndocker exec -i prs-onprem-postgres-timescale psql -U prs_admin -d prs_production &lt; backup.sql\n\n# View database size\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"SELECT pg_size_pretty(pg_database_size('prs_production'));\"\n\n# View active connections\ndocker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"SELECT count(*) FROM pg_stat_activity WHERE state = 'active';\"\n</code></pre>"},{"location":"reference/commands/#commands","title":"Commands","text":"<pre><code>-- View hypertables\nSELECT * FROM timescaledb_information.hypertables;\n\n-- View chunks\nSELECT * FROM timescaledb_information.chunks WHERE hypertable_name = 'notifications';\n\n-- View compression stats\nSELECT * FROM timescaledb_information.compressed_hypertable_stats;\n\n-- View data movement policies\nSELECT * FROM timescaledb_information.data_node_move_policies;\n\n-- View background jobs\nSELECT * FROM timescaledb_information.jobs;\n\n-- Check chunk distribution\nSELECT \n    hypertable_name,\n    tablespace_name,\n    COUNT(*) as chunk_count,\n    pg_size_pretty(SUM(chunk_size)) as total_size\nFROM timescaledb_information.chunks\nGROUP BY hypertable_name, tablespace_name;\n\n-- Manual compression\nSELECT compress_chunk('_timescaledb_internal._hyper_1_1_chunk');\n\n-- Manual data movement\nSELECT move_chunk('_timescaledb_internal._hyper_1_1_chunk', 'hdd_cold');\n</code></pre>"},{"location":"reference/commands/#maintenance","title":"Maintenance","text":"<pre><code>-- Update statistics\nANALYZE notifications;\nANALYZE audit_logs;\n\n-- Vacuum tables\nVACUUM ANALYZE notifications;\n\n-- Check table sizes\nSELECT \n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\nFROM pg_tables \nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n\n-- Check slow queries\nSELECT query, calls, total_time, mean_time \nFROM pg_stat_statements \nORDER BY total_time DESC \nLIMIT 10;\n\n-- Kill long-running queries\nSELECT pg_terminate_backend(pid) \nFROM pg_stat_activity \nWHERE state = 'active' \nAND query_start &lt; NOW() - INTERVAL '1 hour';\n</code></pre>"},{"location":"reference/commands/#redis-commands","title":"Redis Commands","text":""},{"location":"reference/commands/#administration_1","title":"Administration","text":"<pre><code># Connect to Redis\ndocker exec -it prs-onprem-redis redis-cli -a $REDIS_PASSWORD\n\n# Check Redis status\ndocker exec prs-onprem-redis redis-cli -a $REDIS_PASSWORD ping\n\n# Get Redis info\ndocker exec prs-onprem-redis redis-cli -a $REDIS_PASSWORD info\n\n# Monitor Redis commands\ndocker exec prs-onprem-redis redis-cli -a $REDIS_PASSWORD monitor\n\n# Check memory usage\ndocker exec prs-onprem-redis redis-cli -a $REDIS_PASSWORD info memory\n\n# Save Redis snapshot\ndocker exec prs-onprem-redis redis-cli -a $REDIS_PASSWORD bgsave\n</code></pre>"},{"location":"reference/commands/#operations_1","title":"Operations","text":"<pre><code># View all keys\ndocker exec prs-onprem-redis redis-cli -a $REDIS_PASSWORD keys \"*\"\n\n# Get key value\ndocker exec prs-onprem-redis redis-cli -a $REDIS_PASSWORD get \"key_name\"\n\n# Delete key\ndocker exec prs-onprem-redis redis-cli -a $REDIS_PASSWORD del \"key_name\"\n\n# Check key TTL\ndocker exec prs-onprem-redis redis-cli -a $REDIS_PASSWORD ttl \"key_name\"\n\n# Flush all data (DANGEROUS)\ndocker exec prs-onprem-redis redis-cli -a $REDIS_PASSWORD flushall\n\n# Get database size\ndocker exec prs-onprem-redis redis-cli -a $REDIS_PASSWORD dbsize\n</code></pre>"},{"location":"reference/commands/#monitoring-commands","title":"Monitoring Commands","text":""},{"location":"reference/commands/#monitoring","title":"Monitoring","text":"<pre><code># Check system resources\nhtop\ntop\nfree -h\ndf -h\n\n# Monitor disk I/O\niostat -x 1\niotop\n\n# Monitor network\niftop\nnethogs\nss -tuln\n\n# Check system logs\njournalctl -f\ntail -f /var/log/syslog\n\n# Monitor Docker resources\ndocker stats\ndocker system df\n</code></pre>"},{"location":"reference/commands/#monitoring_1","title":"Monitoring","text":"<pre><code># Check application logs\ndocker logs prs-onprem-backend --tail 100 -f\ndocker logs prs-onprem-frontend --tail 100 -f\ndocker logs prs-onprem-nginx --tail 100 -f\n\n# Test API endpoints\ncurl -s https://your-domain.com/api/health | jq '.'\ncurl -I https://your-domain.com/\n\n# Check response times\ntime curl -s https://your-domain.com/api/health\n\n# Monitor application metrics\ncurl -s http://localhost:9090/metrics\n</code></pre>"},{"location":"reference/commands/#testing","title":"Testing","text":"<pre><code># Load testing with Apache Bench\nab -n 1000 -c 10 https://your-domain.com/\nab -n 500 -c 5 https://your-domain.com/api/health\n\n# Database performance testing\ndocker exec prs-onprem-postgres-timescale pgbench -i -s 10 prs_production\ndocker exec prs-onprem-postgres-timescale pgbench -c 5 -j 2 -t 1000 prs_production\n\n# Storage performance testing\nsudo fio --name=test --filename=/mnt/ssd/test --size=1G --rw=randwrite --bs=4k --numjobs=4 --time_based --runtime=60\n</code></pre>"},{"location":"reference/commands/#security-commands","title":"Security Commands","text":""},{"location":"reference/commands/#certificate-management","title":"Certificate Management","text":"<pre><code># Check certificate validity\nopenssl x509 -in 02-docker-configuration/ssl/certificate.crt -text -noout\n\n# Check certificate expiration\nopenssl x509 -in 02-docker-configuration/ssl/certificate.crt -noout -dates\n\n# Test SSL connection\nopenssl s_client -connect your-domain.com:443 -servername your-domain.com\n\n# Verify certificate chain\ncurl -vI https://your-domain.com/\n\n# Renew Let's Encrypt certificate\ncertbot renew --dry-run\ncertbot renew\n</code></pre>"},{"location":"reference/commands/#management_2","title":"Management","text":"<pre><code># Check firewall status\nsudo ufw status verbose\n\n# Add firewall rule\nsudo ufw allow from 192.168.0.0/20 to any port 80\n\n# Remove firewall rule\nsudo ufw delete allow 80\n\n# Reset firewall\nsudo ufw --force reset\n\n# Enable/disable firewall\nsudo ufw enable\nsudo ufw disable\n</code></pre>"},{"location":"reference/commands/#scanning","title":"Scanning","text":"<pre><code># Check for security updates\nsudo apt list --upgradable\n\n# Scan for open ports\nnmap -sS -O localhost\nsudo netstat -tulpn | grep LISTEN\n\n# Check fail2ban status\nsudo fail2ban-client status\nsudo fail2ban-client status sshd\n\n# View authentication logs\nsudo tail -f /var/log/auth.log\n</code></pre>"},{"location":"reference/commands/#maintenance-commands","title":"Maintenance Commands","text":""},{"location":"reference/commands/#operations_2","title":"Operations","text":"<pre><code># Run manual backup\n./scripts/backup-maintenance.sh\n\n# List backups\nls -la /mnt/hdd/postgres-backups/daily/\nls -la /mnt/hdd/redis-backups/\n\n# Verify backup integrity\nsha256sum -c /mnt/hdd/postgres-backups/daily/*.sha256\n\n# Restore from backup\n./scripts/restore-database.sh /mnt/hdd/postgres-backups/daily/backup_20240822.sql\n</code></pre>"},{"location":"reference/commands/#management_3","title":"Management","text":"<pre><code># View application logs\ntail -f /mnt/ssd/logs/application.log\ntail -f /mnt/ssd/logs/error.log\n\n# Rotate logs\n./scripts/log-rotation.sh\n\n# Archive old logs\nfind /mnt/ssd/logs -name \"*.log\" -mtime +7 -exec mv {} /mnt/hdd/app-logs-archive/ \\;\n\n# Compress logs\nfind /mnt/ssd/logs -name \"*.log\" -mtime +1 -exec gzip {} \\;\n</code></pre>"},{"location":"reference/commands/#cleanup","title":"Cleanup","text":"<pre><code># Clean Docker resources\ndocker system prune -f\ndocker volume prune -f\ndocker image prune -f\n\n# Clean temporary files\nsudo find /tmp -type f -mtime +7 -delete\nsudo find /var/tmp -type f -mtime +7 -delete\n\n# Clean package cache\nsudo apt autoremove\nsudo apt autoclean\n\n# Clean old logs\nsudo journalctl --vacuum-time=30d\n</code></pre>"},{"location":"reference/commands/#troubleshooting-commands","title":"Troubleshooting Commands","text":""},{"location":"reference/commands/#debugging","title":"Debugging","text":"<pre><code># Check service health\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml ps\ndocker inspect prs-onprem-backend | grep Health -A 10\n\n# Debug container startup\ndocker logs prs-onprem-backend\ndocker exec prs-onprem-backend ps aux\n\n# Check container resources\ndocker stats prs-onprem-backend\ndocker exec prs-onprem-backend free -h\n\n# Network debugging\ndocker network inspect prs_onprem_network\ndocker exec prs-onprem-backend ping prs-onprem-postgres-timescale\n</code></pre>"},{"location":"reference/commands/#debugging_1","title":"Debugging","text":"<pre><code># Check CPU usage\ntop -p $(docker inspect -f '{{.State.Pid}}' prs-onprem-backend)\n\n# Check memory usage\ncat /proc/$(docker inspect -f '{{.State.Pid}}' prs-onprem-backend)/status\n\n# Check disk I/O\niotop -p $(docker inspect -f '{{.State.Pid}}' prs-onprem-backend)\n\n# Check network connections\nss -tulpn | grep $(docker inspect -f '{{.State.Pid}}' prs-onprem-backend)\n</code></pre>"},{"location":"reference/commands/#procedures","title":"Procedures","text":"<pre><code># Emergency stop all services\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml down\n\n# Emergency restart\nsudo systemctl restart docker\ndocker-compose -f 02-docker-configuration/docker-compose.onprem.yml up -d\n\n# Emergency database recovery\n./scripts/restore-database.sh /mnt/hdd/postgres-backups/daily/latest-backup.sql\n\n# Emergency storage cleanup\nsudo find /mnt/ssd -name \"*.tmp\" -delete\nsudo find /mnt/ssd/logs -name \"*.log\" -mtime +1 -exec gzip {} \\;\n</code></pre> <p>Command Aliases</p> <p>Create aliases for frequently used commands to improve efficiency: <pre><code>alias prs-logs='docker-compose -f 02-docker-configuration/docker-compose.onprem.yml logs -f'\nalias prs-ps='docker-compose -f 02-docker-configuration/docker-compose.onprem.yml ps'\nalias prs-restart='docker-compose -f 02-docker-configuration/docker-compose.onprem.yml restart'\n</code></pre></p> <p>Dangerous Commands</p> <p>Commands marked as DANGEROUS can cause data loss. Always ensure you have recent backups before executing them.</p>"},{"location":"reference/config-files/","title":"Configuration Files Reference","text":""},{"location":"reference/config-files/#overview","title":"Overview","text":"<p>This reference guide provides comprehensive documentation for all configuration files used in the PRS on-premises deployment, including their purposes, formats, and configuration options.</p>"},{"location":"reference/config-files/#docker-configuration-files","title":"Docker Configuration Files","text":""},{"location":"reference/config-files/#docker-compose-configuration","title":"Docker Compose Configuration","text":""},{"location":"reference/config-files/#main-compose-file","title":"Main Compose File","text":"<p>File: <code>02-docker-configuration/docker-compose.onprem.yml</code></p> <pre><code>version: '3.8'\n\nservices:\n  # Nginx Reverse Proxy\n  nginx:\n    image: nginx:1.24-alpine\n    container_name: prs-onprem-nginx\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro\n      - ./nginx/conf.d:/etc/nginx/conf.d:ro\n      - ./ssl:/etc/nginx/ssl:ro\n      - /mnt/ssd/logs/nginx:/var/log/nginx\n    depends_on:\n      - frontend\n      - backend\n    restart: unless-stopped\n    networks:\n      - prs-network\n\n  # Frontend Service\n  frontend:\n    image: prs-frontend:latest\n    container_name: prs-onprem-frontend\n    environment:\n      - VITE_APP_API_URL=${VITE_APP_API_URL}\n      - VITE_APP_BASE_URL=${VITE_APP_BASE_URL}\n      - VITE_APP_ENVIRONMENT=${VITE_APP_ENVIRONMENT}\n    volumes:\n      - /mnt/ssd/uploads:/app/uploads:ro\n    restart: unless-stopped\n    networks:\n      - prs-network\n\n  # Backend Service\n  backend:\n    image: prs-backend:latest\n    container_name: prs-onprem-backend\n    environment:\n      - NODE_ENV=${NODE_ENV}\n      - POSTGRES_HOST=${POSTGRES_HOST}\n      - POSTGRES_PORT=${POSTGRES_PORT}\n      - POSTGRES_DB=${POSTGRES_DB}\n      - POSTGRES_USER=${POSTGRES_USER}\n      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n      - REDIS_HOST=${REDIS_HOST}\n      - REDIS_PORT=${REDIS_PORT}\n      - REDIS_PASSWORD=${REDIS_PASSWORD}\n      - JWT_SECRET=${JWT_SECRET}\n      - SESSION_SECRET=${SESSION_SECRET}\n    volumes:\n      - /mnt/ssd/uploads:/app/uploads\n      - /mnt/ssd/logs/backend:/app/logs\n    depends_on:\n      - postgres\n      - redis\n    restart: unless-stopped\n    networks:\n      - prs-network\n\n  # PostgreSQL with TimescaleDB\n  postgres:\n    image: timescale/timescaledb:2.11.2-pg15\n    container_name: prs-onprem-postgres-timescale\n    environment:\n      - POSTGRES_DB=${POSTGRES_DB}\n      - POSTGRES_USER=${POSTGRES_USER}\n      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256\n    volumes:\n      - /mnt/ssd/postgresql-hot:/var/lib/postgresql/data\n      - /mnt/hdd/wal-archive:/var/lib/postgresql/wal-archive\n      - ./postgres/postgresql.conf:/etc/postgresql/postgresql.conf:ro\n      - ./postgres/init:/docker-entrypoint-initdb.d:ro\n    ports:\n      - \"5432:5432\"\n    restart: unless-stopped\n    networks:\n      - prs-network\n\n  # Redis Cache\n  redis:\n    image: redis:7.2-alpine\n    container_name: prs-onprem-redis\n    command: redis-server --requirepass ${REDIS_PASSWORD} --appendonly yes\n    volumes:\n      - /mnt/ssd/redis:/data\n      - ./redis/redis.conf:/etc/redis/redis.conf:ro\n    ports:\n      - \"6379:6379\"\n    restart: unless-stopped\n    networks:\n      - prs-network\n\nnetworks:\n  prs-network:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.0.0/16\n\nvolumes:\n  postgres_data:\n    driver: local\n  redis_data:\n    driver: local\n</code></pre>"},{"location":"reference/config-files/#environment-configuration","title":"Environment Configuration","text":""},{"location":"reference/config-files/#production-environment-file","title":"Production Environment File","text":"<p>File: <code>02-docker-configuration/.env.production</code></p> <pre><code># Environment Configuration\nNODE_ENV=production\nVITE_APP_ENVIRONMENT=production\n\n# Domain Configuration\nDOMAIN=prs.yourcompany.com\nSERVER_IP=192.168.0.100\nSSL_EMAIL=admin@yourcompany.com\n\n# Database Configuration\nPOSTGRES_HOST=postgres\nPOSTGRES_PORT=5432\nPOSTGRES_DB=prs_production\nPOSTGRES_USER=prs_admin\nPOSTGRES_PASSWORD=prod_secure_db_password_2024\n\n# Redis Configuration\nREDIS_HOST=redis\nREDIS_PORT=6379\nREDIS_PASSWORD=prod_secure_redis_password_2024\n\n# Application Security\nJWT_SECRET=prod_jwt_secret_key_very_long_and_secure_2024\nSESSION_SECRET=prod_session_secret_key_very_long_and_secure_2024\n\n# Frontend Configuration\nVITE_APP_API_URL=https://prs.yourcompany.com/api\nVITE_APP_BASE_URL=https://prs.yourcompany.com\n\n# External API Configuration\nCITYLAND_API_URL=https://api.citylandcondo.com\nCITYLAND_API_USERNAME=production_user\nCITYLAND_API_PASSWORD=production_password\n\n# Email Configuration\nSMTP_HOST=smtp.yourcompany.com\nSMTP_PORT=587\nSMTP_USER=noreply@yourcompany.com\nSMTP_PASSWORD=smtp_production_password\n\n# Monitoring Configuration\nGRAFANA_ADMIN_PASSWORD=grafana_secure_password_2024\nMETRICS_ENABLED=true\nALERTS_ENABLED=true\nALERT_EMAIL=admin@yourcompany.com\n</code></pre>"},{"location":"reference/config-files/#nginx-configuration","title":"Nginx Configuration","text":""},{"location":"reference/config-files/#main-nginx-configuration","title":"Main Nginx Configuration","text":"<p>File: <code>02-docker-configuration/nginx/nginx.conf</code></p> <pre><code>user nginx;\nworker_processes auto;\nerror_log /var/log/nginx/error.log warn;\npid /var/run/nginx.pid;\n\nevents {\n    worker_connections 1024;\n    use epoll;\n    multi_accept on;\n}\n\nhttp {\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n\n    # Logging Configuration\n    log_format main '$remote_addr - $remote_user [$time_local] \"$request\" '\n                    '$status $body_bytes_sent \"$http_referer\" '\n                    '\"$http_user_agent\" \"$http_x_forwarded_for\" '\n                    'rt=$request_time uct=\"$upstream_connect_time\" '\n                    'uht=\"$upstream_header_time\" urt=\"$upstream_response_time\"';\n\n    access_log /var/log/nginx/access.log main;\n\n    # Performance Settings\n    sendfile on;\n    tcp_nopush on;\n    tcp_nodelay on;\n    keepalive_timeout 65;\n    types_hash_max_size 2048;\n    client_max_body_size 50M;\n\n    # Gzip Compression\n    gzip on;\n    gzip_vary on;\n    gzip_min_length 1024;\n    gzip_proxied any;\n    gzip_comp_level 6;\n    gzip_types\n        text/plain\n        text/css\n        text/xml\n        text/javascript\n        application/json\n        application/javascript\n        application/xml+rss\n        application/atom+xml\n        image/svg+xml;\n\n    # Security Headers\n    add_header X-Frame-Options DENY always;\n    add_header X-Content-Type-Options nosniff always;\n    add_header X-XSS-Protection \"1; mode=block\" always;\n    add_header Referrer-Policy \"strict-origin-when-cross-origin\" always;\n    add_header Content-Security-Policy \"default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self' data:; connect-src 'self'; frame-ancestors 'none';\" always;\n\n    # Rate Limiting\n    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;\n    limit_req_zone $binary_remote_addr zone=login:10m rate=1r/s;\n\n    # Include site configurations\n    include /etc/nginx/conf.d/*.conf;\n}\n</code></pre>"},{"location":"reference/config-files/#site-configuration","title":"Site Configuration","text":"<p>File: <code>02-docker-configuration/nginx/conf.d/prs.conf</code></p> <pre><code># Upstream Backend\nupstream backend {\n    server backend:4000;\n    keepalive 32;\n}\n\n# Upstream Frontend\nupstream frontend {\n    server frontend:3000;\n    keepalive 32;\n}\n\n# HTTP to HTTPS Redirect\nserver {\n    listen 80;\n    server_name prs.yourcompany.com;\n    return 301 https://$server_name$request_uri;\n}\n\n# Main HTTPS Server\nserver {\n    listen 443 ssl http2;\n    server_name prs.yourcompany.com;\n\n    # SSL Configuration\n    ssl_certificate /etc/nginx/ssl/certificate.crt;\n    ssl_certificate_key /etc/nginx/ssl/private.key;\n    ssl_trusted_certificate /etc/nginx/ssl/ca-bundle.crt;\n\n    # SSL Security Settings\n    ssl_protocols TLSv1.2 TLSv1.3;\n    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384;\n    ssl_prefer_server_ciphers off;\n    ssl_session_cache shared:SSL:10m;\n    ssl_session_timeout 10m;\n    ssl_session_tickets off;\n\n    # HSTS\n    add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\" always;\n\n    # API Routes\n    location /api/ {\n        limit_req zone=api burst=20 nodelay;\n\n        proxy_pass http://backend;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection 'upgrade';\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_cache_bypass $http_upgrade;\n\n        # Timeouts\n        proxy_connect_timeout 30s;\n        proxy_send_timeout 30s;\n        proxy_read_timeout 30s;\n    }\n\n    # Authentication Routes (Stricter Rate Limiting)\n    location /api/auth/ {\n        limit_req zone=login burst=5 nodelay;\n\n        proxy_pass http://backend;\n        proxy_http_version 1.1;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n\n    # File Upload Routes\n    location /api/files/upload {\n        client_max_body_size 50M;\n        proxy_pass http://backend;\n        proxy_http_version 1.1;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n\n        # Extended timeouts for file uploads\n        proxy_connect_timeout 60s;\n        proxy_send_timeout 60s;\n        proxy_read_timeout 60s;\n    }\n\n    # Static File Serving\n    location /uploads/ {\n        alias /var/www/uploads/;\n        expires 1y;\n        add_header Cache-Control \"public, immutable\";\n        access_log off;\n    }\n\n    # Frontend Application\n    location / {\n        proxy_pass http://frontend;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection 'upgrade';\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_cache_bypass $http_upgrade;\n    }\n\n    # Health Check Endpoint\n    location /health {\n        access_log off;\n        return 200 \"healthy\\n\";\n        add_header Content-Type text/plain;\n    }\n}\n</code></pre>"},{"location":"reference/config-files/#database-configuration","title":"Database Configuration","text":""},{"location":"reference/config-files/#postgresql-configuration","title":"PostgreSQL Configuration","text":"<p>File: <code>02-docker-configuration/postgres/postgresql.conf</code></p> <pre><code># PostgreSQL Configuration for PRS On-Premises\n# Optimized for 16GB RAM, SSD/HDD dual storage\n\n# Connection Settings\nlisten_addresses = '*'\nport = 5432\nmax_connections = 150\n\n# Memory Settings\nshared_buffers = 2GB\neffective_cache_size = 4GB\nwork_mem = 32MB\nmaintenance_work_mem = 512MB\n\n# WAL Settings\nwal_level = replica\nwal_buffers = 32MB\ncheckpoint_completion_target = 0.9\nmax_wal_size = 2GB\nmin_wal_size = 512MB\n\n# Query Planner Settings\nrandom_page_cost = 1.1\neffective_io_concurrency = 200\nseq_page_cost = 1.0\n\n# Background Writer Settings\nbgwriter_delay = 200ms\nbgwriter_lru_maxpages = 100\nbgwriter_lru_multiplier = 2.0\n\n# Autovacuum Settings\nautovacuum = on\nautovacuum_naptime = 1min\nautovacuum_vacuum_threshold = 50\nautovacuum_analyze_threshold = 50\nautovacuum_vacuum_scale_factor = 0.2\nautovacuum_analyze_scale_factor = 0.1\n\n# Logging Settings\nlog_destination = 'stderr'\nlogging_collector = on\nlog_directory = 'log'\nlog_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'\nlog_rotation_age = 1d\nlog_rotation_size = 100MB\nlog_min_duration_statement = 1000\nlog_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '\nlog_checkpoints = on\nlog_connections = on\nlog_disconnections = on\nlog_lock_waits = on\n\n# TimescaleDB Settings\nshared_preload_libraries = 'timescaledb'\ntimescaledb.max_background_workers = 16\n\n# Performance Monitoring\ntrack_activities = on\ntrack_counts = on\ntrack_io_timing = on\ntrack_functions = all\n</code></pre>"},{"location":"reference/config-files/#database-initialization","title":"Database Initialization","text":"<p>File: <code>02-docker-configuration/postgres/init/01-init-timescaledb.sql</code></p> <pre><code>-- Initialize TimescaleDB Extension\nCREATE EXTENSION IF NOT EXISTS timescaledb;\n\n-- Create tablespaces for dual storage\nCREATE TABLESPACE ssd_hot LOCATION '/var/lib/postgresql/data/ssd_hot';\nCREATE TABLESPACE hdd_cold LOCATION '/var/lib/postgresql/data/hdd_cold';\n\n-- Create hypertables for time-series data\nSELECT create_hypertable('notifications', 'created_at', \n    chunk_time_interval =&gt; INTERVAL '1 day',\n    partitioning_column =&gt; 'user_id',\n    number_partitions =&gt; 4\n);\n\nSELECT create_hypertable('audit_logs', 'created_at',\n    chunk_time_interval =&gt; INTERVAL '1 day',\n    partitioning_column =&gt; 'user_id', \n    number_partitions =&gt; 4\n);\n\n-- Set up compression policies\nALTER TABLE notifications SET (\n    timescaledb.compress,\n    timescaledb.compress_segmentby = 'user_id',\n    timescaledb.compress_orderby = 'created_at DESC'\n);\n\nALTER TABLE audit_logs SET (\n    timescaledb.compress,\n    timescaledb.compress_segmentby = 'user_id, action',\n    timescaledb.compress_orderby = 'created_at DESC'\n);\n\n-- Add compression policies\nSELECT add_compression_policy('notifications', INTERVAL '7 days');\nSELECT add_compression_policy('audit_logs', INTERVAL '7 days');\n\n-- Add data movement policies\nSELECT add_move_chunk_policy('notifications', INTERVAL '30 days', 'hdd_cold');\nSELECT add_move_chunk_policy('audit_logs', INTERVAL '30 days', 'hdd_cold');\n</code></pre>"},{"location":"reference/config-files/#redis-configuration","title":"Redis Configuration","text":""},{"location":"reference/config-files/#redis-configuration_1","title":"Redis Configuration","text":"<p>File: <code>02-docker-configuration/redis/redis.conf</code></p> <pre><code># Redis Configuration for PRS On-Premises\n\n# Network\nbind 0.0.0.0\nport 6379\nprotected-mode yes\n\n# General\ndaemonize no\nsupervised no\npidfile /var/run/redis_6379.pid\nloglevel notice\nlogfile \"\"\n\n# Persistence\nsave 900 1\nsave 300 10\nsave 60 10000\nstop-writes-on-bgsave-error yes\nrdbcompression yes\nrdbchecksum yes\ndbfilename dump.rdb\ndir /data\n\n# Append Only File\nappendonly yes\nappendfilename \"appendonly.aof\"\nappendfsync everysec\nno-appendfsync-on-rewrite no\nauto-aof-rewrite-percentage 100\nauto-aof-rewrite-min-size 64mb\n\n# Memory Management\nmaxmemory 1gb\nmaxmemory-policy allkeys-lru\nmaxmemory-samples 5\n\n# Clients\nmaxclients 10000\ntimeout 300\n\n# Security\nrequirepass your_redis_password_here\n\n# Performance\ntcp-keepalive 300\ntcp-backlog 511\n</code></pre>"},{"location":"reference/config-files/#monitoring-configuration","title":"Monitoring Configuration","text":""},{"location":"reference/config-files/#prometheus-configuration","title":"Prometheus Configuration","text":"<p>File: <code>02-docker-configuration/prometheus/prometheus.yml</code></p> <pre><code>global:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nrule_files:\n  - \"rules/*.yml\"\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n          - alertmanager:9093\n\nscrape_configs:\n  # Node Exporter\n  - job_name: 'node-exporter'\n    static_configs:\n      - targets: ['node-exporter:9100']\n\n  # PostgreSQL Exporter\n  - job_name: 'postgres-exporter'\n    static_configs:\n      - targets: ['postgres-exporter:9187']\n\n  # Redis Exporter\n  - job_name: 'redis-exporter'\n    static_configs:\n      - targets: ['redis-exporter:9121']\n\n  # Application Metrics\n  - job_name: 'prs-backend'\n    static_configs:\n      - targets: ['backend:4000']\n    metrics_path: '/metrics'\n\n  # Nginx Metrics\n  - job_name: 'nginx'\n    static_configs:\n      - targets: ['nginx-exporter:9113']\n</code></pre>"},{"location":"reference/config-files/#grafana-configuration","title":"Grafana Configuration","text":"<p>File: <code>02-docker-configuration/grafana/grafana.ini</code></p> <pre><code>[server]\nhttp_port = 3001\ndomain = prs.yourcompany.com\nroot_url = https://prs.yourcompany.com/grafana/\n\n[security]\nadmin_user = admin\nadmin_password = grafana_secure_password_2024\nsecret_key = grafana_secret_key_2024\n\n[auth]\ndisable_login_form = false\ndisable_signout_menu = false\n\n[auth.anonymous]\nenabled = false\n\n[database]\ntype = postgres\nhost = postgres:5432\nname = grafana\nuser = grafana_user\npassword = grafana_password\n\n[session]\nprovider = redis\nprovider_config = addr=redis:6379,pool_size=100,db=2\n\n[log]\nmode = console file\nlevel = info\n\n[paths]\ndata = /var/lib/grafana\nlogs = /var/log/grafana\nplugins = /var/lib/grafana/plugins\nprovisioning = /etc/grafana/provisioning\n</code></pre>"},{"location":"reference/config-files/#ssl-configuration","title":"SSL Configuration","text":""},{"location":"reference/config-files/#ssl-certificate-configuration","title":"SSL Certificate Configuration","text":"<p>File: <code>02-docker-configuration/ssl/ssl.conf</code></p> <pre><code># SSL Configuration Template\n\n# Certificate Paths\nssl_certificate /etc/nginx/ssl/certificate.crt;\nssl_certificate_key /etc/nginx/ssl/private.key;\nssl_trusted_certificate /etc/nginx/ssl/ca-bundle.crt;\n\n# SSL Protocols and Ciphers\nssl_protocols TLSv1.2 TLSv1.3;\nssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-SHA384;\nssl_prefer_server_ciphers off;\n\n# SSL Session Settings\nssl_session_cache shared:SSL:10m;\nssl_session_timeout 10m;\nssl_session_tickets off;\n\n# OCSP Stapling\nssl_stapling on;\nssl_stapling_verify on;\nresolver 8.8.8.8 8.8.4.4 valid=300s;\nresolver_timeout 5s;\n\n# Security Headers\nadd_header Strict-Transport-Security \"max-age=31536000; includeSubDomains; preload\" always;\nadd_header X-Frame-Options DENY always;\nadd_header X-Content-Type-Options nosniff always;\nadd_header X-XSS-Protection \"1; mode=block\" always;\n</code></pre> <p>Configuration Reference Complete</p> <p>This comprehensive reference covers all major configuration files used in the PRS on-premises deployment with detailed explanations and examples.</p> <p>Configuration Management</p> <p>Keep configuration files in version control and use environment-specific configurations for different deployment stages.</p> <p>Security Configuration</p> <p>Always review and customize security settings, passwords, and certificates before deploying to production environments.</p>"},{"location":"reference/environment/","title":"Environment Variables","text":""},{"location":"reference/environment/#overview","title":"Overview","text":"<p>This reference guide provides a comprehensive list of all environment variables used in the PRS on-premises deployment, including their purposes, default values, and configuration examples.</p>"},{"location":"reference/environment/#core-application-variables","title":"Core Application Variables","text":""},{"location":"reference/environment/#database-configuration","title":"Database Configuration","text":"<pre><code># PostgreSQL/TimescaleDB Configuration\nPOSTGRES_HOST=postgres                    # Database host (container name)\nPOSTGRES_PORT=5432                       # Database port\nPOSTGRES_DB=prs_production               # Database name\nPOSTGRES_USER=prs_admin                  # Database username\nPOSTGRES_PASSWORD=secure_random_password # Database password (CHANGE THIS)\n\n# Database Connection Pool\nDB_POOL_MIN=5                            # Minimum connections\nDB_POOL_MAX=20                           # Maximum connections\nDB_POOL_ACQUIRE=30000                    # Connection timeout (ms)\nDB_POOL_IDLE=10000                       # Idle timeout (ms)\n\n# Database Performance\nDB_STATEMENT_TIMEOUT=30000               # Query timeout (ms)\nDB_IDLE_TRANSACTION_TIMEOUT=60000        # Idle transaction timeout (ms)\n</code></pre>"},{"location":"reference/environment/#redis-configuration","title":"Redis Configuration","text":"<pre><code># Redis Cache Configuration\nREDIS_HOST=redis                         # Redis host (container name)\nREDIS_PORT=6379                         # Redis port\nREDIS_PASSWORD=redis_secure_password     # Redis password (CHANGE THIS)\nREDIS_DB=0                              # Redis database number\n\n# Redis Session Store\nREDIS_SESSION_DB=1                       # Session database number\nREDIS_SESSION_TTL=86400                  # Session TTL (seconds)\n</code></pre>"},{"location":"reference/environment/#application-server","title":"Application Server","text":"<pre><code># Server Configuration\nNODE_ENV=production                      # Environment mode\nPORT=4000                               # Backend port\nHOST=0.0.0.0                           # Bind address\n\n# Security\nJWT_SECRET=your_jwt_secret_key_here     # JWT signing key (CHANGE THIS)\nJWT_EXPIRES_IN=24h                      # JWT expiration\nSESSION_SECRET=your_session_secret      # Session secret (CHANGE THIS)\n\n# CORS Configuration\nCORS_ORIGIN=https://your-domain.com     # Allowed origins\nCORS_CREDENTIALS=true                   # Allow credentials\n</code></pre>"},{"location":"reference/environment/#frontend-configuration","title":"Frontend Configuration","text":"<pre><code># Vite/React Configuration\nVITE_APP_API_URL=https://your-domain.com/api    # API base URL\nVITE_APP_BASE_URL=https://your-domain.com       # Application base URL\nVITE_APP_ENVIRONMENT=production                 # Environment name\nVITE_APP_VERSION=1.0.0                         # Application version\n\n# Feature Flags\nVITE_APP_ENABLE_DEBUG=false             # Debug mode\nVITE_APP_ENABLE_ANALYTICS=true          # Analytics tracking\nVITE_APP_ENABLE_NOTIFICATIONS=true      # Push notifications\n</code></pre>"},{"location":"reference/environment/#external-integrations","title":"External Integrations","text":""},{"location":"reference/environment/#cityland-api-integration","title":"CityLand API Integration","text":"<pre><code># CityLand API Configuration\nCITYLAND_API_URL=https://your-api-endpoint.com  # API endpoint\nCITYLAND_API_USERNAME=your_username             # API username\nCITYLAND_API_PASSWORD=your_password             # API password\nCITYLAND_API_TIMEOUT=30000                      # Request timeout (ms)\nCITYLAND_API_RETRY_ATTEMPTS=3                   # Retry attempts\nCITYLAND_API_RETRY_DELAY=1000                   # Retry delay (ms)\n\n# API Rate Limiting\nCITYLAND_API_RATE_LIMIT=100                     # Requests per minute\nCITYLAND_API_BURST_LIMIT=10                     # Burst requests\n</code></pre>"},{"location":"reference/environment/#email-configuration","title":"Email Configuration","text":"<pre><code># SMTP Configuration\nSMTP_HOST=smtp.your-domain.com           # SMTP server\nSMTP_PORT=587                           # SMTP port (587 for TLS, 465 for SSL)\nSMTP_SECURE=false                       # Use SSL (true for port 465)\nSMTP_USER=noreply@your-domain.com       # SMTP username\nSMTP_PASSWORD=smtp_password             # SMTP password\n\n# Email Settings\nEMAIL_FROM=noreply@your-domain.com      # From address\nEMAIL_FROM_NAME=PRS System              # From name\nEMAIL_REPLY_TO=support@your-domain.com  # Reply-to address\n\n# Email Templates\nEMAIL_TEMPLATE_DIR=/app/templates/email # Template directory\nEMAIL_LOGO_URL=https://your-domain.com/logo.png  # Logo URL\n</code></pre>"},{"location":"reference/environment/#file-storage","title":"File Storage","text":"<pre><code># File Upload Configuration\nUPLOAD_MAX_SIZE=52428800                # Max file size (50MB)\nUPLOAD_ALLOWED_TYPES=pdf,doc,docx,xls,xlsx,jpg,jpeg,png  # Allowed types\nUPLOAD_STORAGE_PATH=/app/uploads        # Storage path\nUPLOAD_TEMP_PATH=/app/temp             # Temporary path\n\n# File Processing\nFILE_PROCESSING_ENABLED=true           # Enable file processing\nFILE_VIRUS_SCAN_ENABLED=false         # Enable virus scanning\nFILE_THUMBNAIL_ENABLED=true           # Generate thumbnails\n</code></pre>"},{"location":"reference/environment/#system-configuration","title":"System Configuration","text":""},{"location":"reference/environment/#domain-and-ssl","title":"Domain and SSL","text":"<pre><code># Domain Configuration\nDOMAIN=your-domain.com                  # Primary domain\nSERVER_IP=192.168.0.100                # Server IP address\nSSL_EMAIL=admin@your-domain.com        # SSL certificate email\n\n# SSL Configuration\nSSL_CERT_PATH=/etc/nginx/ssl/certificate.crt    # Certificate path\nSSL_KEY_PATH=/etc/nginx/ssl/private.key         # Private key path\nSSL_CA_PATH=/etc/nginx/ssl/ca-bundle.crt        # CA bundle path\n</code></pre>"},{"location":"reference/environment/#logging-configuration","title":"Logging Configuration","text":"<pre><code># Application Logging\nLOG_LEVEL=info                          # Log level (error, warn, info, debug)\nLOG_FORMAT=json                         # Log format (json, text)\nLOG_FILE=/app/logs/application.log      # Log file path\nLOG_MAX_SIZE=100MB                      # Max log file size\nLOG_MAX_FILES=10                        # Max log files to keep\n\n# Database Logging\nDB_LOG_QUERIES=false                    # Log all queries\nDB_LOG_SLOW_QUERIES=true               # Log slow queries\nDB_SLOW_QUERY_THRESHOLD=1000           # Slow query threshold (ms)\n\n# Access Logging\nACCESS_LOG_ENABLED=true                 # Enable access logs\nACCESS_LOG_FORMAT=combined              # Access log format\n</code></pre>"},{"location":"reference/environment/#performance-configuration","title":"Performance Configuration","text":"<pre><code># Application Performance\nWORKER_PROCESSES=4                      # Number of worker processes\nWORKER_CONNECTIONS=1000                 # Connections per worker\nKEEPALIVE_TIMEOUT=65                    # Keep-alive timeout\n\n# Caching\nCACHE_ENABLED=true                      # Enable caching\nCACHE_TTL=3600                         # Default cache TTL (seconds)\nCACHE_MAX_SIZE=100MB                   # Max cache size\n\n# Rate Limiting\nRATE_LIMIT_ENABLED=true                # Enable rate limiting\nRATE_LIMIT_WINDOW=900000               # Rate limit window (15 minutes)\nRATE_LIMIT_MAX_REQUESTS=100            # Max requests per window\n</code></pre>"},{"location":"reference/environment/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"reference/environment/#metrics-and-monitoring","title":"Metrics and Monitoring","text":"<pre><code># Prometheus Metrics\nMETRICS_ENABLED=true                    # Enable metrics collection\nMETRICS_PORT=9090                      # Metrics port\nMETRICS_PATH=/metrics                  # Metrics endpoint\n\n# Health Checks\nHEALTH_CHECK_ENABLED=true              # Enable health checks\nHEALTH_CHECK_INTERVAL=30000            # Health check interval (ms)\nHEALTH_CHECK_TIMEOUT=5000              # Health check timeout (ms)\n\n# Grafana Configuration\nGRAFANA_ADMIN_USER=admin               # Grafana admin username\nGRAFANA_ADMIN_PASSWORD=secure_password # Grafana admin password (CHANGE THIS)\nGRAFANA_PORT=3001                      # Grafana port\n</code></pre>"},{"location":"reference/environment/#alerting","title":"Alerting","text":"<pre><code># Alert Configuration\nALERTS_ENABLED=true                     # Enable alerting\nALERT_EMAIL=admin@your-domain.com      # Alert email address\nALERT_WEBHOOK_URL=                     # Webhook URL for alerts\n\n# Slack Integration\nSLACK_WEBHOOK_URL=                     # Slack webhook URL\nSLACK_CHANNEL=#prs-alerts              # Slack channel\nSLACK_USERNAME=PRS-Bot                 # Slack bot username\n</code></pre>"},{"location":"reference/environment/#security-configuration","title":"Security Configuration","text":""},{"location":"reference/environment/#authentication-and-authorization","title":"Authentication and Authorization","text":"<pre><code># Authentication\nAUTH_ENABLED=true                       # Enable authentication\nAUTH_SESSION_TIMEOUT=86400             # Session timeout (seconds)\nAUTH_MAX_LOGIN_ATTEMPTS=5              # Max login attempts\nAUTH_LOCKOUT_DURATION=900              # Lockout duration (seconds)\n\n# Password Policy\nPASSWORD_MIN_LENGTH=8                   # Minimum password length\nPASSWORD_REQUIRE_UPPERCASE=true        # Require uppercase\nPASSWORD_REQUIRE_LOWERCASE=true        # Require lowercase\nPASSWORD_REQUIRE_NUMBERS=true          # Require numbers\nPASSWORD_REQUIRE_SYMBOLS=true          # Require symbols\n\n# Two-Factor Authentication\nTWO_FACTOR_ENABLED=false               # Enable 2FA\nTWO_FACTOR_ISSUER=PRS                  # 2FA issuer name\n</code></pre>"},{"location":"reference/environment/#security-headers","title":"Security Headers","text":"<pre><code># Security Configuration\nSECURITY_HEADERS_ENABLED=true          # Enable security headers\nHSTS_MAX_AGE=31536000                 # HSTS max age\nCSP_ENABLED=true                      # Enable CSP\nCSP_REPORT_URI=/api/csp-report        # CSP report URI\n\n# CORS Security\nCORS_MAX_AGE=86400                    # CORS max age\nCORS_ALLOWED_HEADERS=Content-Type,Authorization  # Allowed headers\nCORS_EXPOSED_HEADERS=X-Total-Count    # Exposed headers\n</code></pre>"},{"location":"reference/environment/#development-and-testing","title":"Development and Testing","text":""},{"location":"reference/environment/#development-environment","title":"Development Environment","text":"<pre><code># Development Configuration\nDEBUG=true                             # Enable debug mode\nHOT_RELOAD=true                       # Enable hot reload\nSOURCE_MAPS=true                      # Generate source maps\n\n# Testing\nTEST_DATABASE_URL=postgresql://test_user:test_pass@localhost:5433/prs_test\nTEST_REDIS_URL=redis://localhost:6380/0\nTEST_TIMEOUT=30000                    # Test timeout (ms)\n</code></pre>"},{"location":"reference/environment/#feature-flags","title":"Feature Flags","text":"<pre><code># Feature Toggles\nFEATURE_NEW_DASHBOARD=true            # Enable new dashboard\nFEATURE_ADVANCED_REPORTING=false     # Enable advanced reporting\nFEATURE_MOBILE_APP=false             # Enable mobile app features\nFEATURE_API_V2=false                 # Enable API v2\n</code></pre>"},{"location":"reference/environment/#environment-file-examples","title":"Environment File Examples","text":""},{"location":"reference/environment/#production-environment-envproduction","title":"Production Environment (.env.production)","text":"<pre><code># Production Environment Configuration\nNODE_ENV=production\nDOMAIN=prs.yourcompany.com\nSERVER_IP=192.168.0.100\n\n# Database\nPOSTGRES_HOST=postgres\nPOSTGRES_PORT=5432\nPOSTGRES_DB=prs_production\nPOSTGRES_USER=prs_admin\nPOSTGRES_PASSWORD=prod_secure_db_password_2024\n\n# Redis\nREDIS_HOST=redis\nREDIS_PORT=6379\nREDIS_PASSWORD=prod_secure_redis_password_2024\n\n# Security\nJWT_SECRET=prod_jwt_secret_key_very_long_and_secure_2024\nSESSION_SECRET=prod_session_secret_key_very_long_and_secure_2024\n\n# External APIs\nCITYLAND_API_URL=https://api.citylandcondo.com\nCITYLAND_API_USERNAME=production_user\nCITYLAND_API_PASSWORD=production_password\n\n# Email\nSMTP_HOST=smtp.yourcompany.com\nSMTP_PORT=587\nSMTP_USER=noreply@yourcompany.com\nSMTP_PASSWORD=smtp_production_password\n\n# Monitoring\nGRAFANA_ADMIN_PASSWORD=grafana_secure_password_2024\nMETRICS_ENABLED=true\nALERTS_ENABLED=true\nALERT_EMAIL=admin@yourcompany.com\n\n# Frontend\nVITE_APP_API_URL=https://prs.yourcompany.com/api\nVITE_APP_BASE_URL=https://prs.yourcompany.com\nVITE_APP_ENVIRONMENT=production\n</code></pre>"},{"location":"reference/environment/#staging-environment-envstaging","title":"Staging Environment (.env.staging)","text":"<pre><code># Staging Environment Configuration\nNODE_ENV=staging\nDOMAIN=prs-staging.yourcompany.com\nSERVER_IP=192.168.0.101\n\n# Database\nPOSTGRES_HOST=postgres\nPOSTGRES_PORT=5432\nPOSTGRES_DB=prs_staging\nPOSTGRES_USER=prs_admin\nPOSTGRES_PASSWORD=staging_secure_db_password_2024\n\n# Debug and Testing\nDEBUG=true\nLOG_LEVEL=debug\nFEATURE_NEW_DASHBOARD=true\nFEATURE_ADVANCED_REPORTING=true\n\n# External APIs (Test endpoints)\nCITYLAND_API_URL=https://api-test.citylandcondo.com\nCITYLAND_API_USERNAME=staging_user\nCITYLAND_API_PASSWORD=staging_password\n\n# Frontend\nVITE_APP_API_URL=https://prs-staging.yourcompany.com/api\nVITE_APP_BASE_URL=https://prs-staging.yourcompany.com\nVITE_APP_ENVIRONMENT=staging\nVITE_APP_ENABLE_DEBUG=true\n</code></pre>"},{"location":"reference/environment/#development-environment-envdevelopment","title":"Development Environment (.env.development)","text":"<pre><code># Development Environment Configuration\nNODE_ENV=development\nDOMAIN=localhost\nSERVER_IP=127.0.0.1\n\n# Database\nPOSTGRES_HOST=localhost\nPOSTGRES_PORT=5432\nPOSTGRES_DB=prs_development\nPOSTGRES_USER=prs_dev\nPOSTGRES_PASSWORD=dev_password\n\n# Redis\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_PASSWORD=dev_redis_password\n\n# Development Features\nDEBUG=true\nHOT_RELOAD=true\nSOURCE_MAPS=true\nLOG_LEVEL=debug\n\n# Security (Development only - not secure)\nJWT_SECRET=dev_jwt_secret\nSESSION_SECRET=dev_session_secret\n\n# Frontend\nVITE_APP_API_URL=http://localhost:4000/api\nVITE_APP_BASE_URL=http://localhost:3000\nVITE_APP_ENVIRONMENT=development\nVITE_APP_ENABLE_DEBUG=true\n\n# Feature Flags (All enabled for testing)\nFEATURE_NEW_DASHBOARD=true\nFEATURE_ADVANCED_REPORTING=true\nFEATURE_MOBILE_APP=true\nFEATURE_API_V2=true\n</code></pre>"},{"location":"reference/environment/#environment-variable-validation","title":"Environment Variable Validation","text":""},{"location":"reference/environment/#validation-script","title":"Validation Script","text":"<pre><code>#!/bin/bash\n# validate-environment.sh\n\nREQUIRED_VARS=(\n    \"POSTGRES_PASSWORD\"\n    \"REDIS_PASSWORD\"\n    \"JWT_SECRET\"\n    \"SESSION_SECRET\"\n    \"DOMAIN\"\n    \"SERVER_IP\"\n)\n\nOPTIONAL_VARS=(\n    \"CITYLAND_API_URL\"\n    \"SMTP_HOST\"\n    \"GRAFANA_ADMIN_PASSWORD\"\n)\n\necho \"Validating environment variables...\"\n\n# Check required variables\nfor var in \"${REQUIRED_VARS[@]}\"; do\n    if [ -z \"${!var}\" ]; then\n        echo \"ERROR: Required variable $var is not set\"\n        exit 1\n    else\n        echo \"\u2713 $var is set\"\n    fi\ndone\n\n# Check optional variables\nfor var in \"${OPTIONAL_VARS[@]}\"; do\n    if [ -z \"${!var}\" ]; then\n        echo \"WARNING: Optional variable $var is not set\"\n    else\n        echo \"\u2713 $var is set\"\n    fi\ndone\n\n# Validate password strength\nif [ ${#POSTGRES_PASSWORD} -lt 12 ]; then\n    echo \"WARNING: POSTGRES_PASSWORD should be at least 12 characters\"\nfi\n\nif [ ${#JWT_SECRET} -lt 32 ]; then\n    echo \"WARNING: JWT_SECRET should be at least 32 characters\"\nfi\n\necho \"Environment validation completed\"\n</code></pre> <p>Environment Reference Complete</p> <p>This comprehensive reference covers all environment variables used in the PRS on-premises deployment with examples for different environments.</p> <p>Security Best Practices</p> <p>Always use strong, unique passwords for production environments and never commit sensitive environment files to version control.</p> <p>Environment Isolation</p> <p>Keep development, staging, and production environment variables completely separate and use different credentials for each environment.</p>"},{"location":"scripts/backup/","title":"Backup Scripts","text":""},{"location":"scripts/backup/#overview","title":"Overview","text":"<p>This guide covers all backup-related scripts in the PRS on-premises deployment, including automated backup procedures, restoration scripts, backup management utilities, and NAS (Network Attached Storage) integration for enterprise-grade backup redundancy.</p>"},{"location":"scripts/backup/#nas-integration","title":"NAS Integration","text":"<p>The PRS backup system now includes comprehensive NAS integration for off-site backup storage, providing enhanced disaster recovery capabilities and compliance with enterprise backup requirements.</p>"},{"location":"scripts/backup/#nas-features","title":"NAS Features","text":"<ul> <li>Multi-protocol support: CIFS/SMB and NFS</li> <li>Automatic mounting/unmounting of NAS shares</li> <li>Dual retention policies: Local (30 days) + NAS (90 days)</li> <li>Backup verification and integrity checking</li> <li>Graceful degradation if NAS is unavailable</li> <li>Enterprise security with encryption and secure credentials</li> </ul>"},{"location":"scripts/backup/#nas-configuration","title":"NAS Configuration","text":""},{"location":"scripts/backup/#step-1-configure-nas-settings","title":"Step 1: Configure NAS Settings","text":"<pre><code># Copy the NAS configuration template\ncp /opt/prs-deployment/scripts/nas-config.example.sh /opt/prs-deployment/scripts/nas-config.sh\n\n# Edit with your NAS details\nnano /opt/prs-deployment/scripts/nas-config.sh\n\n# Secure the configuration file\nchmod 600 /opt/prs-deployment/scripts/nas-config.sh\nchown root:root /opt/prs-deployment/scripts/nas-config.sh\n</code></pre>"},{"location":"scripts/backup/#step-2-nas-configuration-examples","title":"Step 2: NAS Configuration Examples","text":"<p>Synology NAS (CIFS/SMB): <pre><code>export BACKUP_TO_NAS=\"true\"\nexport NAS_HOST=\"synology.local\"\nexport NAS_SHARE=\"backup\"\nexport NAS_USERNAME=\"backup_user\"\nexport NAS_PASSWORD=\"your_secure_password\"\nexport NAS_MOUNT_PATH=\"/mnt/nas\"\n</code></pre></p> <p>QNAP NAS (CIFS/SMB): <pre><code>export BACKUP_TO_NAS=\"true\"\nexport NAS_HOST=\"qnap.local\"\nexport NAS_SHARE=\"Backup\"\nexport NAS_USERNAME=\"admin\"\nexport NAS_PASSWORD=\"your_secure_password\"\nexport NAS_MOUNT_PATH=\"/mnt/nas\"\n</code></pre></p> <p>FreeNAS/TrueNAS (NFS): <pre><code>export BACKUP_TO_NAS=\"true\"\nexport NAS_HOST=\"truenas.local\"\nexport NAS_SHARE=\"/mnt/pool1/backups\"\nexport NAS_MOUNT_PATH=\"/mnt/nas\"\n# No username/password needed for NFS\n</code></pre></p>"},{"location":"scripts/backup/#step-3-test-nas-connection","title":"Step 3: Test NAS Connection","text":"<pre><code># Test NAS connectivity and functionality\n/opt/prs-deployment/scripts/test-nas-connection.sh\n</code></pre>"},{"location":"scripts/backup/#step-4-environment-variables","title":"Step 4: Environment Variables","text":"<p>Add to your <code>.env</code> file: <pre><code># NAS Backup Configuration\nBACKUP_TO_NAS=true\nNAS_HOST=your-nas-hostname\nNAS_SHARE=backups\nNAS_USERNAME=backup_user\nNAS_PASSWORD=secure_password\nNAS_MOUNT_PATH=/mnt/nas\nNAS_RETENTION_DAYS=90\n</code></pre></p>"},{"location":"scripts/backup/#core-backup-scripts","title":"Core Backup Scripts","text":""},{"location":"scripts/backup/#full-database-backup-script-with-nas-integration","title":"Full Database Backup Script with NAS Integration","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/backup-full.sh\n# Comprehensive database backup with verification and NAS integration\n\nset -euo pipefail\n\n# Configuration\nLOCAL_BACKUP_DIR=\"/mnt/hdd/postgres-backups/daily\"\nNAS_BACKUP_DIR=\"${NAS_MOUNT_PATH:-/mnt/nas}/postgres-backups/daily\"\nRETENTION_DAYS=30\nNAS_RETENTION_DAYS=90\nCOMPRESSION_LEVEL=9\nENCRYPT_KEY=\"backup@prs.client-domain.com\"\nLOG_FILE=\"/var/log/prs-backup.log\"\n\n# NAS Configuration\nNAS_ENABLED=\"${BACKUP_TO_NAS:-true}\"\nNAS_HOST=\"${NAS_HOST:-}\"\nNAS_SHARE=\"${NAS_SHARE:-backups}\"\nNAS_USERNAME=\"${NAS_USERNAME:-}\"\nNAS_PASSWORD=\"${NAS_PASSWORD:-}\"\nNAS_MOUNT_PATH=\"${NAS_MOUNT_PATH:-/mnt/nas}\"\n\n# Database connection settings (from docker-compose)\nPGHOST=\"prs-onprem-postgres-timescale\"\nPGPORT=\"5432\"\nPGUSER=\"${POSTGRES_USER:-prs_user}\"\nPGDATABASE=\"${POSTGRES_DB:-prs_production}\"\n\n# Logging function\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\n# Error handling\nerror_exit() {\n    log_message \"ERROR: $1\"\n    exit 1\n}\n\n# NAS mounting function\nmount_nas() {\n    if [ \"$NAS_ENABLED\" != \"true\" ]; then\n        log_message \"NAS backup disabled, skipping NAS mount\"\n        return 0\n    fi\n\n    if [ -z \"$NAS_HOST\" ]; then\n        log_message \"WARNING: NAS_HOST not configured, skipping NAS backup\"\n        return 1\n    fi\n\n    log_message \"Mounting NAS for backup storage\"\n    mkdir -p \"$NAS_MOUNT_PATH\"\n\n    if mountpoint -q \"$NAS_MOUNT_PATH\"; then\n        log_message \"NAS already mounted at $NAS_MOUNT_PATH\"\n        return 0\n    fi\n\n    # Mount NAS (supports both CIFS/SMB and NFS)\n    if [ -n \"$NAS_USERNAME\" ] &amp;&amp; [ -n \"$NAS_PASSWORD\" ]; then\n        # CIFS/SMB mount\n        mount -t cifs \"//$NAS_HOST/$NAS_SHARE\" \"$NAS_MOUNT_PATH\" \\\n            -o username=\"$NAS_USERNAME\",password=\"$NAS_PASSWORD\",uid=0,gid=0,file_mode=0600,dir_mode=0700\n    else\n        # NFS mount\n        mount -t nfs \"$NAS_HOST:/$NAS_SHARE\" \"$NAS_MOUNT_PATH\"\n    fi\n\n    log_message \"NAS mounted successfully at $NAS_MOUNT_PATH\"\n}\n\n# Copy backup to NAS\ncopy_to_nas() {\n    local backup_file=\"$1\"\n\n    if [ \"$NAS_ENABLED\" != \"true\" ] || ! mountpoint -q \"$NAS_MOUNT_PATH\"; then\n        return 0\n    fi\n\n    log_message \"Copying backup to NAS\"\n    mkdir -p \"$NAS_BACKUP_DIR\"\n\n    local backup_filename=$(basename \"$backup_file\")\n    local nas_backup_file=\"$NAS_BACKUP_DIR/$backup_filename\"\n\n    if cp \"$backup_file\" \"$nas_backup_file\"; then\n        log_message \"Backup copied to NAS: $nas_backup_file\"\n\n        # Copy checksum file if it exists\n        if [ -f \"${backup_file}.sha256\" ]; then\n            cp \"${backup_file}.sha256\" \"${nas_backup_file}.sha256\"\n        fi\n\n        # Verify NAS copy\n        local local_size=$(stat -c%s \"$backup_file\")\n        local nas_size=$(stat -c%s \"$nas_backup_file\")\n\n        if [ \"$local_size\" -eq \"$nas_size\" ]; then\n            log_message \"NAS copy verified: $(numfmt --to=iec $nas_size)\"\n        else\n            log_message \"ERROR: NAS copy size mismatch\"\n            return 1\n        fi\n    else\n        log_message \"ERROR: Failed to copy backup to NAS\"\n        return 1\n    fi\n}\n\n# Main backup function\nmain() {\n    local DATE=$(date +%Y%m%d_%H%M%S)\n    local BACKUP_FILE=\"$LOCAL_BACKUP_DIR/prs_full_backup_${DATE}.sql\"\n\n    log_message \"Starting full database backup with NAS integration\"\n\n    # Mount NAS if enabled\n    if [ \"$NAS_ENABLED\" = \"true\" ]; then\n        mount_nas || log_message \"WARNING: NAS mount failed, continuing with local backup only\"\n    fi\n\n    # Create local backup directory\n    mkdir -p \"$LOCAL_BACKUP_DIR\" || error_exit \"Failed to create local backup directory\"\n\n    # Pre-backup checks\n    log_message \"Performing pre-backup checks\"\n\n    # Check database connectivity\n    if ! docker exec prs-onprem-postgres-timescale pg_isready -U \"$PGUSER\"; then\n        error_exit \"Database not ready\"\n    fi\n\n    # Check available space (require 5GB free)\n    local AVAILABLE_SPACE=$(df \"$BACKUP_DIR\" | awk 'NR==2 {print $4}')\n    if [ \"$AVAILABLE_SPACE\" -lt 5000000 ]; then\n        error_exit \"Insufficient disk space for backup\"\n    fi\n\n    # Create backup\n    log_message \"Creating database backup: $BACKUP_FILE\"\n    if ! docker exec prs-onprem-postgres-timescale pg_dump \\\n        -U \"$PGUSER\" \\\n        -d \"$PGDATABASE\" \\\n        --verbose \\\n        --format=custom \\\n        --compress=9 \\\n        --no-owner \\\n        --no-privileges \\\n        &gt; \"$BACKUP_FILE\"; then\n        error_exit \"Database backup failed\"\n    fi\n\n    # Verify backup size\n    local BACKUP_SIZE=$(stat -c%s \"$BACKUP_FILE\")\n    if [ \"$BACKUP_SIZE\" -lt 1000000 ]; then  # At least 1MB\n        error_exit \"Backup file too small, possible corruption\"\n    fi\n\n    # Compress backup\n    log_message \"Compressing backup\"\n    gzip -\"$COMPRESSION_LEVEL\" \"$BACKUP_FILE\"\n    BACKUP_FILE=\"${BACKUP_FILE}.gz\"\n\n    # Generate checksum\n    log_message \"Generating checksum\"\n    sha256sum \"$BACKUP_FILE\" &gt; \"${BACKUP_FILE}.sha256\"\n\n    # Encrypt backup (if GPG key available)\n    if command -v gpg &gt;/dev/null 2&gt;&amp;1 &amp;&amp; gpg --list-keys \"$ENCRYPT_KEY\" &gt;/dev/null 2&gt;&amp;1; then\n        log_message \"Encrypting backup\"\n        gpg --trust-model always --encrypt -r \"$ENCRYPT_KEY\" \"$BACKUP_FILE\"\n        rm \"$BACKUP_FILE\"\n        BACKUP_FILE=\"${BACKUP_FILE}.gpg\"\n    fi\n\n    # Final verification\n    local FINAL_SIZE=$(stat -c%s \"$BACKUP_FILE\")\n    log_message \"Local backup completed: $BACKUP_FILE ($(numfmt --to=iec $FINAL_SIZE))\"\n\n    # Copy to NAS\n    if [ \"$NAS_ENABLED\" = \"true\" ]; then\n        copy_to_nas \"$BACKUP_FILE\" || log_message \"WARNING: NAS copy failed\"\n    fi\n\n    # Cleanup old local backups\n    log_message \"Cleaning up old local backups (retention: $RETENTION_DAYS days)\"\n    find \"$LOCAL_BACKUP_DIR\" -name \"prs_full_backup_*.sql*\" -mtime +$RETENTION_DAYS -delete\n\n    # Cleanup old NAS backups\n    if [ \"$NAS_ENABLED\" = \"true\" ] &amp;&amp; mountpoint -q \"$NAS_MOUNT_PATH\"; then\n        log_message \"Cleaning up old NAS backups (retention: $NAS_RETENTION_DAYS days)\"\n        find \"$NAS_BACKUP_DIR\" -name \"prs_full_backup_*.sql*\" -mtime +$NAS_RETENTION_DAYS -delete 2&gt;/dev/null || true\n    fi\n\n    # Send notification\n    if command -v mail &gt;/dev/null 2&gt;&amp;1; then\n        local backup_status=\"Local backup completed\"\n        if [ \"$NAS_ENABLED\" = \"true\" ] &amp;&amp; mountpoint -q \"$NAS_MOUNT_PATH\"; then\n            backup_status=\"Local and NAS backup completed\"\n        fi\n\n        echo \"PRS Database backup completed successfully at $(date). Status: $backup_status\" | \\\n        mail -s \"PRS Database Backup Success\" \"${ADMIN_EMAIL:-admin@prs.client-domain.com}\"\n    fi\n\n    # Unmount NAS\n    if [ \"$NAS_ENABLED\" = \"true\" ] &amp;&amp; mountpoint -q \"$NAS_MOUNT_PATH\"; then\n        umount \"$NAS_MOUNT_PATH\" || log_message \"WARNING: Failed to unmount NAS\"\n    fi\n\n    log_message \"Full backup process completed successfully\"\n}\n\n# Execute main function\nmain \"$@\"\n</code></pre>"},{"location":"scripts/backup/#nas-connection-test-script","title":"NAS Connection Test Script","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/test-nas-connection.sh\n# Test NAS connectivity and backup functionality for PRS deployment\n\nset -euo pipefail\n\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" &amp;&amp; pwd)\"\nLOG_FILE=\"/var/log/prs-nas-test.log\"\n\n# Load NAS configuration\nif [ -f \"$SCRIPT_DIR/nas-config.sh\" ]; then\n    source \"$SCRIPT_DIR/nas-config.sh\"\nelse\n    echo \"ERROR: nas-config.sh not found. Please copy nas-config.example.sh to nas-config.sh and configure it.\"\n    exit 1\nfi\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\ntest_nas_connectivity() {\n    log_message \"Testing NAS connectivity\"\n\n    # Test network connectivity\n    if ping -c 3 \"$NAS_HOST\" &gt;/dev/null 2&gt;&amp;1; then\n        log_message \"\u2705 Network connectivity to NAS successful\"\n    else\n        log_message \"\u274c Network connectivity to NAS failed\"\n        return 1\n    fi\n\n    # Test specific ports\n    if [ -n \"$NAS_USERNAME\" ]; then\n        # Test SMB/CIFS ports\n        if timeout 5 bash -c \"&lt;/dev/tcp/$NAS_HOST/445\" 2&gt;/dev/null; then\n            log_message \"\u2705 SMB/CIFS port 445 accessible\"\n        else\n            log_message \"\u274c SMB/CIFS port 445 not accessible\"\n            return 1\n        fi\n    else\n        # Test NFS port\n        if timeout 5 bash -c \"&lt;/dev/tcp/$NAS_HOST/2049\" 2&gt;/dev/null; then\n            log_message \"\u2705 NFS port 2049 accessible\"\n        else\n            log_message \"\u274c NFS port 2049 not accessible\"\n            return 1\n        fi\n    fi\n}\n\ntest_nas_mount() {\n    log_message \"Testing NAS mount functionality\"\n\n    mkdir -p \"$NAS_MOUNT_PATH\"\n\n    if [ -n \"$NAS_USERNAME\" ] &amp;&amp; [ -n \"$NAS_PASSWORD\" ]; then\n        # CIFS/SMB mount\n        if mount -t cifs \"//$NAS_HOST/$NAS_SHARE\" \"$NAS_MOUNT_PATH\" \\\n            -o username=\"$NAS_USERNAME\",password=\"$NAS_PASSWORD\",uid=0,gid=0; then\n            log_message \"\u2705 CIFS/SMB mount successful\"\n        else\n            log_message \"\u274c CIFS/SMB mount failed\"\n            return 1\n        fi\n    else\n        # NFS mount\n        if mount -t nfs \"$NAS_HOST:/$NAS_SHARE\" \"$NAS_MOUNT_PATH\"; then\n            log_message \"\u2705 NFS mount successful\"\n        else\n            log_message \"\u274c NFS mount failed\"\n            return 1\n        fi\n    fi\n}\n\ntest_nas_read_write() {\n    log_message \"Testing NAS read/write functionality\"\n\n    local test_file=\"$NAS_MOUNT_PATH/prs-test-$(date +%s).txt\"\n    local test_data=\"PRS NAS Test - $(date)\"\n\n    # Write test\n    if echo \"$test_data\" &gt; \"$test_file\"; then\n        log_message \"\u2705 NAS file write successful\"\n    else\n        log_message \"\u274c NAS file write failed\"\n        return 1\n    fi\n\n    # Read test\n    if [ \"$(cat \"$test_file\")\" = \"$test_data\" ]; then\n        log_message \"\u2705 NAS file read successful\"\n    else\n        log_message \"\u274c NAS file read failed\"\n        return 1\n    fi\n\n    # Cleanup\n    rm -f \"$test_file\"\n}\n\nmain() {\n    log_message \"Starting NAS connection test\"\n\n    if test_nas_connectivity &amp;&amp; test_nas_mount &amp;&amp; test_nas_read_write; then\n        log_message \"\ud83c\udf89 All NAS tests passed successfully!\"\n\n        # Cleanup\n        if mountpoint -q \"$NAS_MOUNT_PATH\"; then\n            umount \"$NAS_MOUNT_PATH\"\n        fi\n\n        echo \"\u2705 Your NAS is ready for PRS backup integration\"\n    else\n        log_message \"\u274c NAS tests failed\"\n        exit 1\n    fi\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"scripts/backup/#incremental-backup-script","title":"Incremental Backup Script","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/backup-incremental.sh\n# Incremental backup using WAL files\n\nset -euo pipefail\n\nBACKUP_DIR=\"/mnt/hdd/postgres-backups/incremental\"\nWAL_ARCHIVE_DIR=\"/mnt/hdd/wal-archive\"\nRETENTION_DAYS=7\nLOG_FILE=\"/var/log/prs-backup.log\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nmain() {\n    local DATE=$(date +%Y%m%d_%H%M%S)\n    local INCREMENTAL_FILE=\"$BACKUP_DIR/prs_incremental_backup_${DATE}.tar.gz\"\n\n    log_message \"Starting incremental backup\"\n\n    # Create backup directory\n    mkdir -p \"$BACKUP_DIR\"\n\n    # Find WAL files since last full backup\n    local LAST_FULL_BACKUP=$(ls -t /mnt/hdd/postgres-backups/daily/prs_full_backup_*.sql* 2&gt;/dev/null | head -1)\n\n    if [ -z \"$LAST_FULL_BACKUP\" ]; then\n        log_message \"No full backup found, running full backup first\"\n        /opt/prs-deployment/scripts/backup-full.sh\n        return 0\n    fi\n\n    log_message \"Base backup: $LAST_FULL_BACKUP\"\n\n    # Create incremental backup with WAL files newer than last full backup\n    log_message \"Creating incremental backup: $INCREMENTAL_FILE\"\n\n    if find \"$WAL_ARCHIVE_DIR\" -name \"*.wal\" -newer \"$LAST_FULL_BACKUP\" -print0 | \\\n       tar --null -czf \"$INCREMENTAL_FILE\" --files-from=-; then\n        log_message \"Incremental backup completed successfully\"\n    else\n        log_message \"ERROR: Incremental backup failed\"\n        return 1\n    fi\n\n    # Generate checksum\n    sha256sum \"$INCREMENTAL_FILE\" &gt; \"${INCREMENTAL_FILE}.sha256\"\n\n    # Cleanup old incremental backups\n    find \"$BACKUP_DIR\" -name \"prs_incremental_backup_*.tar.gz*\" -mtime +$RETENTION_DAYS -delete\n\n    local BACKUP_SIZE=$(stat -c%s \"$INCREMENTAL_FILE\")\n    log_message \"Incremental backup completed: $INCREMENTAL_FILE ($(numfmt --to=iec $BACKUP_SIZE))\"\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"scripts/backup/#application-data-backup-script","title":"Application Data Backup Script","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/backup-application-data.sh\n# Backup application files and uploads\n\nset -euo pipefail\n\nBACKUP_DIR=\"/mnt/hdd/app-backups\"\nRETENTION_DAYS=14\nLOG_FILE=\"/var/log/prs-backup.log\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nmain() {\n    local DATE=$(date +%Y%m%d_%H%M%S)\n    local APP_BACKUP_DIR=\"$BACKUP_DIR/$DATE\"\n\n    log_message \"Starting application data backup\"\n\n    # Create backup directory\n    mkdir -p \"$APP_BACKUP_DIR\"\n\n    # Backup uploads directory\n    log_message \"Backing up uploads directory\"\n    if [ -d \"/mnt/ssd/uploads\" ]; then\n        tar -czf \"$APP_BACKUP_DIR/uploads.tar.gz\" -C /mnt/ssd uploads/\n        log_message \"Uploads backup completed\"\n    fi\n\n    # Backup configuration\n    log_message \"Backing up configuration\"\n    tar -czf \"$APP_BACKUP_DIR/configuration.tar.gz\" \\\n        -C /opt/prs-deployment 02-docker-configuration/\n\n    # Backup SSL certificates\n    log_message \"Backing up SSL certificates\"\n    if [ -d \"/opt/prs-deployment/02-docker-configuration/ssl\" ]; then\n        tar -czf \"$APP_BACKUP_DIR/ssl-certificates.tar.gz\" \\\n            -C /opt/prs-deployment/02-docker-configuration ssl/\n    fi\n\n    # Backup logs (recent only)\n    log_message \"Backing up recent logs\"\n    find /mnt/ssd/logs -name \"*.log\" -mtime -7 | \\\n    tar -czf \"$APP_BACKUP_DIR/recent-logs.tar.gz\" --files-from=-\n\n    # Generate manifest\n    cat &gt; \"$APP_BACKUP_DIR/manifest.txt\" &lt;&lt; EOF\nPRS Application Data Backup\nDate: $(date)\nHostname: $(hostname)\nBackup Contents:\n- uploads.tar.gz: User uploaded files\n- configuration.tar.gz: Docker configuration\n- ssl-certificates.tar.gz: SSL certificates\n- recent-logs.tar.gz: Application logs (7 days)\n\nBackup Size: $(du -sh \"$APP_BACKUP_DIR\" | cut -f1)\nEOF\n\n    # Cleanup old backups\n    find \"$BACKUP_DIR\" -maxdepth 1 -type d -name \"20*\" -mtime +$RETENTION_DAYS -exec rm -rf {} \\;\n\n    log_message \"Application data backup completed: $APP_BACKUP_DIR\"\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"scripts/backup/#restoration-scripts","title":"Restoration Scripts","text":""},{"location":"scripts/backup/#database-restoration-script","title":"Database Restoration Script","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/restore-database.sh\n# Restore database from backup\n\nset -euo pipefail\n\nBACKUP_FILE=\"$1\"\nTARGET_DB=\"${2:-prs_production}\"\nLOG_FILE=\"/var/log/prs-restore.log\"\n\nif [ -z \"$BACKUP_FILE\" ]; then\n    echo \"Usage: $0 &lt;backup-file&gt; [target-database]\"\n    echo \"Available backups:\"\n    ls -la /mnt/hdd/postgres-backups/daily/\n    exit 1\nfi\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nmain() {\n    log_message \"Starting database restoration\"\n    log_message \"Backup file: $BACKUP_FILE\"\n    log_message \"Target database: $TARGET_DB\"\n\n    # Verify backup file exists\n    if [ ! -f \"$BACKUP_FILE\" ]; then\n        log_message \"ERROR: Backup file not found: $BACKUP_FILE\"\n        exit 1\n    fi\n\n    # Verify checksum if available\n    if [ -f \"${BACKUP_FILE}.sha256\" ]; then\n        log_message \"Verifying backup integrity\"\n        if sha256sum -c \"${BACKUP_FILE}.sha256\"; then\n            log_message \"Backup integrity verified\"\n        else\n            log_message \"ERROR: Backup integrity check failed\"\n            exit 1\n        fi\n    fi\n\n    # Stop application services\n    log_message \"Stopping application services\"\n    docker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml \\\n        stop frontend backend worker\n\n    # Wait for connections to close\n    sleep 10\n\n    # Terminate existing connections\n    log_message \"Terminating existing database connections\"\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -c \"\n    SELECT pg_terminate_backend(pid)\n    FROM pg_stat_activity\n    WHERE datname = '$TARGET_DB' AND pid &lt;&gt; pg_backend_pid();\n    \"\n\n    # Drop and recreate database\n    log_message \"Recreating target database\"\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -c \"DROP DATABASE IF EXISTS $TARGET_DB;\"\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -c \"CREATE DATABASE $TARGET_DB;\"\n\n    # Restore database\n    log_message \"Restoring database from backup\"\n\n    if [[ \"$BACKUP_FILE\" == *.gpg ]]; then\n        # Decrypt and restore\n        log_message \"Decrypting and restoring encrypted backup\"\n        gpg --quiet --decrypt \"$BACKUP_FILE\" | \\\n        gunzip | \\\n        docker exec -i prs-onprem-postgres-timescale pg_restore \\\n            -U prs_admin -d \"$TARGET_DB\" --clean --if-exists --verbose\n    elif [[ \"$BACKUP_FILE\" == *.gz ]]; then\n        # Decompress and restore\n        log_message \"Decompressing and restoring backup\"\n        gunzip -c \"$BACKUP_FILE\" | \\\n        docker exec -i prs-onprem-postgres-timescale pg_restore \\\n            -U prs_admin -d \"$TARGET_DB\" --clean --if-exists --verbose\n    else\n        # Direct restore\n        log_message \"Restoring uncompressed backup\"\n        docker exec -i prs-onprem-postgres-timescale pg_restore \\\n            -U prs_admin -d \"$TARGET_DB\" --clean --if-exists --verbose &lt; \"$BACKUP_FILE\"\n    fi\n\n    if [ $? -eq 0 ]; then\n        log_message \"Database restoration completed successfully\"\n    else\n        log_message \"ERROR: Database restoration failed\"\n        exit 1\n    fi\n\n    # Verify restoration\n    log_message \"Verifying restoration\"\n    TABLE_COUNT=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d \"$TARGET_DB\" -t -c \"\n    SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';\n    \" | xargs)\n\n    log_message \"Restored $TABLE_COUNT tables\"\n\n    if [ \"$TABLE_COUNT\" -gt 10 ]; then\n        log_message \"Restoration verification passed\"\n    else\n        log_message \"WARNING: Low table count, possible incomplete restoration\"\n    fi\n\n    # Restart application services\n    log_message \"Restarting application services\"\n    docker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml \\\n        start frontend backend worker\n\n    log_message \"Database restoration process completed\"\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"scripts/backup/#point-in-time-recovery-script","title":"Point-in-Time Recovery Script","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/restore-point-in-time.sh\n# Point-in-time recovery using WAL files\n\nset -euo pipefail\n\nRECOVERY_TIME=\"$1\"\nRECOVERY_DIR=\"/tmp/prs-recovery-$(date +%Y%m%d_%H%M%S)\"\nLOG_FILE=\"/var/log/prs-restore.log\"\n\nif [ -z \"$RECOVERY_TIME\" ]; then\n    echo \"Usage: $0 'YYYY-MM-DD HH:MM:SS'\"\n    echo \"Example: $0 '2024-08-22 14:30:00'\"\n    exit 1\nfi\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nmain() {\n    log_message \"Starting point-in-time recovery to: $RECOVERY_TIME\"\n\n    # Create recovery directory\n    mkdir -p \"$RECOVERY_DIR\"\n\n    # Find appropriate base backup\n    local RECOVERY_TIMESTAMP=$(date -d \"$RECOVERY_TIME\" +%s)\n    local BASE_BACKUP=\"\"\n\n    for backup in $(ls -t /mnt/hdd/postgres-backups/daily/prs_full_backup_*.sql*); do\n        local BACKUP_DATE=$(echo \"$backup\" | grep -o '[0-9]\\{8\\}_[0-9]\\{6\\}')\n        local BACKUP_TIMESTAMP=$(date -d \"${BACKUP_DATE:0:8} ${BACKUP_DATE:9:2}:${BACKUP_DATE:11:2}:${BACKUP_DATE:13:2}\" +%s)\n\n        if [ \"$BACKUP_TIMESTAMP\" -le \"$RECOVERY_TIMESTAMP\" ]; then\n            BASE_BACKUP=\"$backup\"\n            break\n        fi\n    done\n\n    if [ -z \"$BASE_BACKUP\" ]; then\n        log_message \"ERROR: No suitable base backup found for recovery time\"\n        exit 1\n    fi\n\n    log_message \"Using base backup: $BASE_BACKUP\"\n\n    # Copy base backup\n    cp \"$BASE_BACKUP\" \"$RECOVERY_DIR/\"\n\n    # Collect required WAL files\n    log_message \"Collecting WAL files for recovery\"\n    find /mnt/hdd/wal-archive -name \"*.wal\" -newer \"$BASE_BACKUP\" -exec cp {} \"$RECOVERY_DIR/\" \\;\n\n    # Stop current database\n    log_message \"Stopping current database\"\n    docker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml stop postgres\n\n    # Backup current data directory\n    log_message \"Backing up current data directory\"\n    mv /mnt/ssd/postgresql-hot /mnt/ssd/postgresql-hot.backup.$(date +%Y%m%d_%H%M%S)\n\n    # Create new data directory\n    mkdir -p /mnt/ssd/postgresql-hot\n    chown 999:999 /mnt/ssd/postgresql-hot\n\n    # Restore base backup\n    log_message \"Restoring base backup\"\n    # ... (restoration logic similar to restore-database.sh)\n\n    # Configure recovery\n    cat &gt; /tmp/recovery.conf &lt;&lt; EOF\nrestore_command = 'cp $RECOVERY_DIR/%f %p'\nrecovery_target_time = '$RECOVERY_TIME'\nrecovery_target_action = 'promote'\nEOF\n\n    cp /tmp/recovery.conf /mnt/ssd/postgresql-hot/\n\n    # Start database in recovery mode\n    log_message \"Starting database in recovery mode\"\n    docker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml up -d postgres\n\n    # Monitor recovery\n    log_message \"Monitoring recovery progress\"\n    while docker exec prs-onprem-postgres-timescale test -f /var/lib/postgresql/data/recovery.conf; do\n        log_message \"Recovery in progress...\"\n        sleep 10\n    done\n\n    log_message \"Point-in-time recovery completed to: $RECOVERY_TIME\"\n\n    # Cleanup\n    rm -rf \"$RECOVERY_DIR\"\n    rm -f /tmp/recovery.conf\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"scripts/backup/#backup-management-scripts","title":"Backup Management Scripts","text":""},{"location":"scripts/backup/#backup-verification-script","title":"Backup Verification Script","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/verify-backups.sh\n# Verify backup integrity and completeness\n\nset -euo pipefail\n\nBACKUP_DIR=\"/mnt/hdd/postgres-backups/daily\"\nLOG_FILE=\"/var/log/prs-backup-verification.log\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nmain() {\n    log_message \"Starting backup verification\"\n\n    local VERIFIED=0\n    local FAILED=0\n\n    # Check all backups from last 7 days\n    for backup in $(find \"$BACKUP_DIR\" -name \"prs_full_backup_*.sql*\" -mtime -7); do\n        log_message \"Verifying backup: $backup\"\n\n        # Check file size\n        local SIZE=$(stat -c%s \"$backup\")\n        if [ \"$SIZE\" -lt 1000000 ]; then\n            log_message \"ERROR: Backup file too small: $backup\"\n            ((FAILED++))\n            continue\n        fi\n\n        # Check checksum if available\n        if [ -f \"${backup}.sha256\" ]; then\n            if sha256sum -c \"${backup}.sha256\" &gt;/dev/null 2&gt;&amp;1; then\n                log_message \"Checksum verification passed: $backup\"\n            else\n                log_message \"ERROR: Checksum verification failed: $backup\"\n                ((FAILED++))\n                continue\n            fi\n        fi\n\n        # Test restoration to temporary database\n        log_message \"Testing restoration: $backup\"\n        if test_restore \"$backup\"; then\n            log_message \"Restoration test passed: $backup\"\n            ((VERIFIED++))\n        else\n            log_message \"ERROR: Restoration test failed: $backup\"\n            ((FAILED++))\n        fi\n    done\n\n    log_message \"Backup verification completed: $VERIFIED verified, $FAILED failed\"\n\n    if [ \"$FAILED\" -gt 0 ]; then\n        echo \"Backup verification found $FAILED failed backups\" | \\\n        mail -s \"PRS Backup Verification Alert\" admin@your-domain.com\n    fi\n}\n\ntest_restore() {\n    local backup_file=\"$1\"\n    local test_db=\"prs_backup_test_$(date +%s)\"\n\n    # Create test database\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -c \"CREATE DATABASE $test_db;\" &gt;/dev/null 2&gt;&amp;1\n\n    # Attempt restoration\n    if [[ \"$backup_file\" == *.gpg ]]; then\n        gpg --quiet --decrypt \"$backup_file\" | gunzip | \\\n        docker exec -i prs-onprem-postgres-timescale pg_restore -U prs_admin -d \"$test_db\" &gt;/dev/null 2&gt;&amp;1\n    elif [[ \"$backup_file\" == *.gz ]]; then\n        gunzip -c \"$backup_file\" | \\\n        docker exec -i prs-onprem-postgres-timescale pg_restore -U prs_admin -d \"$test_db\" &gt;/dev/null 2&gt;&amp;1\n    else\n        docker exec -i prs-onprem-postgres-timescale pg_restore -U prs_admin -d \"$test_db\" &lt; \"$backup_file\" &gt;/dev/null 2&gt;&amp;1\n    fi\n\n    local result=$?\n\n    # Cleanup test database\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -c \"DROP DATABASE $test_db;\" &gt;/dev/null 2&gt;&amp;1\n\n    return $result\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"scripts/backup/#backup-cleanup-script","title":"Backup Cleanup Script","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/cleanup-backups.sh\n# Clean up old backups based on retention policies\n\nset -euo pipefail\n\nLOG_FILE=\"/var/log/prs-backup-cleanup.log\"\n\n# Retention policies (days)\nDAILY_RETENTION=30\nINCREMENTAL_RETENTION=7\nAPP_BACKUP_RETENTION=14\nWAL_RETENTION=7\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nmain() {\n    log_message \"Starting backup cleanup\"\n\n    # Clean daily backups\n    log_message \"Cleaning daily backups (retention: $DAILY_RETENTION days)\"\n    DELETED=$(find /mnt/hdd/postgres-backups/daily -name \"prs_full_backup_*\" -mtime +$DAILY_RETENTION -delete -print | wc -l)\n    log_message \"Deleted $DELETED old daily backups\"\n\n    # Clean incremental backups\n    log_message \"Cleaning incremental backups (retention: $INCREMENTAL_RETENTION days)\"\n    DELETED=$(find /mnt/hdd/postgres-backups/incremental -name \"prs_incremental_backup_*\" -mtime +$INCREMENTAL_RETENTION -delete -print | wc -l)\n    log_message \"Deleted $DELETED old incremental backups\"\n\n    # Clean application backups\n    log_message \"Cleaning application backups (retention: $APP_BACKUP_RETENTION days)\"\n    DELETED=$(find /mnt/hdd/app-backups -maxdepth 1 -type d -name \"20*\" -mtime +$APP_BACKUP_RETENTION -exec rm -rf {} \\; -print | wc -l)\n    log_message \"Deleted $DELETED old application backup directories\"\n\n    # Clean WAL archives\n    log_message \"Cleaning WAL archives (retention: $WAL_RETENTION days)\"\n    DELETED=$(find /mnt/hdd/wal-archive -name \"*.wal\" -mtime +$WAL_RETENTION -delete -print | wc -l)\n    log_message \"Deleted $DELETED old WAL files\"\n\n    # Report storage savings\n    log_message \"Backup cleanup completed\"\n\n    # Check remaining storage\n    BACKUP_USAGE=$(du -sh /mnt/hdd/postgres-backups | cut -f1)\n    WAL_USAGE=$(du -sh /mnt/hdd/wal-archive | cut -f1)\n    APP_USAGE=$(du -sh /mnt/hdd/app-backups | cut -f1)\n\n    log_message \"Current backup storage usage:\"\n    log_message \"  Database backups: $BACKUP_USAGE\"\n    log_message \"  WAL archives: $WAL_USAGE\"\n    log_message \"  Application backups: $APP_USAGE\"\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"scripts/backup/#nas-setup-and-configuration-guide","title":"NAS Setup and Configuration Guide","text":""},{"location":"scripts/backup/#supported-nas-systems","title":"Supported NAS Systems","text":"<p>The PRS backup system supports the following NAS systems:</p> NAS System Protocol Authentication Notes Synology DSM CIFS/SMB Username/Password Recommended for small-medium deployments QNAP QTS CIFS/SMB Username/Password Enterprise features available FreeNAS/TrueNAS NFS None (IP-based) High performance, enterprise-grade Windows Server CIFS/SMB Domain/Local Active Directory integration Linux NFS NFS None (IP-based) Lightweight, high performance"},{"location":"scripts/backup/#nas-configuration-examples","title":"NAS Configuration Examples","text":""},{"location":"scripts/backup/#synology-nas-setup","title":"Synology NAS Setup","text":"<ol> <li>Create Backup User:</li> <li>Control Panel &gt; User &gt; Create &gt; backup_user</li> <li> <p>Assign to \"administrators\" group for backup access</p> </li> <li> <p>Create Backup Share:</p> </li> <li>Control Panel &gt; Shared Folder &gt; Create &gt; \"backup\"</li> <li> <p>Enable SMB/AFP/NFS service</p> </li> <li> <p>Configure PRS: <pre><code>export NAS_HOST=\"synology.local\"\nexport NAS_SHARE=\"backup\"\nexport NAS_USERNAME=\"backup_user\"\nexport NAS_PASSWORD=\"secure_password\"\n</code></pre></p> </li> </ol>"},{"location":"scripts/backup/#qnap-nas-setup","title":"QNAP NAS Setup","text":"<ol> <li>Enable File Services:</li> <li>Control Panel &gt; Network &amp; File Services &gt; SMB/CIFS</li> <li> <p>Enable SMB service</p> </li> <li> <p>Create Backup Folder:</p> </li> <li>File Station &gt; Create Folder &gt; \"Backup\"</li> <li> <p>Set permissions for backup user</p> </li> <li> <p>Configure PRS: <pre><code>export NAS_HOST=\"qnap.local\"\nexport NAS_SHARE=\"Backup\"\nexport NAS_USERNAME=\"admin\"\nexport NAS_PASSWORD=\"qnap_password\"\n</code></pre></p> </li> </ol>"},{"location":"scripts/backup/#freenastruenas-setup","title":"FreeNAS/TrueNAS Setup","text":"<ol> <li>Create Dataset:</li> <li> <p>Storage &gt; Pools &gt; Add Dataset &gt; \"backups\"</p> </li> <li> <p>Configure NFS Share:</p> </li> <li>Sharing &gt; Unix (NFS) Shares &gt; Add</li> <li>Path: /mnt/pool1/backups</li> <li> <p>Authorized Networks: 192.168.1.0/24</p> </li> <li> <p>Configure PRS: <pre><code>export NAS_HOST=\"truenas.local\"\nexport NAS_SHARE=\"/mnt/pool1/backups\"\n# No username/password for NFS\n</code></pre></p> </li> </ol>"},{"location":"scripts/backup/#nas-security-best-practices","title":"NAS Security Best Practices","text":""},{"location":"scripts/backup/#1-network-security","title":"1. Network Security","text":"<ul> <li>Use dedicated VLAN for backup traffic</li> <li>Configure firewall rules to restrict NAS access</li> <li>Enable NAS firewall and limit IP ranges</li> </ul>"},{"location":"scripts/backup/#2-authentication-security","title":"2. Authentication Security","text":"<ul> <li>Use strong passwords (minimum 12 characters)</li> <li>Create dedicated backup user accounts</li> <li>Regularly rotate backup credentials</li> <li>Enable two-factor authentication where available</li> </ul>"},{"location":"scripts/backup/#3-data-security","title":"3. Data Security","text":"<pre><code># Enable backup encryption\nexport ENCRYPT_NAS_BACKUPS=\"true\"\nexport GPG_RECIPIENT=\"backup@prs.client-domain.com\"\n\n# Use secure file permissions\nexport NAS_FILE_MODE=\"0600\"\nexport NAS_DIR_MODE=\"0700\"\n</code></pre>"},{"location":"scripts/backup/#troubleshooting-nas-issues","title":"Troubleshooting NAS Issues","text":""},{"location":"scripts/backup/#common-connection-issues","title":"Common Connection Issues","text":"<p>Problem: \"Network connectivity to NAS failed\" <pre><code># Check network connectivity\nping your-nas-hostname\n\n# Check DNS resolution\nnslookup your-nas-hostname\n\n# Check routing\ntraceroute your-nas-hostname\n</code></pre></p> <p>Problem: \"SMB/CIFS port 445 not accessible\" <pre><code># Check if SMB service is running on NAS\ntelnet your-nas-hostname 445\n\n# Check firewall rules\niptables -L | grep 445\n\n# Verify SMB service on NAS is enabled\n</code></pre></p> <p>Problem: \"NFS port 2049 not accessible\" <pre><code># Check NFS service\nrpcinfo -p your-nas-hostname\n\n# Test NFS mount manually\nshowmount -e your-nas-hostname\n</code></pre></p>"},{"location":"scripts/backup/#authentication-issues","title":"Authentication Issues","text":"<p>Problem: \"CIFS/SMB mount failed\" <pre><code># Test credentials manually\nsmbclient //your-nas-hostname/share -U username\n\n# Check SMB version compatibility\nmount -t cifs //nas/share /mnt/test -o username=user,vers=3.0\n\n# Verify user permissions on NAS\n</code></pre></p>"},{"location":"scripts/backup/#performance-issues","title":"Performance Issues","text":"<p>Problem: \"Backup copy to NAS is slow\" <pre><code># Test network bandwidth\niperf3 -c your-nas-hostname\n\n# Check NAS performance\ndd if=/dev/zero of=/mnt/nas/test bs=1M count=1000\n\n# Monitor network utilization\niftop -i eth0\n</code></pre></p>"},{"location":"scripts/backup/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"scripts/backup/#nas-health-monitoring","title":"NAS Health Monitoring","text":"<pre><code># Add to crontab for regular NAS health checks\n0 */6 * * * /opt/prs-deployment/scripts/test-nas-connection.sh\n\n# Monitor NAS space usage\n*/15 * * * * df -h /mnt/nas | awk 'NR==2 {if($5+0 &gt; 85) print \"NAS space warning: \"$5\" used\"}'\n</code></pre>"},{"location":"scripts/backup/#email-alerts","title":"Email Alerts","text":"<pre><code># Configure email alerts for NAS issues\nexport ADMIN_EMAIL=\"admin@prs.client-domain.com\"\nexport NAS_SPACE_WARNING_THRESHOLD=85\nexport NAS_SPACE_CRITICAL_THRESHOLD=95\n</code></pre> <p>Backup Scripts with NAS Integration Ready</p> <p>Your PRS deployment now has comprehensive backup scripts with enterprise-grade NAS integration, covering database backups, application data, incremental backups, and restoration procedures.</p> <p>NAS Automation</p> <p>Use the provided NAS configuration and testing scripts to ensure reliable off-site backup storage. Regular testing of NAS connectivity is recommended.</p> <p>Security &amp; Compliance</p> <p>Always encrypt sensitive backup data, use secure NAS credentials, and test restoration procedures regularly. Implement proper network security for NAS access.</p>"},{"location":"scripts/deployment/","title":"Deployment Scripts","text":""},{"location":"scripts/deployment/#overview","title":"Overview","text":"<p>This guide documents all deployment scripts used in the PRS on-premises deployment, including their purpose, usage, and configuration options.</p>"},{"location":"scripts/deployment/#scripts-directory-structure","title":"Scripts Directory Structure","text":"<pre><code>scripts/\n\u251c\u2500\u2500 deploy-onprem.sh              # Main deployment script\n\u251c\u2500\u2500 setup-env.sh                  # Environment setup\n\u251c\u2500\u2500 setup-storage.sh              # Storage configuration\n\u251c\u2500\u2500 ssl-automation-citylandcondo.sh # SSL certificate automation\n\u251c\u2500\u2500 backup-maintenance.sh         # Backup procedures with NAS integration\n\u251c\u2500\u2500 backup-full.sh                # Full database backup with NAS support\n\u251c\u2500\u2500 backup-application-data.sh    # Application backup with NAS support\n\u251c\u2500\u2500 system-health-check.sh        # Health monitoring\n\u251c\u2500\u2500 security-hardening-check.sh   # Security validation\n\u251c\u2500\u2500 repo-config.sh                # Repository configuration\n\u251c\u2500\u2500 nas-config.example.sh         # NAS configuration template\n\u251c\u2500\u2500 test-nas-connection.sh        # NAS connectivity testing\n\u251c\u2500\u2500 restore-point-in-time.sh      # Point-in-time recovery\n\u2514\u2500\u2500 utils/                        # Utility scripts\n    \u251c\u2500\u2500 docker-cleanup.sh         # Docker maintenance\n    \u251c\u2500\u2500 log-rotation.sh           # Log management\n    \u2514\u2500\u2500 performance-test.sh       # Performance testing\n</code></pre>"},{"location":"scripts/deployment/#main-deployment-script","title":"Main Deployment Script","text":""},{"location":"scripts/deployment/#deploy-onpremsh","title":"deploy-onprem.sh","text":"<p>Purpose: Complete deployment automation for PRS on-premises environment</p> <p>Usage: <pre><code>./deploy-onprem.sh [environment] [options]\n\n# Examples:\n./deploy-onprem.sh prod                    # Deploy production\n./deploy-onprem.sh staging                 # Deploy staging\n./deploy-onprem.sh prod --skip-build       # Skip image building\n./deploy-onprem.sh prod --force-rebuild    # Force rebuild all images\n</code></pre></p> <p>Script Overview: <pre><code>#!/bin/bash\nset -e\n\n# Configuration\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" &amp;&amp; pwd)\"\nPROJECT_ROOT=\"$(dirname \"$SCRIPT_DIR\")\"\nENVIRONMENT=\"${1:-prod}\"\n\n# Colors for output\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nBLUE='\\033[0;34m'\nNC='\\033[0m'\n\n# Logging functions\nlog_info() { echo -e \"${BLUE}[INFO]${NC} $1\"; }\nlog_success() { echo -e \"${GREEN}[SUCCESS]${NC} $1\"; }\nlog_warning() { echo -e \"${YELLOW}[WARNING]${NC} $1\"; }\nlog_error() { echo -e \"${RED}[ERROR]${NC} $1\"; }\n\n# Main deployment function\nmain() {\n    log_info \"Starting PRS deployment for environment: $ENVIRONMENT\"\n\n    # Validate environment\n    validate_environment\n\n    # Setup repositories\n    setup_repositories\n\n    # Build Docker images\n    build_images\n\n    # Deploy services\n    deploy_services\n\n    # Configure database\n    setup_database\n\n    # Run health checks\n    health_checks\n\n    log_success \"Deployment completed successfully!\"\n}\n\n# Environment validation\nvalidate_environment() {\n    log_info \"Validating deployment environment...\"\n\n    # Check Docker\n    if ! command -v docker &amp;&gt; /dev/null; then\n        log_error \"Docker is not installed\"\n        exit 1\n    fi\n\n    # Check Docker Compose\n    if ! command -v docker-compose &amp;&gt; /dev/null; then\n        log_error \"Docker Compose is not installed\"\n        exit 1\n    fi\n\n    # Check environment file\n    if [ ! -f \"$PROJECT_ROOT/02-docker-configuration/.env\" ]; then\n        log_error \"Environment file not found. Run setup-env.sh first.\"\n        exit 1\n    fi\n\n    # Check storage\n    if [ ! -d \"/mnt/ssd\" ] || [ ! -d \"/mnt/hdd\" ]; then\n        log_error \"Storage not configured. Run setup-storage.sh first.\"\n        exit 1\n    fi\n\n    log_success \"Environment validation passed\"\n}\n\n# Repository setup\nsetup_repositories() {\n    log_info \"Setting up application repositories...\"\n\n    # Source repository configuration\n    source \"$SCRIPT_DIR/repo-config.sh\"\n\n    # Create repositories directory\n    mkdir -p \"$REPOS_BASE_DIR\"\n    cd \"$REPOS_BASE_DIR\"\n\n    # Clone or update backend repository\n    if [ ! -d \"$BACKEND_REPO_NAME\" ]; then\n        log_info \"Cloning backend repository...\"\n        git clone \"$BACKEND_REPO_URL\" \"$BACKEND_REPO_NAME\"\n    else\n        log_info \"Updating backend repository...\"\n        cd \"$BACKEND_REPO_NAME\"\n        git fetch origin\n        git reset --hard \"origin/$BACKEND_BRANCH\"\n        cd ..\n    fi\n\n    # Clone or update frontend repository\n    if [ ! -d \"$FRONTEND_REPO_NAME\" ]; then\n        log_info \"Cloning frontend repository...\"\n        git clone \"$FRONTEND_REPO_URL\" \"$FRONTEND_REPO_NAME\"\n    else\n        log_info \"Updating frontend repository...\"\n        cd \"$FRONTEND_REPO_NAME\"\n        git fetch origin\n        git reset --hard \"origin/$FRONTEND_BRANCH\"\n        cd ..\n    fi\n\n    log_success \"Repository setup completed\"\n}\n\n# Docker image building\nbuild_images() {\n    log_info \"Building Docker images...\"\n\n    cd \"$PROJECT_ROOT/02-docker-configuration\"\n\n    # Build backend image\n    log_info \"Building backend image...\"\n    docker build -t prs-backend:latest \\\n        -f ../dockerfiles/Dockerfile.backend \\\n        \"$REPOS_BASE_DIR/$BACKEND_REPO_NAME\"\n\n    # Build frontend image\n    log_info \"Building frontend image...\"\n    docker build -t prs-frontend:latest \\\n        -f ../dockerfiles/Dockerfile.frontend \\\n        \"$REPOS_BASE_DIR/$FRONTEND_REPO_NAME\"\n\n    log_success \"Docker images built successfully\"\n}\n\n# Service deployment\ndeploy_services() {\n    log_info \"Deploying services...\"\n\n    cd \"$PROJECT_ROOT/02-docker-configuration\"\n\n    # Stop existing services\n    docker-compose -f docker-compose.onprem.yml down\n\n    # Start services\n    docker-compose -f docker-compose.onprem.yml up -d\n\n    # Wait for services to be ready\n    log_info \"Waiting for services to start...\"\n    sleep 30\n\n    log_success \"Services deployed successfully\"\n}\n\n# Database setup\nsetup_database() {\n    log_info \"Setting up database...\"\n\n    # Wait for PostgreSQL to be ready\n    while ! docker exec prs-onprem-postgres-timescale pg_isready -U prs_admin; do\n        log_info \"Waiting for PostgreSQL to be ready...\"\n        sleep 5\n    done\n\n    # Create tablespaces\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n        CREATE TABLESPACE IF NOT EXISTS ssd_hot LOCATION '/mnt/ssd/postgresql-hot';\n        CREATE TABLESPACE IF NOT EXISTS hdd_cold LOCATION '/mnt/hdd/postgresql-cold';\n    \"\n\n    # Run migrations\n    docker exec prs-onprem-backend npm run migrate\n\n    # Setup TimescaleDB\n    docker exec prs-onprem-backend npm run setup:timescaledb\n\n    log_success \"Database setup completed\"\n}\n\n# Health checks\nhealth_checks() {\n    log_info \"Running health checks...\"\n\n    # Check service status\n    docker-compose -f \"$PROJECT_ROOT/02-docker-configuration/docker-compose.onprem.yml\" ps\n\n    # Test API endpoint\n    if curl -f -s https://localhost/api/health &gt; /dev/null; then\n        log_success \"API health check passed\"\n    else\n        log_warning \"API health check failed\"\n    fi\n\n    # Test database connectivity\n    if docker exec prs-onprem-postgres-timescale pg_isready -U prs_admin; then\n        log_success \"Database health check passed\"\n    else\n        log_error \"Database health check failed\"\n        exit 1\n    fi\n\n    log_success \"Health checks completed\"\n}\n\n# Parse command line arguments\nwhile [[ $# -gt 0 ]]; do\n    case $1 in\n        --skip-build)\n            SKIP_BUILD=true\n            shift\n            ;;\n        --force-rebuild)\n            FORCE_REBUILD=true\n            shift\n            ;;\n        --help)\n            show_help\n            exit 0\n            ;;\n        *)\n            shift\n            ;;\n    esac\ndone\n\n# Run main function\nmain \"$@\"\n</code></pre></p>"},{"location":"scripts/deployment/#environment-setup-script","title":"Environment Setup Script","text":""},{"location":"scripts/deployment/#setup-envsh","title":"setup-env.sh","text":"<p>Purpose: Automated environment configuration and secret generation</p> <p>Usage: <pre><code>./setup-env.sh [--interactive] [--force]\n</code></pre></p> <p>Key Features: - Generates secure passwords and secrets - Creates environment file from template - Validates configuration - Interactive mode for custom settings</p> <pre><code>#!/bin/bash\nset -e\n\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" &amp;&amp; pwd)\"\nPROJECT_ROOT=\"$(dirname \"$SCRIPT_DIR\")\"\nENV_FILE=\"$PROJECT_ROOT/02-docker-configuration/.env\"\nENV_TEMPLATE=\"$PROJECT_ROOT/02-docker-configuration/.env.example\"\n\n# Generate secure environment\ngenerate_environment() {\n    log_info \"Generating secure environment configuration...\"\n\n    # Generate secure passwords\n    POSTGRES_PASSWORD=$(openssl rand -base64 32)\n    REDIS_PASSWORD=$(openssl rand -base64 32)\n    JWT_SECRET=$(openssl rand -base64 32)\n    ENCRYPTION_KEY=$(openssl rand -base64 32)\n    OTP_KEY=$(openssl rand -base64 16)\n    GRAFANA_ADMIN_PASSWORD=$(openssl rand -base64 16)\n\n    # Copy template and replace placeholders\n    cp \"$ENV_TEMPLATE\" \"$ENV_FILE\"\n\n    # Replace generated values\n    sed -i \"s/POSTGRES_PASSWORD=.*/POSTGRES_PASSWORD=$POSTGRES_PASSWORD/\" \"$ENV_FILE\"\n    sed -i \"s/REDIS_PASSWORD=.*/REDIS_PASSWORD=$REDIS_PASSWORD/\" \"$ENV_FILE\"\n    sed -i \"s/JWT_SECRET=.*/JWT_SECRET=$JWT_SECRET/\" \"$ENV_FILE\"\n    sed -i \"s/ENCRYPTION_KEY=.*/ENCRYPTION_KEY=$ENCRYPTION_KEY/\" \"$ENV_FILE\"\n    sed -i \"s/OTP_KEY=.*/OTP_KEY=$OTP_KEY/\" \"$ENV_FILE\"\n    sed -i \"s/GRAFANA_ADMIN_PASSWORD=.*/GRAFANA_ADMIN_PASSWORD=$GRAFANA_ADMIN_PASSWORD/\" \"$ENV_FILE\"\n\n    log_success \"Environment file generated: $ENV_FILE\"\n}\n\n# Interactive configuration\ninteractive_setup() {\n    echo \"=== PRS Environment Setup ===\"\n    echo \"\"\n\n    read -p \"Enter domain name [your-domain.com]: \" DOMAIN\n    DOMAIN=${DOMAIN:-your-domain.com}\n\n    read -p \"Enter server IP [192.168.0.100]: \" SERVER_IP\n    SERVER_IP=${SERVER_IP:-192.168.0.100}\n\n    read -p \"Enter SSL email [admin@$DOMAIN]: \" SSL_EMAIL\n    SSL_EMAIL=${SSL_EMAIL:-admin@$DOMAIN}\n\n    # Update environment file\n    sed -i \"s/DOMAIN=.*/DOMAIN=$DOMAIN/\" \"$ENV_FILE\"\n    sed -i \"s/SERVER_IP=.*/SERVER_IP=$SERVER_IP/\" \"$ENV_FILE\"\n    sed -i \"s/SSL_EMAIL=.*/SSL_EMAIL=$SSL_EMAIL/\" \"$ENV_FILE\"\n\n    echo \"\"\n    echo \"Configuration updated successfully!\"\n}\n</code></pre>"},{"location":"scripts/deployment/#storage-setup-script","title":"Storage Setup Script","text":""},{"location":"scripts/deployment/#setup-storagesh","title":"setup-storage.sh","text":"<p>Purpose: Configure dual storage architecture (SSD/HDD)</p> <p>Usage: <pre><code>sudo ./setup-storage.sh [--verify-only]\n</code></pre></p> <p>Key Features: - Creates storage directory structure - Sets proper permissions - Configures ownership for services - Validates storage configuration</p> <pre><code>#!/bin/bash\nset -e\n\n# Storage configuration\nSSD_MOUNT=\"/mnt/ssd\"\nHDD_MOUNT=\"/mnt/hdd\"\n\n# Create storage directories\ncreate_directories() {\n    log_info \"Creating storage directory structure...\"\n\n    # SSD directories\n    mkdir -p \"$SSD_MOUNT\"/{postgresql-hot,redis-data,uploads,logs,nginx-cache,prometheus-data,grafana-data,portainer-data}\n\n    # HDD directories\n    mkdir -p \"$HDD_MOUNT\"/{postgresql-cold,postgres-backups,app-logs-archive,redis-backups,prometheus-archive,config-backups}\n\n    log_success \"Storage directories created\"\n}\n\n# Set permissions\nset_permissions() {\n    log_info \"Setting storage permissions...\"\n\n    # PostgreSQL (UID 999)\n    chown -R 999:999 \"$SSD_MOUNT/postgresql-hot\" \"$HDD_MOUNT/postgresql-cold\" \"$HDD_MOUNT/postgres-backups\"\n    chmod 700 \"$SSD_MOUNT/postgresql-hot\" \"$HDD_MOUNT/postgresql-cold\"\n\n    # Redis (UID 999)\n    chown -R 999:999 \"$SSD_MOUNT/redis-data\" \"$HDD_MOUNT/redis-backups\"\n    chmod 755 \"$SSD_MOUNT/redis-data\"\n\n    # Grafana (UID 472)\n    chown -R 472:472 \"$SSD_MOUNT/grafana-data\"\n\n    # Prometheus (UID 65534)\n    chown -R 65534:65534 \"$SSD_MOUNT/prometheus-data\" \"$HDD_MOUNT/prometheus-archive\"\n\n    # Nginx/Application (UID 33 or 1000)\n    chown -R www-data:www-data \"$SSD_MOUNT/nginx-cache\" \"$SSD_MOUNT/uploads\"\n    chown -R 1000:1000 \"$SSD_MOUNT/logs\" \"$HDD_MOUNT/app-logs-archive\"\n\n    # General permissions\n    chmod 755 \"$SSD_MOUNT\"/* \"$HDD_MOUNT\"/*\n\n    log_success \"Permissions set successfully\"\n}\n\n# Verify storage\nverify_storage() {\n    log_info \"Verifying storage configuration...\"\n\n    # Check mount points\n    if ! mountpoint -q \"$SSD_MOUNT\"; then\n        log_warning \"SSD not mounted at $SSD_MOUNT\"\n    fi\n\n    if ! mountpoint -q \"$HDD_MOUNT\"; then\n        log_warning \"HDD not mounted at $HDD_MOUNT\"\n    fi\n\n    # Check directory structure\n    for dir in postgresql-hot redis-data uploads logs; do\n        if [ ! -d \"$SSD_MOUNT/$dir\" ]; then\n            log_error \"Missing SSD directory: $dir\"\n            exit 1\n        fi\n    done\n\n    for dir in postgresql-cold postgres-backups app-logs-archive; do\n        if [ ! -d \"$HDD_MOUNT/$dir\" ]; then\n            log_error \"Missing HDD directory: $dir\"\n            exit 1\n        fi\n    done\n\n    log_success \"Storage verification passed\"\n}\n</code></pre>"},{"location":"scripts/deployment/#ssl-automation-script","title":"SSL Automation Script","text":""},{"location":"scripts/deployment/#ssl-automation-citylandcondosh","title":"ssl-automation-citylandcondo.sh","text":"<p>Purpose: Automated SSL certificate management</p> <p>Usage: <pre><code>./ssl-automation-citylandcondo.sh [--force] [--staging]\n</code></pre></p> <p>Key Features: - Let's Encrypt integration - Certificate renewal automation - Nginx configuration update - Certificate validation</p> <pre><code>#!/bin/bash\nset -e\n\nDOMAIN=\"${DOMAIN:-your-domain.com}\"\nSSL_DIR=\"/opt/prs-deployment/02-docker-configuration/ssl\"\nEMAIL=\"${SSL_EMAIL:-admin@$DOMAIN}\"\n\n# Certificate generation\ngenerate_certificate() {\n    log_info \"Generating SSL certificate for $DOMAIN...\"\n\n    # Stop nginx temporarily\n    docker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml stop nginx\n\n    # Generate certificate\n    certbot certonly \\\n        --standalone \\\n        --email \"$EMAIL\" \\\n        --agree-tos \\\n        --no-eff-email \\\n        --domains \"$DOMAIN\" \\\n        --cert-path \"$SSL_DIR/certificate.crt\" \\\n        --key-path \"$SSL_DIR/private.key\" \\\n        --chain-path \"$SSL_DIR/ca-bundle.crt\"\n\n    # Restart nginx\n    docker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml start nginx\n\n    log_success \"SSL certificate generated successfully\"\n}\n\n# Certificate renewal\nrenew_certificate() {\n    log_info \"Renewing SSL certificate...\"\n\n    certbot renew --quiet\n\n    # Reload nginx\n    docker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml exec nginx nginx -s reload\n\n    log_success \"SSL certificate renewed\"\n}\n\n# Setup auto-renewal\nsetup_auto_renewal() {\n    log_info \"Setting up automatic certificate renewal...\"\n\n    # Add cron job for renewal\n    (crontab -l 2&gt;/dev/null; echo \"0 3 * * * /opt/prs-deployment/scripts/ssl-automation-citylandcondo.sh --renew\") | crontab -\n\n    log_success \"Auto-renewal configured\"\n}\n</code></pre>"},{"location":"scripts/deployment/#health-check-script","title":"Health Check Script","text":""},{"location":"scripts/deployment/#system-health-checksh","title":"system-health-check.sh","text":"<p>Purpose: Comprehensive system health monitoring</p> <p>Usage: <pre><code>./system-health-check.sh [--verbose] [--json]\n</code></pre></p> <p>Key Features: - Service status monitoring - Resource usage checking - Database connectivity testing - Performance metrics collection</p> <pre><code>#!/bin/bash\n\n# Health check functions\ncheck_services() {\n    log_info \"Checking service status...\"\n\n    SERVICES=(\n        \"prs-onprem-nginx\"\n        \"prs-onprem-frontend\"\n        \"prs-onprem-backend\"\n        \"prs-onprem-postgres-timescale\"\n        \"prs-onprem-redis\"\n        \"prs-onprem-grafana\"\n        \"prs-onprem-prometheus\"\n    )\n\n    for service in \"${SERVICES[@]}\"; do\n        if docker ps --filter \"name=$service\" --filter \"status=running\" | grep -q \"$service\"; then\n            log_success \"\u2713 $service is running\"\n        else\n            log_error \"\u2717 $service is not running\"\n            HEALTH_ISSUES=$((HEALTH_ISSUES + 1))\n        fi\n    done\n}\n\ncheck_resources() {\n    log_info \"Checking system resources...\"\n\n    # CPU usage\n    CPU_USAGE=$(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | cut -d'%' -f1)\n    if (( $(echo \"$CPU_USAGE &gt; 80\" | bc -l) )); then\n        log_warning \"High CPU usage: ${CPU_USAGE}%\"\n    else\n        log_success \"\u2713 CPU usage: ${CPU_USAGE}%\"\n    fi\n\n    # Memory usage\n    MEMORY_USAGE=$(free | grep Mem | awk '{printf(\"%.1f\", $3/$2 * 100.0)}')\n    if (( $(echo \"$MEMORY_USAGE &gt; 85\" | bc -l) )); then\n        log_warning \"High memory usage: ${MEMORY_USAGE}%\"\n    else\n        log_success \"\u2713 Memory usage: ${MEMORY_USAGE}%\"\n    fi\n\n    # Storage usage\n    SSD_USAGE=$(df -h /mnt/ssd | awk 'NR==2{print $5}' | cut -d'%' -f1)\n    HDD_USAGE=$(df -h /mnt/hdd | awk 'NR==2{print $5}' | cut -d'%' -f1)\n\n    if [ \"$SSD_USAGE\" -gt 85 ]; then\n        log_warning \"High SSD usage: ${SSD_USAGE}%\"\n    else\n        log_success \"\u2713 SSD usage: ${SSD_USAGE}%\"\n    fi\n\n    if [ \"$HDD_USAGE\" -gt 80 ]; then\n        log_warning \"High HDD usage: ${HDD_USAGE}%\"\n    else\n        log_success \"\u2713 HDD usage: ${HDD_USAGE}%\"\n    fi\n}\n\ncheck_connectivity() {\n    log_info \"Checking connectivity...\"\n\n    # Database connectivity\n    if docker exec prs-onprem-postgres-timescale pg_isready -U prs_admin &gt;/dev/null 2&gt;&amp;1; then\n        log_success \"\u2713 Database connectivity\"\n    else\n        log_error \"\u2717 Database connectivity failed\"\n        HEALTH_ISSUES=$((HEALTH_ISSUES + 1))\n    fi\n\n    # Redis connectivity\n    if docker exec prs-onprem-redis redis-cli -a \"$REDIS_PASSWORD\" ping &gt;/dev/null 2&gt;&amp;1; then\n        log_success \"\u2713 Redis connectivity\"\n    else\n        log_error \"\u2717 Redis connectivity failed\"\n        HEALTH_ISSUES=$((HEALTH_ISSUES + 1))\n    fi\n\n    # API endpoint\n    if curl -f -s https://localhost/api/health &gt;/dev/null 2&gt;&amp;1; then\n        log_success \"\u2713 API endpoint\"\n    else\n        log_warning \"API endpoint check failed\"\n    fi\n}\n\n# Main health check\nmain() {\n    HEALTH_ISSUES=0\n\n    log_info \"=== PRS System Health Check ===\"\n\n    check_services\n    check_resources\n    check_connectivity\n\n    if [ \"$HEALTH_ISSUES\" -eq 0 ]; then\n        log_success \"=== All health checks passed ===\"\n        exit 0\n    else\n        log_error \"=== Health check failed with $HEALTH_ISSUES issues ===\"\n        exit 1\n    fi\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"scripts/deployment/#utility-scripts","title":"Utility Scripts","text":""},{"location":"scripts/deployment/#docker-cleanupsh","title":"docker-cleanup.sh","text":"<p>Purpose: Docker system maintenance and cleanup</p> <pre><code>#!/bin/bash\n# Docker cleanup utility\n\n# Remove stopped containers\ndocker container prune -f\n\n# Remove unused images\ndocker image prune -f\n\n# Remove unused volumes\ndocker volume prune -f\n\n# Remove unused networks\ndocker network prune -f\n\n# Clean build cache\ndocker builder prune -f\n\nlog_success \"Docker cleanup completed\"\n</code></pre>"},{"location":"scripts/deployment/#log-rotationsh","title":"log-rotation.sh","text":"<p>Purpose: Application log rotation and archival</p> <pre><code>#!/bin/bash\n# Log rotation script\n\nLOG_DIR=\"/mnt/ssd/logs\"\nARCHIVE_DIR=\"/mnt/hdd/app-logs-archive\"\nRETENTION_DAYS=30\n\n# Rotate and compress logs older than 1 day\nfind \"$LOG_DIR\" -name \"*.log\" -mtime +1 -exec gzip {} \\;\n\n# Move compressed logs older than 7 days to archive\nfind \"$LOG_DIR\" -name \"*.log.gz\" -mtime +7 -exec mv {} \"$ARCHIVE_DIR/\" \\;\n\n# Remove archived logs older than retention period\nfind \"$ARCHIVE_DIR\" -name \"*.log.gz\" -mtime +$RETENTION_DAYS -delete\n\nlog_success \"Log rotation completed\"\n</code></pre>"},{"location":"scripts/deployment/#nas-integration-for-enterprise-backup","title":"NAS Integration for Enterprise Backup","text":""},{"location":"scripts/deployment/#overview_1","title":"Overview","text":"<p>The PRS deployment now includes comprehensive NAS (Network Attached Storage) integration for enterprise-grade backup redundancy and disaster recovery capabilities.</p>"},{"location":"scripts/deployment/#nas-configuration","title":"NAS Configuration","text":""},{"location":"scripts/deployment/#quick-setup","title":"Quick Setup","text":"<ol> <li> <p>Configure NAS Settings: <pre><code># Copy NAS configuration template\ncp /opt/prs-deployment/scripts/nas-config.example.sh /opt/prs-deployment/scripts/nas-config.sh\n\n# Edit with your NAS details\nnano /opt/prs-deployment/scripts/nas-config.sh\n\n# Secure the configuration\nchmod 600 /opt/prs-deployment/scripts/nas-config.sh\n</code></pre></p> </li> <li> <p>Test NAS Connection: <pre><code># Verify NAS connectivity and functionality\n/opt/prs-deployment/scripts/test-nas-connection.sh\n</code></pre></p> </li> <li> <p>Enable NAS in Environment: <pre><code># Add to .env file\necho \"BACKUP_TO_NAS=true\" &gt;&gt; /opt/prs-deployment/02-docker-configuration/.env\necho \"NAS_HOST=your-nas-hostname\" &gt;&gt; /opt/prs-deployment/02-docker-configuration/.env\necho \"NAS_SHARE=backups\" &gt;&gt; /opt/prs-deployment/02-docker-configuration/.env\n</code></pre></p> </li> </ol>"},{"location":"scripts/deployment/#supported-nas-systems","title":"Supported NAS Systems","text":"<ul> <li>Synology DSM (CIFS/SMB)</li> <li>QNAP QTS (CIFS/SMB)</li> <li>FreeNAS/TrueNAS (NFS)</li> <li>Windows Server (CIFS/SMB)</li> <li>Linux NFS servers</li> </ul>"},{"location":"scripts/deployment/#enterprise-features","title":"Enterprise Features","text":"<ul> <li>Dual retention policies: Local (30 days) + NAS (90 days)</li> <li>Automatic mounting/unmounting of NAS shares</li> <li>Backup verification and integrity checking</li> <li>Graceful degradation if NAS is unavailable</li> <li>Comprehensive logging and error handling</li> <li>Email notifications with NAS status</li> </ul>"},{"location":"scripts/deployment/#integration-with-deployment-scripts","title":"Integration with Deployment Scripts","text":"<p>The following deployment scripts now include NAS integration:</p> <ul> <li><code>backup-full.sh</code> - Database backups with NAS copy</li> <li><code>backup-application-data.sh</code> - Application data with NAS storage</li> <li><code>backup-maintenance.sh</code> - Comprehensive maintenance with NAS</li> <li><code>daily-maintenance-automation.sh</code> - Automated NAS operations</li> </ul>"},{"location":"scripts/deployment/#security-considerations","title":"Security Considerations","text":"<ul> <li>Secure credential storage in protected configuration files</li> <li>Network isolation for backup traffic</li> <li>Encryption support for sensitive backup data</li> <li>Access control with dedicated backup user accounts</li> </ul> <p>Script Automation</p> <p>All scripts can be automated using cron jobs for regular maintenance and monitoring tasks. NAS integration provides additional backup redundancy.</p> <p>Permissions</p> <p>Ensure scripts have proper execution permissions and are run with appropriate user privileges. Verify NAS connectivity before enabling automated backups.</p>"},{"location":"scripts/maintenance/","title":"Maintenance Scripts","text":""},{"location":"scripts/maintenance/#overview","title":"Overview","text":"<p>This guide covers all maintenance-related scripts in the PRS on-premises deployment, including automated maintenance procedures, system optimization, and preventive maintenance tasks.</p>"},{"location":"scripts/maintenance/#core-maintenance-scripts","title":"Core Maintenance Scripts","text":""},{"location":"scripts/maintenance/#daily-maintenance-automation","title":"Daily Maintenance Automation","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/daily-maintenance-automation.sh\n# Comprehensive daily maintenance automation\n\nset -euo pipefail\n\nLOG_FILE=\"/var/log/prs-maintenance.log\"\nMAINTENANCE_LOCK=\"/var/run/prs-maintenance.lock\"\nEMAIL_REPORT=\"admin@your-domain.com\"\n\n# Maintenance configuration\nVACUUM_THRESHOLD=1000        # Dead tuples threshold for vacuum\nLOG_RETENTION_DAYS=7         # Log retention in days\nTEMP_FILE_AGE_HOURS=24      # Temporary file cleanup age\nBACKUP_VERIFICATION=true     # Enable backup verification\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\nacquire_lock() {\n    if [ -f \"$MAINTENANCE_LOCK\" ]; then\n        local lock_pid=$(cat \"$MAINTENANCE_LOCK\")\n        if kill -0 \"$lock_pid\" 2&gt;/dev/null; then\n            log_message \"Maintenance already running (PID: $lock_pid)\"\n            exit 1\n        else\n            log_message \"Removing stale lock file\"\n            rm -f \"$MAINTENANCE_LOCK\"\n        fi\n    fi\n\n    echo $$ &gt; \"$MAINTENANCE_LOCK\"\n    trap 'rm -f \"$MAINTENANCE_LOCK\"; exit' INT TERM EXIT\n}\n\ndatabase_maintenance() {\n    log_message \"Starting database maintenance\"\n\n    # Update table statistics\n    log_message \"Updating table statistics\"\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n    ANALYZE notifications;\n    ANALYZE audit_logs;\n    ANALYZE requisitions;\n    ANALYZE purchase_orders;\n    ANALYZE users;\n    ANALYZE departments;\n    \"\n\n    # Check for tables needing vacuum\n    local tables_needing_vacuum=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT tablename FROM pg_stat_user_tables \n    WHERE n_dead_tup &gt; $VACUUM_THRESHOLD \n    AND n_dead_tup::float / NULLIF(n_live_tup + n_dead_tup, 0) &gt; 0.1;\n    \" | xargs)\n\n    if [ -n \"$tables_needing_vacuum\" ]; then\n        log_message \"Vacuuming tables: $tables_needing_vacuum\"\n        for table in $tables_needing_vacuum; do\n            docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"VACUUM ANALYZE $table;\"\n        done\n    else\n        log_message \"No tables require vacuuming\"\n    fi\n\n    # Check database health\n    local db_health=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT \n        CASE \n            WHEN pg_is_in_recovery() THEN 'RECOVERY'\n            WHEN EXISTS (SELECT 1 FROM pg_stat_activity WHERE state = 'active' AND query_start &lt; now() - interval '1 hour') THEN 'SLOW_QUERIES'\n            ELSE 'HEALTHY'\n        END;\n    \" | xargs)\n\n    log_message \"Database health status: $db_health\"\n\n    if [ \"$db_health\" != \"HEALTHY\" ]; then\n        log_message \"WARNING: Database health issue detected: $db_health\"\n    fi\n}\n\nstorage_maintenance() {\n    log_message \"Starting storage maintenance\"\n\n    # Check storage usage\n    local ssd_usage=$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\n    local hdd_usage=$(df /mnt/hdd | awk 'NR==2 {print $5}' | sed 's/%//')\n\n    log_message \"Storage usage - SSD: ${ssd_usage}%, HDD: ${hdd_usage}%\"\n\n    # Trigger data movement if SSD usage is high\n    if [ \"$ssd_usage\" -gt 85 ]; then\n        log_message \"High SSD usage detected, triggering data movement\"\n        docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n        SELECT move_chunk(chunk_name, 'hdd_cold')\n        FROM timescaledb_information.chunks \n        WHERE range_start &lt; NOW() - INTERVAL '14 days'\n        AND tablespace_name = 'ssd_hot'\n        LIMIT 5;\n        \"\n    fi\n\n    # Clean temporary files\n    log_message \"Cleaning temporary files\"\n    find /tmp -type f -mtime +1 -delete 2&gt;/dev/null || true\n    find /var/tmp -type f -mtime +1 -delete 2&gt;/dev/null || true\n\n    # Clean old application logs\n    log_message \"Managing application logs\"\n    find /mnt/ssd/logs -name \"*.log\" -mtime +$LOG_RETENTION_DAYS -exec gzip {} \\;\n    find /mnt/ssd/logs -name \"*.log.gz\" -mtime +30 -delete\n\n    # Docker system cleanup\n    log_message \"Cleaning Docker system\"\n    docker system prune -f --volumes\n\n    # Check for large files\n    local large_files=$(find /mnt/ssd -type f -size +1G 2&gt;/dev/null | wc -l)\n    if [ \"$large_files\" -gt 0 ]; then\n        log_message \"Found $large_files files larger than 1GB\"\n        find /mnt/ssd -type f -size +1G -exec ls -lh {} \\; &gt;&gt; \"$LOG_FILE\"\n    fi\n}\n\napplication_maintenance() {\n    log_message \"Starting application maintenance\"\n\n    # Check application health\n    local api_status=$(curl -s -o /dev/null -w \"%{http_code}\" https://localhost/api/health || echo \"000\")\n    local frontend_status=$(curl -s -o /dev/null -w \"%{http_code}\" https://localhost/ || echo \"000\")\n\n    log_message \"Application status - API: $api_status, Frontend: $frontend_status\"\n\n    # Check for memory leaks\n    local backend_memory=$(docker stats prs-onprem-backend --no-stream --format \"{{.MemUsage}}\" | cut -d'/' -f1 | sed 's/[^0-9.]//g')\n    local frontend_memory=$(docker stats prs-onprem-frontend --no-stream --format \"{{.MemUsage}}\" | cut -d'/' -f1 | sed 's/[^0-9.]//g')\n\n    log_message \"Memory usage - Backend: ${backend_memory}MB, Frontend: ${frontend_memory}MB\"\n\n    # Restart services if memory usage is too high\n    if (( $(echo \"$backend_memory &gt; 2000\" | bc -l) )); then\n        log_message \"High backend memory usage, restarting service\"\n        docker-compose -f /opt/prs-deployment/02-docker-configuration/docker-compose.onprem.yml restart backend\n    fi\n\n    # Check for failed background jobs\n    local failed_jobs=$(docker exec prs-onprem-redis redis-cli -a \"$REDIS_PASSWORD\" llen \"failed_jobs\" 2&gt;/dev/null || echo \"0\")\n    if [ \"$failed_jobs\" -gt 0 ]; then\n        log_message \"WARNING: $failed_jobs failed background jobs detected\"\n    fi\n\n    # Clear expired sessions\n    log_message \"Clearing expired sessions\"\n    docker exec prs-onprem-redis redis-cli -a \"$REDIS_PASSWORD\" eval \"\n    local keys = redis.call('keys', 'session:*')\n    local expired = 0\n    for i=1,#keys do\n        local ttl = redis.call('ttl', keys[i])\n        if ttl == -1 then\n            redis.call('del', keys[i])\n            expired = expired + 1\n        end\n    end\n    return expired\n    \" 0 2&gt;/dev/null || echo \"0\"\n}\n\nsecurity_maintenance() {\n    log_message \"Starting security maintenance\"\n\n    # Check for failed login attempts\n    local failed_logins=$(grep \"Failed password\" /var/log/auth.log | grep \"$(date +%Y-%m-%d)\" | wc -l)\n    log_message \"Failed login attempts today: $failed_logins\"\n\n    if [ \"$failed_logins\" -gt 20 ]; then\n        log_message \"WARNING: High number of failed login attempts: $failed_logins\"\n    fi\n\n    # Check SSL certificate expiration\n    if [ -f \"/opt/prs-deployment/02-docker-configuration/ssl/certificate.crt\" ]; then\n        local cert_expiry=$(openssl x509 -in /opt/prs-deployment/02-docker-configuration/ssl/certificate.crt -noout -enddate | cut -d= -f2)\n        local expiry_epoch=$(date -d \"$cert_expiry\" +%s)\n        local current_epoch=$(date +%s)\n        local days_until_expiry=$(( (expiry_epoch - current_epoch) / 86400 ))\n\n        log_message \"SSL certificate expires in $days_until_expiry days\"\n\n        if [ \"$days_until_expiry\" -lt 30 ]; then\n            log_message \"WARNING: SSL certificate expires in $days_until_expiry days\"\n        fi\n    fi\n\n    # Check for security updates\n    local security_updates=$(apt list --upgradable 2&gt;/dev/null | grep security | wc -l)\n    if [ \"$security_updates\" -gt 0 ]; then\n        log_message \"Available security updates: $security_updates\"\n    fi\n\n    # Audit user permissions\n    local inactive_users=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT count(*) FROM users \n    WHERE last_login_at &lt; NOW() - INTERVAL '90 days'\n    OR last_login_at IS NULL;\n    \" | xargs)\n\n    if [ \"$inactive_users\" -gt 0 ]; then\n        log_message \"Inactive users (90+ days): $inactive_users\"\n    fi\n}\n\nbackup_verification() {\n    if [ \"$BACKUP_VERIFICATION\" = true ]; then\n        log_message \"Verifying recent backups\"\n\n        local latest_backup=$(ls -t /mnt/hdd/postgres-backups/daily/*.sql* 2&gt;/dev/null | head -1)\n\n        if [ -n \"$latest_backup\" ]; then\n            local backup_age_hours=$(( ($(date +%s) - $(stat -c %Y \"$latest_backup\")) / 3600 ))\n            log_message \"Latest backup: $latest_backup (${backup_age_hours}h old)\"\n\n            if [ \"$backup_age_hours\" -gt 25 ]; then\n                log_message \"WARNING: Latest backup is older than 25 hours\"\n            fi\n\n            # Verify backup integrity\n            if [ -f \"${latest_backup}.sha256\" ]; then\n                if sha256sum -c \"${latest_backup}.sha256\" &gt;/dev/null 2&gt;&amp;1; then\n                    log_message \"Backup integrity verification passed\"\n                else\n                    log_message \"ERROR: Backup integrity verification failed\"\n                fi\n            else\n                log_message \"WARNING: No checksum file for latest backup\"\n            fi\n        else\n            log_message \"ERROR: No backup files found\"\n        fi\n    fi\n}\n\nperformance_optimization() {\n    log_message \"Starting performance optimization\"\n\n    # Check and optimize TimescaleDB compression\n    local uncompressed_chunks=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT count(*) FROM timescaledb_information.chunks \n    WHERE range_start &lt; NOW() - INTERVAL '7 days'\n    AND NOT is_compressed\n    AND hypertable_name IN ('notifications', 'audit_logs');\n    \" | xargs)\n\n    if [ \"$uncompressed_chunks\" -gt 0 ]; then\n        log_message \"Compressing $uncompressed_chunks old chunks\"\n        docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n        SELECT compress_chunk(chunk_name) \n        FROM timescaledb_information.chunks \n        WHERE range_start &lt; NOW() - INTERVAL '7 days'\n        AND NOT is_compressed\n        AND hypertable_name IN ('notifications', 'audit_logs')\n        LIMIT 10;\n        \"\n    fi\n\n    # Check query performance\n    local slow_queries=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT count(*) FROM pg_stat_statements \n    WHERE mean_time &gt; 1000 AND calls &gt; 10;\n    \" | xargs)\n\n    if [ \"$slow_queries\" -gt 5 ]; then\n        log_message \"WARNING: $slow_queries slow queries detected\"\n    fi\n\n    # Check cache hit ratio\n    local cache_hit_ratio=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT round(100.0 * sum(blks_hit) / nullif(sum(blks_hit) + sum(blks_read), 0), 2)\n    FROM pg_stat_database WHERE datname = 'prs_production';\n    \" | xargs)\n\n    log_message \"Database cache hit ratio: ${cache_hit_ratio}%\"\n\n    if (( $(echo \"$cache_hit_ratio &lt; 95\" | bc -l) )); then\n        log_message \"WARNING: Low cache hit ratio: ${cache_hit_ratio}%\"\n    fi\n}\n\ngenerate_maintenance_report() {\n    local report_file=\"/tmp/daily-maintenance-report-$(date +%Y%m%d).txt\"\n\n    cat &gt; \"$report_file\" &lt;&lt; EOF\nPRS Daily Maintenance Report\n============================\nDate: $(date)\nHostname: $(hostname)\n\nSYSTEM STATUS\n-------------\n- CPU Usage: $(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | sed 's/%us,//')%\n- Memory Usage: $(free | grep Mem | awk '{printf \"%.1f\", $3/$2 * 100.0}')%\n- SSD Usage: $(df /mnt/ssd | awk 'NR==2 {print $5}')\n- HDD Usage: $(df /mnt/hdd | awk 'NR==2 {print $5}')\n- Load Average: $(uptime | awk -F'load average:' '{print $2}')\n\nDATABASE STATUS\n---------------\n- Database Size: $(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT pg_size_pretty(pg_database_size('prs_production'));\" | xargs)\n- Active Connections: $(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT count(*) FROM pg_stat_activity;\" | xargs)\n- Cache Hit Ratio: $(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT round(100.0 * sum(blks_hit) / nullif(sum(blks_hit) + sum(blks_read), 0), 2) FROM pg_stat_database WHERE datname = 'prs_production';\" | xargs)%\n\nAPPLICATION STATUS\n------------------\n- API Status: $(curl -s -o /dev/null -w \"%{http_code}\" https://localhost/api/health || echo \"ERROR\")\n- Frontend Status: $(curl -s -o /dev/null -w \"%{http_code}\" https://localhost/ || echo \"ERROR\")\n- Active Sessions: $(docker exec prs-onprem-redis redis-cli -a \"$REDIS_PASSWORD\" eval \"return #redis.call('keys', 'session:*')\" 0 2&gt;/dev/null || echo \"N/A\")\n\nMAINTENANCE ACTIONS\n-------------------\n$(tail -50 \"$LOG_FILE\" | grep \"$(date +%Y-%m-%d)\")\n\nRECOMMENDATIONS\n---------------\n$(if [ \"$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\" -gt 80 ]; then echo \"- Monitor SSD usage closely\"; fi)\n$(if [ \"$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT count(*) FROM pg_stat_statements WHERE mean_time &gt; 1000 AND calls &gt; 10;\" | xargs)\" -gt 5 ]; then echo \"- Review slow queries\"; fi)\n$(if [ \"$(grep \"Failed password\" /var/log/auth.log | grep \"$(date +%Y-%m-%d)\" | wc -l)\" -gt 20 ]; then echo \"- Investigate failed login attempts\"; fi)\nEOF\n\n    log_message \"Maintenance report generated: $report_file\"\n\n    # Email report if configured\n    if command -v mail &gt;/dev/null 2&gt;&amp;1 &amp;&amp; [ -n \"$EMAIL_REPORT\" ]; then\n        mail -s \"PRS Daily Maintenance Report\" \"$EMAIL_REPORT\" &lt; \"$report_file\"\n    fi\n}\n\nmain() {\n    acquire_lock\n\n    log_message \"Starting daily maintenance automation\"\n\n    database_maintenance\n    storage_maintenance\n    application_maintenance\n    security_maintenance\n    backup_verification\n    performance_optimization\n\n    generate_maintenance_report\n\n    log_message \"Daily maintenance automation completed successfully\"\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"scripts/maintenance/#weekly-maintenance-automation","title":"Weekly Maintenance Automation","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/weekly-maintenance-automation.sh\n# Comprehensive weekly maintenance automation\n\nset -euo pipefail\n\nLOG_FILE=\"/var/log/prs-maintenance.log\"\nWEEK_NUMBER=$(date +%V)\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\ncomprehensive_database_maintenance() {\n    log_message \"Starting comprehensive database maintenance\"\n\n    # Full VACUUM ANALYZE\n    log_message \"Performing full VACUUM ANALYZE\"\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n    SET maintenance_work_mem = '1GB';\n    VACUUM (ANALYZE, VERBOSE) notifications;\n    VACUUM (ANALYZE, VERBOSE) audit_logs;\n    VACUUM (ANALYZE, VERBOSE) requisitions;\n    VACUUM (ANALYZE, VERBOSE) purchase_orders;\n    VACUUM (ANALYZE, VERBOSE) users;\n    VACUUM (ANALYZE, VERBOSE) departments;\n    \"\n\n    # Reindex small tables\n    log_message \"Reindexing small tables\"\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n    REINDEX TABLE users;\n    REINDEX TABLE departments;\n    REINDEX TABLE categories;\n    REINDEX TABLE vendors;\n    \"\n\n    # Update TimescaleDB compression\n    log_message \"Updating TimescaleDB compression\"\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n    SELECT compress_chunk(chunk_name) \n    FROM timescaledb_information.chunks \n    WHERE range_start &lt; NOW() - INTERVAL '7 days'\n    AND NOT is_compressed\n    AND hypertable_name IN ('notifications', 'audit_logs')\n    LIMIT 20;\n    \"\n\n    # Check for unused indexes\n    log_message \"Checking for unused indexes\"\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n    SELECT \n        schemaname,\n        tablename,\n        indexname,\n        pg_size_pretty(pg_relation_size(indexrelid)) as size,\n        idx_scan\n    FROM pg_stat_user_indexes\n    WHERE idx_scan = 0\n      AND pg_relation_size(indexrelid) &gt; 1024*1024\n    ORDER BY pg_relation_size(indexrelid) DESC;\n    \" &gt; /tmp/unused-indexes-week$WEEK_NUMBER.log\n}\n\nsystem_optimization() {\n    log_message \"Starting system optimization\"\n\n    # Clean package cache\n    apt-get clean\n    apt-get autoremove -y\n\n    # Update locate database\n    if command -v updatedb &gt;/dev/null 2&gt;&amp;1; then\n        updatedb\n    fi\n\n    # Optimize system logs\n    journalctl --vacuum-time=30d\n\n    # Check for system updates\n    apt update\n    local available_updates=$(apt list --upgradable 2&gt;/dev/null | wc -l)\n    log_message \"Available system updates: $available_updates\"\n\n    # Optimize Docker\n    docker system prune -af --volumes\n    docker image prune -af\n\n    # Check disk fragmentation (if ext4)\n    if mount | grep -q \"ext4\"; then\n        log_message \"Checking filesystem fragmentation\"\n        e4defrag -c /mnt/ssd &gt; /tmp/ssd-fragmentation.log 2&gt;&amp;1 || true\n        e4defrag -c /mnt/hdd &gt; /tmp/hdd-fragmentation.log 2&gt;&amp;1 || true\n    fi\n}\n\nsecurity_audit() {\n    log_message \"Starting weekly security audit\"\n\n    # Check for security updates\n    local security_updates=$(apt list --upgradable 2&gt;/dev/null | grep security | wc -l)\n    log_message \"Available security updates: $security_updates\"\n\n    if [ \"$security_updates\" -gt 0 ]; then\n        log_message \"Security updates available - manual review required\"\n        apt list --upgradable 2&gt;/dev/null | grep security &gt; /tmp/security-updates-week$WEEK_NUMBER.txt\n    fi\n\n    # Check user access patterns\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n    SELECT \n        u.username,\n        u.role,\n        u.last_login_at,\n        COUNT(al.id) as actions_this_week\n    FROM users u\n    LEFT JOIN audit_logs al ON u.id = al.user_id \n        AND al.created_at &gt;= NOW() - INTERVAL '7 days'\n    GROUP BY u.id, u.username, u.role, u.last_login_at\n    ORDER BY actions_this_week DESC;\n    \" &gt; /tmp/user-activity-week$WEEK_NUMBER.log\n\n    # Check for suspicious activities\n    local suspicious_activities=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT count(*) FROM audit_logs \n    WHERE created_at &gt;= NOW() - INTERVAL '7 days'\n    AND action IN ('delete', 'bulk_delete', 'admin_override');\n    \" | xargs)\n\n    if [ \"$suspicious_activities\" -gt 10 ]; then\n        log_message \"WARNING: High number of sensitive actions this week: $suspicious_activities\"\n    fi\n}\n\nperformance_analysis() {\n    log_message \"Starting performance analysis\"\n\n    # Analyze query performance\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n    SELECT \n        'Top 10 Slow Queries (This Week)' as analysis;\n\n    SELECT \n        left(query, 80) as query_snippet,\n        calls,\n        round(mean_time::numeric, 2) as avg_time_ms,\n        round(total_time::numeric, 2) as total_time_ms\n    FROM pg_stat_statements \n    WHERE calls &gt; 100\n    ORDER BY total_time DESC \n    LIMIT 10;\n    \" &gt; /tmp/performance-analysis-week$WEEK_NUMBER.log\n\n    # Check compression effectiveness\n    local compression_stats=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT \n        round(\n            AVG((before_compression_total_bytes::numeric - after_compression_total_bytes::numeric) \n            / before_compression_total_bytes::numeric * 100), 2\n        )\n    FROM timescaledb_information.compressed_hypertable_stats;\n    \" | xargs)\n\n    log_message \"Average compression ratio: ${compression_stats}%\"\n\n    # Check storage growth\n    local db_size_gb=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT ROUND(pg_database_size('prs_production') / 1024.0 / 1024.0 / 1024.0, 2);\n    \" | xargs)\n\n    log_message \"Current database size: ${db_size_gb}GB\"\n}\n\ngenerate_weekly_report() {\n    local report_file=\"/tmp/weekly-maintenance-report-week$WEEK_NUMBER.txt\"\n\n    cat &gt; \"$report_file\" &lt;&lt; EOF\nPRS Weekly Maintenance Report - Week $WEEK_NUMBER\n=================================================\nGenerated: $(date)\n\nMAINTENANCE SUMMARY\n-------------------\n- Database maintenance: VACUUM ANALYZE completed\n- System optimization: Package cleanup and Docker optimization\n- Security audit: $(apt list --upgradable 2&gt;/dev/null | grep security | wc -l) security updates available\n- Performance analysis: Completed query and compression analysis\n\nSYSTEM HEALTH\n--------------\n- Database Size: $(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT pg_size_pretty(pg_database_size('prs_production'));\" | xargs)\n- Compression Ratio: $(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT round(AVG((before_compression_total_bytes::numeric - after_compression_total_bytes::numeric) / before_compression_total_bytes::numeric * 100), 2) FROM timescaledb_information.compressed_hypertable_stats;\" | xargs)%\n- SSD Usage: $(df /mnt/ssd | awk 'NR==2 {print $5}')\n- HDD Usage: $(df /mnt/hdd | awk 'NR==2 {print $5}')\n\nPERFORMANCE METRICS\n-------------------\n$(cat /tmp/performance-analysis-week$WEEK_NUMBER.log)\n\nSECURITY STATUS\n---------------\n$(if [ -f /tmp/security-updates-week$WEEK_NUMBER.txt ]; then echo \"Security updates available:\"; cat /tmp/security-updates-week$WEEK_NUMBER.txt; else echo \"No security updates available\"; fi)\n\nRECOMMENDATIONS\n---------------\n$(if [ \"$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\" -gt 80 ]; then echo \"- Consider SSD capacity expansion\"; fi)\n$(if [ \"$(apt list --upgradable 2&gt;/dev/null | grep security | wc -l)\" -gt 0 ]; then echo \"- Apply available security updates\"; fi)\n$(if [ \"$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT count(*) FROM pg_stat_user_indexes WHERE idx_scan = 0 AND pg_relation_size(indexrelid) &gt; 1024*1024;\" | xargs)\" -gt 3 ]; then echo \"- Review and remove unused indexes\"; fi)\nEOF\n\n    log_message \"Weekly maintenance report generated: $report_file\"\n\n    # Email report if configured\n    if command -v mail &gt;/dev/null 2&gt;&amp;1; then\n        mail -s \"PRS Weekly Maintenance Report - Week $WEEK_NUMBER\" admin@your-domain.com &lt; \"$report_file\"\n    fi\n}\n\nmain() {\n    log_message \"Starting weekly maintenance automation (Week $WEEK_NUMBER)\"\n\n    comprehensive_database_maintenance\n    system_optimization\n    security_audit\n    performance_analysis\n\n    generate_weekly_report\n\n    log_message \"Weekly maintenance automation completed successfully\"\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"scripts/maintenance/#maintenance-status-monitor","title":"Maintenance Status Monitor","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/maintenance-status-monitor.sh\n# Monitor maintenance job status and health\n\nset -euo pipefail\n\nLOG_FILE=\"/var/log/prs-maintenance.log\"\nSTATUS_FILE=\"/var/run/prs-maintenance-status\"\n\ncheck_maintenance_status() {\n    echo \"PRS Maintenance Status Monitor\"\n    echo \"==============================\"\n    echo \"Generated: $(date)\"\n    echo \"\"\n\n    # Check last maintenance runs\n    echo \"LAST MAINTENANCE RUNS:\"\n    echo \"----------------------\"\n    if [ -f \"$LOG_FILE\" ]; then\n        echo \"Daily: $(grep \"daily maintenance.*completed\" \"$LOG_FILE\" | tail -1 | cut -d' ' -f1-2 || echo \"Never\")\"\n        echo \"Weekly: $(grep \"weekly maintenance.*completed\" \"$LOG_FILE\" | tail -1 | cut -d' ' -f1-2 || echo \"Never\")\"\n        echo \"Monthly: $(grep \"monthly.*completed\" \"$LOG_FILE\" | tail -1 | cut -d' ' -f1-2 || echo \"Never\")\"\n    else\n        echo \"No maintenance log found\"\n    fi\n\n    echo \"\"\n\n    # Check for maintenance errors\n    echo \"RECENT MAINTENANCE ISSUES:\"\n    echo \"--------------------------\"\n    if [ -f \"$LOG_FILE\" ]; then\n        local error_count=$(grep -c \"ERROR\\|WARNING\" \"$LOG_FILE\" | tail -100 || echo \"0\")\n        echo \"Errors/Warnings (last 100 entries): $error_count\"\n\n        if [ \"$error_count\" -gt 0 ]; then\n            echo \"Recent issues:\"\n            grep \"ERROR\\|WARNING\" \"$LOG_FILE\" | tail -5\n        fi\n    fi\n\n    echo \"\"\n\n    # Check maintenance job schedule\n    echo \"SCHEDULED MAINTENANCE JOBS:\"\n    echo \"---------------------------\"\n    crontab -l | grep prs-deployment || echo \"No maintenance jobs scheduled\"\n\n    echo \"\"\n\n    # Check system health\n    echo \"CURRENT SYSTEM STATUS:\"\n    echo \"----------------------\"\n    echo \"CPU: $(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | sed 's/%us,//')%\"\n    echo \"Memory: $(free | grep Mem | awk '{printf \"%.1f\", $3/$2 * 100.0}')%\"\n    echo \"SSD: $(df /mnt/ssd | awk 'NR==2 {print $5}')\"\n    echo \"HDD: $(df /mnt/hdd | awk 'NR==2 {print $5}')\"\n\n    # Save status\n    cat &gt; \"$STATUS_FILE\" &lt;&lt; EOF\n{\n    \"timestamp\": \"$(date -Iseconds)\",\n    \"daily_maintenance\": \"$(grep \"daily maintenance.*completed\" \"$LOG_FILE\" | tail -1 | cut -d' ' -f1-2 || echo \"Never\")\",\n    \"weekly_maintenance\": \"$(grep \"weekly maintenance.*completed\" \"$LOG_FILE\" | tail -1 | cut -d' ' -f1-2 || echo \"Never\")\",\n    \"error_count\": \"$(grep -c \"ERROR\\|WARNING\" \"$LOG_FILE\" | tail -100 || echo \"0\")\",\n    \"cpu_usage\": \"$(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | sed 's/%us,//')\",\n    \"memory_usage\": \"$(free | grep Mem | awk '{printf \"%.1f\", $3/$2 * 100.0}')\",\n    \"ssd_usage\": \"$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\",\n    \"hdd_usage\": \"$(df /mnt/hdd | awk 'NR==2 {print $5}' | sed 's/%//')\"\n}\nEOF\n}\n\ncheck_maintenance_status\n</code></pre> <p>Maintenance Scripts Ready</p> <p>Your PRS deployment now has comprehensive maintenance scripts covering daily automation, weekly optimization, and status monitoring for complete system maintenance.</p> <p>Automation Setup</p> <p>Use cron to schedule these maintenance scripts for automated system care and optimization.</p> <p>Monitoring Required</p> <p>Always monitor maintenance script execution and review logs regularly to ensure all maintenance tasks complete successfully.</p>"},{"location":"scripts/monitoring/","title":"Monitoring Scripts","text":""},{"location":"scripts/monitoring/#overview","title":"Overview","text":"<p>This guide covers all monitoring-related scripts in the PRS on-premises deployment, including system monitoring, performance tracking, alerting, and dashboard automation.</p>"},{"location":"scripts/monitoring/#core-monitoring-scripts","title":"Core Monitoring Scripts","text":""},{"location":"scripts/monitoring/#system-performance-monitor","title":"System Performance Monitor","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/system-performance-monitor.sh\n# Comprehensive system performance monitoring\n\nset -euo pipefail\n\nMETRICS_FILE=\"/var/lib/node_exporter/textfile_collector/prs-metrics.prom\"\nLOG_FILE=\"/var/log/prs-monitoring.log\"\nALERT_THRESHOLD_FILE=\"/etc/prs/alert-thresholds.conf\"\n\n# Default alert thresholds\nCPU_THRESHOLD=80\nMEMORY_THRESHOLD=85\nDISK_THRESHOLD=90\nRESPONSE_TIME_THRESHOLD=2000\nCONNECTION_THRESHOLD=120\n\n# Load custom thresholds if available\nif [ -f \"$ALERT_THRESHOLD_FILE\" ]; then\n    source \"$ALERT_THRESHOLD_FILE\"\nfi\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\ncollect_system_metrics() {\n    local timestamp=$(date +%s)\n\n    # CPU metrics\n    local cpu_usage=$(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | sed 's/%us,//')\n    local load_1min=$(uptime | awk -F'load average:' '{print $2}' | awk '{print $1}' | sed 's/,//')\n    local load_5min=$(uptime | awk -F'load average:' '{print $2}' | awk '{print $2}' | sed 's/,//')\n    local load_15min=$(uptime | awk -F'load average:' '{print $2}' | awk '{print $3}')\n\n    # Memory metrics\n    local memory_total=$(free -b | grep Mem | awk '{print $2}')\n    local memory_used=$(free -b | grep Mem | awk '{print $3}')\n    local memory_free=$(free -b | grep Mem | awk '{print $4}')\n    local memory_available=$(free -b | grep Mem | awk '{print $7}')\n    local memory_usage_percent=$(echo \"scale=2; $memory_used * 100 / $memory_total\" | bc)\n\n    # Disk metrics\n    local ssd_total=$(df -B1 /mnt/ssd | awk 'NR==2 {print $2}')\n    local ssd_used=$(df -B1 /mnt/ssd | awk 'NR==2 {print $3}')\n    local ssd_available=$(df -B1 /mnt/ssd | awk 'NR==2 {print $4}')\n    local ssd_usage_percent=$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\n\n    local hdd_total=$(df -B1 /mnt/hdd | awk 'NR==2 {print $2}')\n    local hdd_used=$(df -B1 /mnt/hdd | awk 'NR==2 {print $3}')\n    local hdd_available=$(df -B1 /mnt/hdd | awk 'NR==2 {print $4}')\n    local hdd_usage_percent=$(df /mnt/hdd | awk 'NR==2 {print $5}' | sed 's/%//')\n\n    # Network metrics\n    local network_rx_bytes=$(cat /proc/net/dev | grep eth0 | awk '{print $2}')\n    local network_tx_bytes=$(cat /proc/net/dev | grep eth0 | awk '{print $10}')\n\n    # Write Prometheus metrics\n    cat &gt; \"$METRICS_FILE\" &lt;&lt; EOF\n# HELP prs_cpu_usage_percent CPU usage percentage\n# TYPE prs_cpu_usage_percent gauge\nprs_cpu_usage_percent $cpu_usage\n\n# HELP prs_load_average System load average\n# TYPE prs_load_average gauge\nprs_load_average{period=\"1m\"} $load_1min\nprs_load_average{period=\"5m\"} $load_5min\nprs_load_average{period=\"15m\"} $load_15min\n\n# HELP prs_memory_bytes Memory usage in bytes\n# TYPE prs_memory_bytes gauge\nprs_memory_bytes{type=\"total\"} $memory_total\nprs_memory_bytes{type=\"used\"} $memory_used\nprs_memory_bytes{type=\"free\"} $memory_free\nprs_memory_bytes{type=\"available\"} $memory_available\n\n# HELP prs_memory_usage_percent Memory usage percentage\n# TYPE prs_memory_usage_percent gauge\nprs_memory_usage_percent $memory_usage_percent\n\n# HELP prs_disk_bytes Disk usage in bytes\n# TYPE prs_disk_bytes gauge\nprs_disk_bytes{device=\"ssd\",type=\"total\"} $ssd_total\nprs_disk_bytes{device=\"ssd\",type=\"used\"} $ssd_used\nprs_disk_bytes{device=\"ssd\",type=\"available\"} $ssd_available\nprs_disk_bytes{device=\"hdd\",type=\"total\"} $hdd_total\nprs_disk_bytes{device=\"hdd\",type=\"used\"} $hdd_used\nprs_disk_bytes{device=\"hdd\",type=\"available\"} $hdd_available\n\n# HELP prs_disk_usage_percent Disk usage percentage\n# TYPE prs_disk_usage_percent gauge\nprs_disk_usage_percent{device=\"ssd\"} $ssd_usage_percent\nprs_disk_usage_percent{device=\"hdd\"} $hdd_usage_percent\n\n# HELP prs_network_bytes Network traffic in bytes\n# TYPE prs_network_bytes counter\nprs_network_bytes{direction=\"rx\"} $network_rx_bytes\nprs_network_bytes{direction=\"tx\"} $network_tx_bytes\nEOF\n\n    log_message \"System metrics collected: CPU=${cpu_usage}%, Memory=${memory_usage_percent}%, SSD=${ssd_usage_percent}%, HDD=${hdd_usage_percent}%\"\n}\n\ncollect_application_metrics() {\n    # Database metrics\n    local db_connections=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT count(*) FROM pg_stat_activity;\" 2&gt;/dev/null | xargs || echo \"0\")\n    local db_size_bytes=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT pg_database_size('prs_production');\" 2&gt;/dev/null | xargs || echo \"0\")\n    local cache_hit_ratio=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT round(100.0 * sum(blks_hit) / nullif(sum(blks_hit) + sum(blks_read), 0), 2) FROM pg_stat_database WHERE datname = 'prs_production';\" 2&gt;/dev/null | xargs || echo \"0\")\n\n    # Redis metrics\n    local redis_connected_clients=$(docker exec prs-onprem-redis redis-cli -a \"$REDIS_PASSWORD\" info clients 2&gt;/dev/null | grep connected_clients | cut -d: -f2 | tr -d '\\r' || echo \"0\")\n    local redis_used_memory=$(docker exec prs-onprem-redis redis-cli -a \"$REDIS_PASSWORD\" info memory 2&gt;/dev/null | grep used_memory: | cut -d: -f2 | tr -d '\\r' || echo \"0\")\n\n    # Application metrics\n    local active_sessions=$(docker exec prs-onprem-redis redis-cli -a \"$REDIS_PASSWORD\" eval \"return #redis.call('keys', 'session:*')\" 0 2&gt;/dev/null || echo \"0\")\n\n    # API response time test\n    local api_response_time=$(curl -w \"%{time_total}\" -o /dev/null -s https://localhost/api/health 2&gt;/dev/null || echo \"0\")\n    local api_response_time_ms=$(echo \"$api_response_time * 1000\" | bc)\n\n    # Service status\n    local services=(\"prs-onprem-nginx\" \"prs-onprem-frontend\" \"prs-onprem-backend\" \"prs-onprem-postgres-timescale\" \"prs-onprem-redis\")\n\n    # Append application metrics to Prometheus file\n    cat &gt;&gt; \"$METRICS_FILE\" &lt;&lt; EOF\n\n# HELP prs_database_connections Active database connections\n# TYPE prs_database_connections gauge\nprs_database_connections $db_connections\n\n# HELP prs_database_size_bytes Database size in bytes\n# TYPE prs_database_size_bytes gauge\nprs_database_size_bytes $db_size_bytes\n\n# HELP prs_database_cache_hit_ratio Database cache hit ratio percentage\n# TYPE prs_database_cache_hit_ratio gauge\nprs_database_cache_hit_ratio $cache_hit_ratio\n\n# HELP prs_redis_connected_clients Redis connected clients\n# TYPE prs_redis_connected_clients gauge\nprs_redis_connected_clients $redis_connected_clients\n\n# HELP prs_redis_used_memory_bytes Redis memory usage in bytes\n# TYPE prs_redis_used_memory_bytes gauge\nprs_redis_used_memory_bytes $redis_used_memory\n\n# HELP prs_active_sessions Active user sessions\n# TYPE prs_active_sessions gauge\nprs_active_sessions $active_sessions\n\n# HELP prs_api_response_time_ms API response time in milliseconds\n# TYPE prs_api_response_time_ms gauge\nprs_api_response_time_ms $api_response_time_ms\nEOF\n\n    # Service status metrics\n    for service in \"${services[@]}\"; do\n        if docker ps --filter \"name=$service\" --filter \"status=running\" | grep -q \"$service\"; then\n            echo \"prs_service_up{service=\\\"$service\\\"} 1\" &gt;&gt; \"$METRICS_FILE\"\n        else\n            echo \"prs_service_up{service=\\\"$service\\\"} 0\" &gt;&gt; \"$METRICS_FILE\"\n        fi\n    done\n\n    log_message \"Application metrics collected: DB_Conn=$db_connections, Sessions=$active_sessions, API_Time=${api_response_time_ms}ms\"\n}\n\ncheck_alerts() {\n    local cpu_usage=$(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | sed 's/%us,//' | cut -d. -f1)\n    local memory_usage=$(free | grep Mem | awk '{printf \"%.0f\", $3/$2 * 100.0}')\n    local ssd_usage=$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\n    local hdd_usage=$(df /mnt/hdd | awk 'NR==2 {print $5}' | sed 's/%//')\n    local db_connections=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT count(*) FROM pg_stat_activity;\" 2&gt;/dev/null | xargs || echo \"0\")\n    local api_response_time_ms=$(curl -w \"%{time_total}\" -o /dev/null -s https://localhost/api/health 2&gt;/dev/null | awk '{print $1 * 1000}' || echo \"0\")\n\n    # Check thresholds and send alerts\n    if [ \"$cpu_usage\" -gt \"$CPU_THRESHOLD\" ]; then\n        send_alert \"WARNING\" \"High CPU usage: ${cpu_usage}% (threshold: ${CPU_THRESHOLD}%)\"\n    fi\n\n    if [ \"$memory_usage\" -gt \"$MEMORY_THRESHOLD\" ]; then\n        send_alert \"WARNING\" \"High memory usage: ${memory_usage}% (threshold: ${MEMORY_THRESHOLD}%)\"\n    fi\n\n    if [ \"$ssd_usage\" -gt \"$DISK_THRESHOLD\" ]; then\n        send_alert \"CRITICAL\" \"High SSD usage: ${ssd_usage}% (threshold: ${DISK_THRESHOLD}%)\"\n    fi\n\n    if [ \"$hdd_usage\" -gt \"$DISK_THRESHOLD\" ]; then\n        send_alert \"WARNING\" \"High HDD usage: ${hdd_usage}% (threshold: ${DISK_THRESHOLD}%)\"\n    fi\n\n    if [ \"$db_connections\" -gt \"$CONNECTION_THRESHOLD\" ]; then\n        send_alert \"WARNING\" \"High database connections: $db_connections (threshold: $CONNECTION_THRESHOLD)\"\n    fi\n\n    if (( $(echo \"$api_response_time_ms &gt; $RESPONSE_TIME_THRESHOLD\" | bc -l) )); then\n        send_alert \"WARNING\" \"Slow API response: ${api_response_time_ms}ms (threshold: ${RESPONSE_TIME_THRESHOLD}ms)\"\n    fi\n}\n\nsend_alert() {\n    local severity=\"$1\"\n    local message=\"$2\"\n    local alert_file=\"/tmp/prs-last-alert-$(echo \"$message\" | md5sum | cut -d' ' -f1)\"\n    local current_time=$(date +%s)\n\n    # Rate limiting: don't send same alert within 1 hour\n    if [ -f \"$alert_file\" ]; then\n        local last_alert_time=$(cat \"$alert_file\")\n        if [ $((current_time - last_alert_time)) -lt 3600 ]; then\n            return 0\n        fi\n    fi\n\n    echo \"$current_time\" &gt; \"$alert_file\"\n\n    log_message \"ALERT [$severity] $message\"\n\n    # Send email alert\n    if command -v mail &gt;/dev/null 2&gt;&amp;1; then\n        echo \"$message\" | mail -s \"PRS Monitoring Alert: $severity\" admin@your-domain.com\n    fi\n\n    # Send to webhook if configured\n    if [ -n \"${ALERT_WEBHOOK_URL:-}\" ]; then\n        curl -X POST \"$ALERT_WEBHOOK_URL\" \\\n            -H \"Content-Type: application/json\" \\\n            -d \"{\\\"severity\\\":\\\"$severity\\\",\\\"message\\\":\\\"$message\\\",\\\"timestamp\\\":\\\"$(date -Iseconds)\\\"}\" \\\n            &gt;/dev/null 2&gt;&amp;1 || true\n    fi\n}\n\nmain() {\n    log_message \"Starting performance monitoring cycle\"\n\n    # Create metrics directory if it doesn't exist\n    mkdir -p \"$(dirname \"$METRICS_FILE\")\"\n\n    # Collect metrics\n    collect_system_metrics\n    collect_application_metrics\n\n    # Check for alerts\n    check_alerts\n\n    log_message \"Performance monitoring cycle completed\"\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"scripts/monitoring/#database-performance-monitor","title":"Database Performance Monitor","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/database-performance-monitor.sh\n# Database-specific performance monitoring\n\nset -euo pipefail\n\nLOG_FILE=\"/var/log/prs-db-monitoring.log\"\nMETRICS_FILE=\"/var/lib/node_exporter/textfile_collector/prs-db-metrics.prom\"\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\ncollect_database_metrics() {\n    log_message \"Collecting database performance metrics\"\n\n    # Database connection and activity metrics\n    local db_stats=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT \n        (SELECT count(*) FROM pg_stat_activity) as total_connections,\n        (SELECT count(*) FROM pg_stat_activity WHERE state = 'active') as active_connections,\n        (SELECT count(*) FROM pg_stat_activity WHERE state = 'idle') as idle_connections,\n        (SELECT round(100.0 * sum(blks_hit) / nullif(sum(blks_hit) + sum(blks_read), 0), 2) FROM pg_stat_database WHERE datname = 'prs_production') as cache_hit_ratio,\n        (SELECT pg_database_size('prs_production')) as database_size,\n        (SELECT sum(xact_commit + xact_rollback) FROM pg_stat_database WHERE datname = 'prs_production') as total_transactions;\n    \" | tr '|' ' ')\n\n    read total_conn active_conn idle_conn cache_hit db_size total_txn &lt;&lt;&lt; \"$db_stats\"\n\n    # Query performance metrics\n    local slow_queries=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT count(*) FROM pg_stat_statements WHERE mean_time &gt; 1000 AND calls &gt; 10;\n    \" | xargs)\n\n    # Table statistics\n    local table_stats=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT \n        sum(n_tup_ins) as total_inserts,\n        sum(n_tup_upd) as total_updates,\n        sum(n_tup_del) as total_deletes,\n        sum(n_live_tup) as live_tuples,\n        sum(n_dead_tup) as dead_tuples\n    FROM pg_stat_user_tables;\n    \" | tr '|' ' ')\n\n    read total_ins total_upd total_del live_tup dead_tup &lt;&lt;&lt; \"$table_stats\"\n\n    # TimescaleDB specific metrics\n    local ts_stats=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT \n        count(*) as total_chunks,\n        count(*) FILTER (WHERE is_compressed = true) as compressed_chunks,\n        coalesce(round(avg((before_compression_total_bytes::numeric - after_compression_total_bytes::numeric) / before_compression_total_bytes::numeric * 100), 2), 0) as avg_compression_ratio\n    FROM timescaledb_information.chunks c\n    LEFT JOIN timescaledb_information.compressed_hypertable_stats s ON c.hypertable_name = s.hypertable_name;\n    \" | tr '|' ' ')\n\n    read total_chunks compressed_chunks avg_compression &lt;&lt;&lt; \"$ts_stats\"\n\n    # Write metrics to Prometheus format\n    cat &gt; \"$METRICS_FILE\" &lt;&lt; EOF\n# HELP prs_db_connections Database connections\n# TYPE prs_db_connections gauge\nprs_db_connections{state=\"total\"} ${total_conn:-0}\nprs_db_connections{state=\"active\"} ${active_conn:-0}\nprs_db_connections{state=\"idle\"} ${idle_conn:-0}\n\n# HELP prs_db_cache_hit_ratio Database cache hit ratio percentage\n# TYPE prs_db_cache_hit_ratio gauge\nprs_db_cache_hit_ratio ${cache_hit:-0}\n\n# HELP prs_db_size_bytes Database size in bytes\n# TYPE prs_db_size_bytes gauge\nprs_db_size_bytes ${db_size:-0}\n\n# HELP prs_db_transactions_total Total database transactions\n# TYPE prs_db_transactions_total counter\nprs_db_transactions_total ${total_txn:-0}\n\n# HELP prs_db_slow_queries Number of slow queries\n# TYPE prs_db_slow_queries gauge\nprs_db_slow_queries ${slow_queries:-0}\n\n# HELP prs_db_table_operations_total Table operations\n# TYPE prs_db_table_operations_total counter\nprs_db_table_operations_total{operation=\"insert\"} ${total_ins:-0}\nprs_db_table_operations_total{operation=\"update\"} ${total_upd:-0}\nprs_db_table_operations_total{operation=\"delete\"} ${total_del:-0}\n\n# HELP prs_db_tuples Table tuples\n# TYPE prs_db_tuples gauge\nprs_db_tuples{state=\"live\"} ${live_tup:-0}\nprs_db_tuples{state=\"dead\"} ${dead_tup:-0}\n\n# HELP prs_timescaledb_chunks TimescaleDB chunks\n# TYPE prs_timescaledb_chunks gauge\nprs_timescaledb_chunks{state=\"total\"} ${total_chunks:-0}\nprs_timescaledb_chunks{state=\"compressed\"} ${compressed_chunks:-0}\n\n# HELP prs_timescaledb_compression_ratio Average compression ratio percentage\n# TYPE prs_timescaledb_compression_ratio gauge\nprs_timescaledb_compression_ratio ${avg_compression:-0}\nEOF\n\n    log_message \"Database metrics collected: Connections=${total_conn:-0}, Cache_Hit=${cache_hit:-0}%, Slow_Queries=${slow_queries:-0}\"\n}\n\nanalyze_query_performance() {\n    log_message \"Analyzing query performance\"\n\n    # Get top slow queries\n    docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -c \"\n    SELECT \n        left(query, 80) as query_snippet,\n        calls,\n        round(mean_time::numeric, 2) as avg_time_ms,\n        round(total_time::numeric, 2) as total_time_ms,\n        round(100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0), 2) as hit_percent\n    FROM pg_stat_statements \n    WHERE calls &gt; 100\n    ORDER BY total_time DESC \n    LIMIT 10;\n    \" &gt; /tmp/slow-queries-$(date +%Y%m%d_%H%M%S).log\n\n    # Check for table bloat\n    local bloated_tables=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT count(*) FROM pg_stat_user_tables \n    WHERE n_dead_tup &gt; 1000 \n    AND n_dead_tup::float / NULLIF(n_live_tup + n_dead_tup, 0) &gt; 0.1;\n    \" | xargs)\n\n    if [ \"$bloated_tables\" -gt 0 ]; then\n        log_message \"WARNING: $bloated_tables tables have significant bloat\"\n    fi\n}\n\nmain() {\n    log_message \"Starting database performance monitoring\"\n\n    collect_database_metrics\n    analyze_query_performance\n\n    log_message \"Database performance monitoring completed\"\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"scripts/monitoring/#application-health-monitor","title":"Application Health Monitor","text":"<pre><code>#!/bin/bash\n# /opt/prs-deployment/scripts/application-health-monitor.sh\n# Application-specific health monitoring\n\nset -euo pipefail\n\nLOG_FILE=\"/var/log/prs-app-monitoring.log\"\nHEALTH_ENDPOINTS=(\n    \"https://localhost/api/health\"\n    \"https://localhost/api/version\"\n    \"https://localhost/\"\n)\n\nlog_message() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') - $1\" | tee -a \"$LOG_FILE\"\n}\n\ncheck_application_health() {\n    log_message \"Checking application health endpoints\"\n\n    local all_healthy=true\n\n    for endpoint in \"${HEALTH_ENDPOINTS[@]}\"; do\n        local response_code=$(curl -s -o /dev/null -w \"%{http_code}\" \"$endpoint\" 2&gt;/dev/null || echo \"000\")\n        local response_time=$(curl -w \"%{time_total}\" -o /dev/null -s \"$endpoint\" 2&gt;/dev/null || echo \"0\")\n\n        if [ \"$response_code\" = \"200\" ]; then\n            log_message \"\u2713 $endpoint - OK (${response_time}s)\"\n        else\n            log_message \"\u2717 $endpoint - FAILED (HTTP $response_code)\"\n            all_healthy=false\n        fi\n    done\n\n    if [ \"$all_healthy\" = false ]; then\n        send_health_alert \"Application health check failed\"\n    fi\n}\n\ncheck_service_logs() {\n    log_message \"Checking service logs for errors\"\n\n    local services=(\"prs-onprem-backend\" \"prs-onprem-frontend\" \"prs-onprem-nginx\")\n\n    for service in \"${services[@]}\"; do\n        local error_count=$(docker logs \"$service\" --since 1h 2&gt;&amp;1 | grep -i error | wc -l)\n        local warning_count=$(docker logs \"$service\" --since 1h 2&gt;&amp;1 | grep -i warning | wc -l)\n\n        log_message \"$service: $error_count errors, $warning_count warnings (last hour)\"\n\n        if [ \"$error_count\" -gt 10 ]; then\n            send_health_alert \"High error count in $service: $error_count errors in last hour\"\n        fi\n    done\n}\n\nmonitor_user_activity() {\n    log_message \"Monitoring user activity\"\n\n    # Check active sessions\n    local active_sessions=$(docker exec prs-onprem-redis redis-cli -a \"$REDIS_PASSWORD\" eval \"return #redis.call('keys', 'session:*')\" 0 2&gt;/dev/null || echo \"0\")\n\n    # Check recent user activity\n    local recent_logins=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"\n    SELECT count(*) FROM audit_logs \n    WHERE action = 'login' \n    AND created_at &gt;= NOW() - INTERVAL '1 hour';\n    \" 2&gt;/dev/null | xargs || echo \"0\")\n\n    log_message \"Active sessions: $active_sessions, Recent logins: $recent_logins\"\n\n    # Alert on unusual activity\n    if [ \"$active_sessions\" -gt 200 ]; then\n        send_health_alert \"Unusually high session count: $active_sessions\"\n    fi\n}\n\nsend_health_alert() {\n    local message=\"$1\"\n\n    log_message \"HEALTH ALERT: $message\"\n\n    if command -v mail &gt;/dev/null 2&gt;&amp;1; then\n        echo \"$message\" | mail -s \"PRS Application Health Alert\" admin@your-domain.com\n    fi\n}\n\nmain() {\n    log_message \"Starting application health monitoring\"\n\n    check_application_health\n    check_service_logs\n    monitor_user_activity\n\n    log_message \"Application health monitoring completed\"\n}\n\nmain \"$@\"\n</code></pre>"},{"location":"scripts/monitoring/#monitoring-automation","title":"Monitoring Automation","text":""},{"location":"scripts/monitoring/#monitoring-cron-setup","title":"Monitoring Cron Setup","text":"<pre><code>#!/bin/bash\n# Setup monitoring automation\n\nsetup_monitoring_cron() {\n    (crontab -l 2&gt;/dev/null; cat &lt;&lt; 'EOF'\n# PRS Monitoring Schedule\n\n# System performance monitoring every minute\n* * * * * /opt/prs-deployment/scripts/system-performance-monitor.sh &gt;/dev/null 2&gt;&amp;1\n\n# Database performance monitoring every 5 minutes\n*/5 * * * * /opt/prs-deployment/scripts/database-performance-monitor.sh &gt;/dev/null 2&gt;&amp;1\n\n# Application health monitoring every 2 minutes\n*/2 * * * * /opt/prs-deployment/scripts/application-health-monitor.sh &gt;/dev/null 2&gt;&amp;1\n\n# Comprehensive health check every 5 minutes\n*/5 * * * * /opt/prs-deployment/scripts/system-health-check.sh &gt;/dev/null 2&gt;&amp;1\n\n# Log analysis every 15 minutes\n*/15 * * * * /opt/prs-deployment/scripts/log-analysis.sh &gt;/dev/null 2&gt;&amp;1\n\n# Generate monitoring report daily at 8:00 AM\n0 8 * * * /opt/prs-deployment/scripts/generate-monitoring-report.sh\nEOF\n    ) | crontab -\n\n    echo \"Monitoring cron jobs configured successfully\"\n}\n\nsetup_monitoring_cron\n</code></pre>"},{"location":"scripts/monitoring/#monitoring-dashboard-script","title":"Monitoring Dashboard Script","text":"<pre><code>#!/bin/bash\n# Real-time monitoring dashboard\n\ndisplay_monitoring_dashboard() {\n    while true; do\n        clear\n        echo \"==============================================\"\n        echo \"         PRS Real-time Monitoring Dashboard\"\n        echo \"==============================================\"\n        echo \"Last Updated: $(date)\"\n        echo \"\"\n\n        # System Status\n        echo \"SYSTEM STATUS\"\n        echo \"-------------\"\n        printf \"CPU:    %3s%% \" \"$(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | sed 's/%us,//' | cut -d. -f1)\"\n        [ \"$(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | sed 's/%us,//' | cut -d. -f1)\" -gt 80 ] &amp;&amp; echo \"[HIGH]\" || echo \"[OK]\"\n\n        printf \"Memory: %3s%% \" \"$(free | grep Mem | awk '{printf \"%.0f\", $3/$2 * 100.0}')\"\n        [ \"$(free | grep Mem | awk '{printf \"%.0f\", $3/$2 * 100.0}')\" -gt 85 ] &amp;&amp; echo \"[HIGH]\" || echo \"[OK]\"\n\n        printf \"SSD:    %3s%% \" \"$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\"\n        [ \"$(df /mnt/ssd | awk 'NR==2 {print $5}' | sed 's/%//')\" -gt 90 ] &amp;&amp; echo \"[HIGH]\" || echo \"[OK]\"\n\n        printf \"HDD:    %3s%% \" \"$(df /mnt/hdd | awk 'NR==2 {print $5}' | sed 's/%//')\"\n        [ \"$(df /mnt/hdd | awk 'NR==2 {print $5}' | sed 's/%//')\" -gt 85 ] &amp;&amp; echo \"[HIGH]\" || echo \"[OK]\"\n\n        echo \"\"\n\n        # Service Status\n        echo \"SERVICE STATUS\"\n        echo \"--------------\"\n        local services=(\"prs-onprem-nginx\" \"prs-onprem-frontend\" \"prs-onprem-backend\" \"prs-onprem-postgres-timescale\" \"prs-onprem-redis\")\n\n        for service in \"${services[@]}\"; do\n            if docker ps --filter \"name=$service\" --filter \"status=running\" | grep -q \"$service\"; then\n                printf \"%-25s [RUNNING]\\n\" \"$service\"\n            else\n                printf \"%-25s [STOPPED]\\n\" \"$service\"\n            fi\n        done\n\n        echo \"\"\n\n        # Application Metrics\n        echo \"APPLICATION METRICS\"\n        echo \"-------------------\"\n        local active_sessions=$(docker exec prs-onprem-redis redis-cli -a \"$REDIS_PASSWORD\" eval \"return #redis.call('keys', 'session:*')\" 0 2&gt;/dev/null || echo \"N/A\")\n        local db_connections=$(docker exec prs-onprem-postgres-timescale psql -U prs_admin -d prs_production -t -c \"SELECT count(*) FROM pg_stat_activity;\" 2&gt;/dev/null | xargs || echo \"N/A\")\n        local api_response=$(curl -w \"%{time_total}\" -o /dev/null -s https://localhost/api/health 2&gt;/dev/null || echo \"N/A\")\n\n        echo \"Active Sessions: $active_sessions\"\n        echo \"DB Connections:  $db_connections\"\n        echo \"API Response:    ${api_response}s\"\n\n        echo \"\"\n        echo \"Press Ctrl+C to exit\"\n        sleep 5\n    done\n}\n\n# Run dashboard\ndisplay_monitoring_dashboard\n</code></pre> <p>Monitoring Scripts Ready</p> <p>Your PRS deployment now has comprehensive monitoring scripts covering system performance, database health, application monitoring, and real-time dashboards.</p> <p>Automation</p> <p>Use the provided cron setup to automate monitoring tasks and ensure continuous system observation.</p> <p>Alert Configuration</p> <p>Configure email and webhook alerts properly to ensure you receive notifications about system issues promptly.</p>"}]}